<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python闭包与装饰器]]></title>
    <url>%2F2018%2F09%2F29%2FPython_Decorator_Closure%2F</url>
    <content type="text"><![CDATA[先说闭包。 闭包闭包指延伸了作用域的函数，其中包含内部函数对外部函数定义域中自由变量的引用。闭包函数执行完成之后，能够保持当前的状态。 下面用闭包实现一个计算历史平均值的函数：1234567891011def make_averager(): series = [] def averager(new_value): series.append(new_value) total = sum(series) return total / len(series) return averageravg = make_averager()avg(10) # 10avg(11) # 10.5 修改这个函数，不保存历史输入1234567891011121314```pythondef make_averager(): count = 0 total = 0 def averager(new_value): count += 1 total += new_value return total / count return averageravg = make_averager()avg(10) 报错，原因如下： 当 count 是数字或任何不可变类型时，count += 1 语句的作用其实与 count = count + 1 一样。因此,我们在 averager 的定义体中为 count 赋值了，这会把count 变成局部变量。total 变量也受这个问题影响。上一例没遇到这个问题，因为我们没有给 series 赋值，我们只是调用series.append，并把它传给 sum 和 len。也就是说，我们利用了列表是可变的对象这一事实。但是对数字、字符串、元组等不可变类型来说，只能读取，不能更新。如果尝试重新绑定，例如 count = count + 1，其实会隐式创建局部变量 count。这样，count 就不是自由变量了，因此不会保存在闭包中。 nonlocal声明nonlocal声明的作用是把变量标记为自由变量，即使在函数中为变量赋予新值了，也会变成自由变量。如果为 nonlocal 声明的变量赋予新值，闭包中保存的绑定会更新。nonlocal清楚的表明：如果在闭包内给该变量赋值，那么修改的其实是闭包外的那个作用域中的变量，nonlocal不能延伸到模块级别。 因此正确的代码应为：123456789101112131415def make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averageravg = make_averager()avg(10) # 10avg(11) # 10.5 装饰器装饰器是可调用的对象，其参数是另一个函数（被装饰的函数），返回值也是一个函数。Gamma 等人写的《设计模式：可复用面向对象软件的基础》一书是这样概述“装饰器”模式的： “动态地给一个对象添加一些额外的职责”。 装饰器的两大特性： 能把被装饰的函数替换成其他函数。 装饰器在加载模块时立即执行。 装饰器用法上有如下特点： 装饰器通常在一个模块中定义，然后应用到其他模块的函数上。 大多数装饰器会在内部定义一个函数，然后将其返回（而不是返回原函数）。 返回原函数这种技术并非没有用处，很多 Python Web 框架使用这样的装饰器把函数添加到某种中央注册处。 1234567891011def deco(func): def inner(): print('run inner()') func() return inner@decodef target(): print('run target()') target() run inner() run target() 1234567891011121314151617181920212223import timedef clock(func): def clocked(*args): t0 = time.perf_counter() result = func(*args) elapsed = time.perf_counter() - t0 name = func.__name__ arg_str = ','.join(repr(arg) for arg in args) print('[%0.8fs] %s(%s) -&gt; %r' % (elapsed, name, arg_str, result)) return result return clocked@clockdef snooze(seconds): time.sleep(seconds) @clockdef factorial(n): return 1 if n &lt; 2 else n * factorial(n-1)snooze(.123)factorial(6) [0.12320082s] snooze(0.123) -&gt; None [0.00000126s] factorial(1) -&gt; 1 [0.00007819s] factorial(2) -&gt; 2 [0.00013763s] factorial(3) -&gt; 6 [0.00018307s] factorial(4) -&gt; 24 [0.00023142s] factorial(5) -&gt; 120 [0.00027923s] factorial(6) -&gt; 720 720]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ Prime：类]]></title>
    <url>%2F2018%2F09%2F27%2FC%2B%2B_Primer_Class2%2F</url>
    <content type="text"><![CDATA[本文是《C++ Primer》一书第七章的读书笔记。本文将通过构造SalesData、Screen和WindowMgr三个类来说明C++类的特性。 SalesData销售数据类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class SalesData&#123;public: //两个友元函数 friend istream &amp; read(istream &amp;, SalesData &amp;); friend ostream &amp; print(ostream &amp;, const SalesData &amp;); SalesData() = default; SalesData(istream &amp;); SalesData(const string &amp;s, unsigned n, double p); string Isbn() const &#123; return bookNo; &#125;; SalesData&amp; Combine(const SalesData&amp;); //将一个SalesData对象加在另一个身上 double AvgPrice() const;private: string bookNo; //书号 double units_sold = 0; //售出的册数 double price = 0.0; //单价 double revenue = 0.0; //销售总额 double average = 0.0;&#125;;SalesData::SalesData(istream &amp;is)&#123; read(is, *this);&#125;SalesData::SalesData(const string &amp;s, unsigned n, double p): bookNo(s), units_sold(n), price(p), revenue(p*n) &#123;&#125;;double SalesData::AvgPrice() const&#123; if (units_sold) return revenue / units_sold; else return 0;&#125;SalesData&amp; SalesData::Combine(const SalesData &amp;rhs)&#123; units_sold += rhs.units_sold; revenue += rhs.revenue; return *this;&#125;SalesData Add(const SalesData &amp;lhs, const SalesData &amp;rhs)&#123; SalesData sale_sum = lhs; sale_sum.Combine(rhs); return sale_sum;&#125; 引入const成员函数在定义的Isbn()和AvePrice()函数时，参数列表后面加入了const，为何要这么做？ 默认情况下，this指针是指向类类型的非常量版本的常量指针，因此默认情况下我们不能把this绑定到一个常量对象上，也就意味着我们不能在一个常量对象上调用普通的成员函数。 而当类开发者不允许成员函数改变类内变量的时候，普通的this指针无法做到，因为它指向的是非常量。此时就需要将this指针声明为一个指向常量的常量指针。C++通过在成员函数参数列表之后家const关键字来修改this指针类型为指向非常量。 返回一个this对象的函数函数combine的设计初衷类似于复合赋值运算符+=，调用该函数的对象代表的是赋值运算符左侧的运算对象，右侧运算对象则通过显式的实参被传入函数。 该函数一个值得关注的部分是它的返回类型和返回语句。当我们定义的函数类似于内置运算符时，应该令该函数的行为尽量模仿这个运算符。内置运算符把它的左侧运算对象当成左值返回。因此成员函数也应返回：return *this; //返回执行该函数的对象，并且函数返回类型为引用（如果声明为普通的返回类型，那得到的就是一个副本）。 非成员函数接口声明一般来说，如果非成员函数是类借口的组成部分，则这些函数的声明应该与类在同一个头文件当中。123456789101112131415istream &amp;read(istream &amp;is, SalesData &amp;item)&#123; double price = 0; is &gt;&gt; item.bookNo &gt;&gt; item.units_sold &gt;&gt; price; item.revenue = price * item.units_sold; return is;&#125;ostream &amp;print(ostream &amp;os, const SalesData &amp;item)&#123; os &lt;&lt; item.Isbn() &lt;&lt; " " &lt;&lt; item.units_sold &lt;&lt; " " &lt;&lt; item.revenue &lt;&lt; " " &lt;&lt; item.AvgPrice(); return os;&#125; read 和 print 分别接受各自 IO 类型的引用作为其参数。 IO 类属于不能被拷贝的类型，我们只能通过引用来传递它们。 构造函数构造函数不能声明成const的。 如果我们的类没有显式地定义构造函数，那么编译器就会为我们隐式地定义一个默认构造函数，又叫合成的默认构造函数。合成默认构造函数无须任何实参。 对于大多数类来说，这个合成默认构造函数将按照如下规则初始化类的数据成员： 如果存在类内初始值，用它来初始化成员。 否则，默认初始化该成员。 只有当类没有声明任何构造函数时，编译器才会自动地生成默认构造函数。 某些类不能依赖于合成的默认构造函数合成的默认构造函数只适合非常简单的类，比如现在定义的这个SalesData版本，对于一个普通的类来说，必须定义它自己的默认构造函数，原因有三： 第一个原因也是最容易理解的一个原因是编译器只有在发现类不包含任何构造函数的情况下才会替我们生成一个默认的构造函数。一旦我们定义了一些其他的构造函数，那么除非我们再定义一个默认的构造函数，否则类将没有默认构造函数。 第二个原因是对于某些类来说，合成的默认构造函数可能执行错误的操作。如果定义在块中的内置类型或复合类型（比如数组和指针）的对象被默认初始化，则它们的值将是未定义的，如果视图拷贝或以其他形式访问此变量就会引发错误。该准则同样适用于默认初始化的内置类型成员。因此，含有内置类型或复合类型成员的类型应该在类的内部初始化这些成员，或者定义一个自己的默认构造函数。否则，用户在创建类的对象时就可能得到未定义的值。 第三个原因是有的时候编译器不能为某些类合成默认的构造函数。例如，如果类中包含一个其他类类型的成员且这个成员的类型没有默认的构造函数，那么编译器将无法承受该成员。对于这样的类来说，我们必须自定义默认构造函数，否则该类将没有可用的默认构造函数。 ==default的含义1SalesData() = default; 首先明确一点，因为该构造函数不接受任何实参，所以它时一个默认构造函数。我们定义这个构造函数的目的仅仅是因为我们既需要其他形式的构造函数，也需要默认的构造函数。我们希望这个函数的作用完全等同于之前使用的合成默认构造函数。 在 C++11 中，如果我们需要默认的行为，那么可以通过在参数列表后面写上 = default 来要求编译器生成构造函数。其中， = default 既可以和声明一起出现在类的内部，也可以定义出现在类的外部。和其他函数一样，如果 = default 在类的内部，则默认构造函数是内联的；如果它在类的外部，则将该成员默认情况下不是内联的。 拷贝、赋值和析构除了定义类的对象如何初始化之外，类还需要控制拷贝、赋值和销毁对象时发生的行为。对象在几种情况下会被拷贝，如我们初始化变量以及以值的方式传递或返回一个对象等。当我们使用赋值运算符时会发生对象的赋值操作。当对象不再存在时执行销毁的操作，比如一个局部对象会在创建它的块结束时被销毁，当vector对象（或者数组）销毁时存储在其中的对象也会被销毁。 如果我们不主动定义这些操作，则编译器将替我们合成它们。 某些类不能依赖于合成的版本尽管编译器能替我们合成拷贝、赋值和销毁的操作，但是必须要清楚的一点是，对于某些类来说合成的版本无法正常工作。特别是，当类需要分配类对象之外的资源时，合成的版本常常会失效。例如，管理动态内存的类通常不能依赖于上述操作的合成版本。 不过值得注意的是，很多需要动态内存的类能（而且应该）使用vector对象或者string对象管理必要的储存空间。使用vector或者string的类能避免分配和释放内存带来的复杂性。 进一步讲，如果类包含vector或者string成员，则其拷贝、赋值和销毁的合成版本能够正常工作。当我们对含有vector成员的对象执行拷贝或者赋值操作时，vector类会设法拷贝或赋值成员中的元素。当这样的对象被销毁时，将销毁vector对象，也就是依次销毁vector中的每一个元素。这一点与string是非常类似的。 class与struct的区别class与struct定义的类唯一区别是，class的默认访问权限是private，而struc默认访问权限是public。 友元类可以允许其他类或者函数访问它的非公有成员，方法是令其他类或者函数成为它的友元（friend）。如果类想把一个函数作为它的友元，只需要增加一条以 friend 关键字开始的函数声明语句即可。 友元声明只能出现在类定义的内部，但是在类内出现的具体位置不限。友元不是类的成员也不受它所在区域访问控制级别的约束。一般来说，最好在类定义开始或结束前的位置集中声明友元。 友元的声明仅仅指定了访问的权限，而非一个通常意义上的函数声明。如果我们希望类的用户能够调用某个友元函数，那么我们就必须在友元声明之外再专门对函数进行一次声明。 Screen和WindowMgr窗口管理类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677class Screen&#123;public: typedef string::size_type pos; Screen() = default; //cursor隐式地使用类内初始值 Screen(pos ht, pos wd, char c): height(ht), width(wd), contents(ht * wd, c) &#123;&#125; char get() const &#123; return contents[cursor]; &#125; inline char get(pos ht, pos wd) const; Screen &amp;move(pos r, pos c); Screen &amp;set(char); Screen &amp;set(pos, pos, char); //两个display函数，根据对象是否是const重载了display Screen &amp;display(std::ostream &amp;os) &#123; do_display(os); return *this; &#125; const Screen &amp;display(std::ostream &amp;os) const &#123; do_display(os); return *this; &#125; void some_member() const;private: //该函数负责显示Screen的内容 void do_display(ostream &amp;os) const &#123; os &lt;&lt; contents; &#125; pos cursor = 0; pos height = 0; pos width = 0; string contents; mutable size_t access_ctr; //即使在一个const对象内也能被修改&#125;;class WindowMgr&#123;private: //默认情况下，一个WindowMgr包含一个标准尺寸的空白Screen std::vector&lt;Screen&gt; screens&#123;Screen(24, 80, ' ')&#125;;&#125;;char Screen::get(pos r, pos c) const&#123; pos row = r * width; return contents[row + c];&#125;inline Screen &amp;Screen::move(pos r, pos c)&#123; pos row = r * width; cursor = row + c; return *this; //将this对象作为左值返回&#125;inline Screen &amp;Screen::set(char ch)&#123; contents[cursor] = ch; //设置当前光标所在位置的新值 return *this; //将this对象作为左值返回&#125;inline Screen &amp;Screen::set(pos row, pos col, char ch)&#123; contents[row * width + col] = ch; //设置指定位置的新值 return *this; //将this对象作为左值返回&#125;void Screen::some_member() const&#123; ++access_ctr; //保存一个计数值，用于记录成员函数被调用的次数 //该成员需要完成的其他工作&#125; 定义一个类型成员12345678class A&#123;public: typedef std::string::size_type type_s; using type_int = unsigned int;private: type_s a = 0;&#125;; 除了定义数据和函数成员之外，类还可以自定义某种类型在类中的别名。由类定义的类型名字和其他成员一样存在访问限制，可以是public或者private中的一种。 定义时可以使用typedef或者using。 必须先定义后使用，这一点与普通成员变量有所区别，所以类型成员通常出现在类开始的地方。 内联函数 定义在类内部的成员函数是自动inline的，如get()。 我们可以在类的内部把inline作为声明的一部分显式地声明成员函数，也能在类的外部用inline关键字修饰函数的定义。 inline成员函数也应该与相应的类定义在同一个头文件中。 可变数据成员有时我们希望能修改类的某个数据成员，即使是在一个const成员函数内。可以通过在变量的声明中加入mutable关键字做到这一点。 一个可变数据成员（mutable data member）永远不会是const，甚至当它是const对象的成员时也如此。因此一个const成员函数可以改变一个可变成员的值。 返回*this的成员函数注意到，move和set操作返回值都是调用对象的引用。返回引用的函数是左值的，意味着这些函数返回的是对象本身而非对象的副本。如果我们把这一系列这样的操作连接在一个表达式中的话，这些操作将在同一个对象上执行。 12cout &lt;&lt; "s" &lt;&lt; endl;myScreen.move(4, 0).set('#') 从const成员返回*this我们添加一个名为display的操作，他负责打印Screen的内容。我们希望这个函数能和move以及set出现在同一序列中，因此类似于move和set，display函数也应该返回执行它的对象的引用。 从逻辑上来说，显示一个Screen并不需要改变他的内容，因此我们令display为一个const成员，此时，this将是一个指向const的指针而*this是const对象，由此推断，display的返回类型应该是const Screen&amp;。然而，如果真的令display返回一个const引用，则我们不能把display嵌入到一组动作的序列中去：123Screen myScreen；//如果display返回常量引用，则调用set将引发错误myScreen.display(cout).set('*'); 即使myScreen是个非常量对象，对set的调用也无法通过编译。问题在于display的const版本返回的会是常量引用，而我们显然无权set一个常量对象。 一个const成员函数如果以引用的形式返回*this，那么它的返回类型是常量引用。 基于const的重载对于Screen类中的display函数作以下说明： 通过区分成员函数是否是const的，我们可以对其进行重载。具体说来，因为非常量版本的函数对于常量对象是不可用的，所有我们只能在一个常量对象上调用const成员函数。另一方面，虽然可以在非常量对象上调用常量版本和非常量版本，但显然此时非常量版本是一个更好的匹配。 当一个成员调用另外一个成员时，this指针在其中隐式地传递。因此，当display调用do_display时，它的this指针隐式地传递给do_display，而当display的非常量版本调用do_display时，它的this指针将隐式地从指向非常量的指针转换成指向常量的指针。当do_display完成后，display函数各自返回解引用this所得的对象。在非常量版本中，this指向一个非常量对象，因此display返回一个普通的（非常量）引用；而const成员则返回一个常量引用。 当我们在某个对象上调用display时，该对象是否是const决定了应该调用display的哪个版本：1234Screen myScreen(5,3);const Screen blank(5,3);myScreen.set('#').display(cout); //调用非常量版本blank.display(cout); //调用常量版本]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法总结]]></title>
    <url>%2F2018%2F09%2F14%2FSummary_Of_Sorting_Algorithm%2F</url>
    <content type="text"></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python标识、相等性和别名]]></title>
    <url>%2F2018%2F09%2F03%2FPython_Identification_%2F</url>
    <content type="text"><![CDATA[每个变量都有标识、类型和值。对象一旦创建，它的标识绝不会变；可以把标识理解为对象在内存中的地址。 id()返回标识的整数标识，可以认为是对象内存地址。 ==运算符比较两个对象的值，is比较对象的标识。 =直接赋值，传递的是对象的引用，相当于一个别名，其中一个改变另一个也会跟着变。 元组的不可变是相对不可变。 123t1 = (1, 2, [30, 40])t2 = (1, 2, [30, 40])t1 == t2 True 1id(t1[-1]) 140181428815816 12t1[-1].append(99)t1 (1, 2, [30, 40, 99, 99]) 1id(t1[-1]) 140181428815816 1t1 == t2 False 虽然 t1 和 t2 是不同的对象，但是二者相等——与预期相符。t1[-1] 的标识没变，只是值变了。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python函数参数的传递]]></title>
    <url>%2F2018%2F09%2F02%2FPython_Function_Arg_Deliver%2F</url>
    <content type="text"><![CDATA[Python 唯一支持的参数传递模式是共享传参(call by sharing)，共享传参指函数的各个形式参数获得实参中各个引用的副本。也就是说，函数内部的形参是实参的别名。函数可能会修改接收到的任何可变对象。 不要用可变类型作为参数的默认值12345678class HauntedBus: """备受幽灵乘客折磨的校车""" def __init__(self, passengers=[]): # ➊ self.passengers = passengers # ➋ def pick(self, name): self.passengers.append(name) # ➌ def drop(self, name): self.passengers.remove(name) ❶ 如果没传入 passengers 参数，使用默认绑定的列表对象，一开始是空列表。❷ 这个赋值语句把 self.passengers 变成 passengers 的别名，而没有传入passengers 参数时,后者又是默认列表的别名。❸ 在 self.passengers 上调用 .remove() 和 .append() 方法时，修改的其实是默认列表，它是函数对象的一个属性。 123456bus1 = HauntedBus() bus1.pick('Carrie')bus2 = HauntedBus() bus2.passengers # ['Carrie']bus2.passengers is bus1.passengers True 可以看出bus2的列表竟然不为空。这种问题很难发现。实例化 HauntedBus 时，如果传入乘客，会按预期运作。但是不为 HauntedBus 指定乘客的话，奇怪的事就发生了，这是因为self.passengers 变成了 passengers 参数默认值的别名。出现这个问题的根源是，默认值在定义函数时计算(通常在加载模块时)，因此默认值变成了函数对象的属性。因此，如果默认值是可变对象，而且修改了它的值，那么后续的函数调用都会受到影响。 防御可变参数如果定义的函数接收可变参数,应该谨慎考虑调用方是否期望修改传入的参数。 1234567891011121314151617class TwilightBus: """让乘客销声匿迹的校车""" def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = passengers def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) basketball_team = ['Sue', 'Tina', 'Maya', 'Diana', 'Pat'] # ➊bus = TwilightBus(basketball_team) # ➋bus.drop('Tina') # ➌bus.drop('Pat')basketball_team # ➍ [&#39;Sue&#39;, &#39;Maya&#39;, &#39;Diana&#39;] basketball_team 中有 5 个学生的名字，使用这队学生实例化 TwilightBus。两个学生下车了，下车的学生从篮球队中消失了! 当 passengers 不为 None 时，self.passengers 变成 passengers 的别名，而后者是传给 __init__ 方法的实参的别名。在 self.passengers 上调用 .remove() 和 .append() 方法其实会修改传给构造方法的那个列表。 这里的问题是，校车为传给构造方法的列表创建了别名。正确的做法是，校车自己维护乘客列表。修正的方法很简单:在 init 中，传入 passengers 参数时，应该把参数值的副本赋值给 self.passengers。 正确的做法应该如下： 12345def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = list(passengers)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】C++编译与链接]]></title>
    <url>%2F2018%2F09%2F01%2FC%2B%2B_Complie_And_Link%2F</url>
    <content type="text"><![CDATA[大家知道计算机使用的一系列的1和0，那个一个C++语言程序又是如何从一个个.h和.cpp文件变成包含1和0的可执行文件呢？ 可以认为有以下的几个环节：源程序-&gt;预处理-&gt;编译和优化-&gt;生成目标文件-&gt;链接-&gt;可执行文件 在预编译的时候，.h头文件会被复制、扩展到包含它的.cpp文件里，然后编译器编译该.cpp文件为一个.obj文件，每个.cpp文件作为一个编译单元独立编译。当编译器将一个工程里的所有.cpp文件以分离的方式编译完毕后，再由链接器进行链接成为一个可执行文件。 预处理C++的预处理是指在C++程序源代码被编译之前，由预处理器对C++程序源代码进行的处理。这个过程并不对程序的源代码进行解析。 预处理器主要负责以下的几处： 宏的替换 删除注释 处理所有的条件预编译指令，如：#if #ifdef #elif #else #endif 处理#include预编译指令，将被包含的文件插进到该指令的位置，这个过程是递归的 添加行号与文件名标识，以便产生调试用的行号信息以及编译错误或警告时能够显示行号 保留所有的#pragma编译器指令，因为编译器需要使用它们 编译、链接把预处理完的cpp文件进行一系列词法分析、语法分析、语义分析及优化后生成汇编代码。之后汇编代码-&gt;机器指令。各文件的编译和汇编是独立的。如果编译通过，就会把对应的CPP转换成OBJ文件。 编译单元：根据C++标准，每一个CPP文件就是一个编译单元。每个编译单元之间是相互独立并且互相不可知。 目标文件：由编译所生成的文件，以机器码的形式包含了编译单元里所有的代码和数据，还有一些其他信息，如未解决符号表，导出符号表和地址重定向表等。目标文件是以二进制的形式存在的。 假设有一个A.cpp文件，如下定义：123456int n = 1;void FunA()&#123; ++n;&#125; 它编译出来的目标文件A.obj就会有一个区域（或者说是段），包含以上的数据和函数，其中就有n、FunA，以文件偏移量形式给出可能就是下面这种情况：123偏移量 内容 长度0x0000 n 40x0004 FunA ?? 实际目标文件的布局可能不是这样，这里只是方便学习才这样表示，??便是未知。目标文件的各个数据可能不是连续的，也不一定是从0x0000开始。 有另外一个B.cpp文件，定义如下：123456extern int n;void FunB()&#123; ++n;&#125; 它对应的B.obj的二进制：12偏移量 内容 长度0x0000 FunB ?? 由于n被声明为extern，而extern关键字告诉编译器n已经在别的编译单元里定义了，在这个单元里不用定义。由于编译单元之间是互不相关的，所以编译器就不知道n究竟在哪里，所以在函数B.obj中就没有办法生成n的地址。 为了让各个编译单元结合起来，就需要链接器了。为了能让链接器知道哪些地方的地址没有填好（也就是还????），那么目标文件中就要有一个表来告诉链接器，这个表就是“未解决符号表”（unresolved symbol table）。同样，提供n的目标文件也要提供一个“导出符号表”（exprot symbol table），来告诉链接器自己可以提供哪些地址。 因此，一个目标文件不仅要提供数据和二进制代码，还要提供两个表：未解决符号表和导出符号表，来告诉链接器自己需要什么和自己能提供些什么。 那么这两个表是怎么建立对应关系的呢？ 在C/C++中，每一个变量及函数都会有自己的符号，如变量n的符号就是n，函数的符号会更加复杂，根据编译器不同而不同。 A.obj的导出符号表为123符号 地址n 0x0000_FunA 0x0004 未解决符号为空。 B.obj的导出符号表为12符号 地址_FunB 0x0000 未解决符号表为12符号 地址n 0x0001 这个表告诉链接器，在本编译单元0x0001位置有一个地址，该地址不明，但符号是n。 在链接的时候，链接器在B.obj中发现了未解决符号，就会在所有的编译单元中的导出符号表去查找与这个未解决符号相匹配的符号名，如果找到，就把这个符号的地址填到B.obj的未解决符号的地址处。如果没有找到，就会报链接错误。在此例中，在A.obj中会找到符号n，就会把n的地址填到B.obj的0x0001处。然后解决地址重复的问题，对每个目标文件的地址进行调整，提供一个地址重定向表。 总结： 目标文件至少要提供三个表：未解决符号表，导出符号表和地址重定向表。 未解决符号表：列出了本单元里有引用但是不在本单元定义的符号及其出现的地址。 导出符号表：提供了本编译单元具有定义，并且可以提供给其他编译单元使用的符号及其在本单元中的地址。 地址重定向表：提供了本编译单元所有对自身地址的引用记录。 当链接器进行链接的时候，首先决定各个目标文件在最终可执行文件里的位置。然后访问所有目标文件的地址重定义表，对其中记录的地址进行重定向（加上一个偏移量，即该编译单元在可执行文件上的起始地址）。然后遍历所有目标文件的未解决符号表，并且在所有的导出符号表里查找匹配的符号，并在未解决符号表中所记录的位置上填写实现地址。最后把所有的目标文件的内容写在各自的位置上，再做一些其他工作，就生成一个可执行文件。 说明：实现链接的时候会更加复杂，一般实现的目标文件都会把数据，代码分成好向个区，重定向按区进行，但原理都是一样的。 外部链接与内部链接内部链接如果一个名称对于他的编译单元是局部的，并且在链接时不会与其他的编译单元中同样的名字冲突，那么这个名称就拥有内部链接。这个实体有内部链接，他就不会与其他.cpp文件同名的实体冲突。换个说法，那些编译单元（.cpp）中不能向其他编译单元（.cpp）展示提供其定义的函数、变量就拥有内部链接。 那么哪些实体拥有内部链接？ 静态(static)全局变量定义.如: static int x; 枚举类型定义.如: enum Boolean {NO,YES }; 类定义. 如: class Point { int d_x; int d_y; ... }; 内联函数定义.如: inline int operator==(const Point&amp; left,const Point&amp;right) { ... } union的定义. 名字空间中const常量定义 外部链接一个多文件的程序中，一个实体可以在链接时与其他编译单元交互，那么这个实体就拥有外部链接。 换个说法，那些编译单元（.cpp）中能向其他编译单元（.cpp）提供其定义，让其他编译单元(.cpp)使用的函数、变量就拥有外部链接。 那么哪些实体拥有外部链接？ 非内联的类成员函数.如: Point&amp; Point::operator+=(const Point&amp; right) { ... } 非内联、非静态的自由函数. 如: Point operator+(const Point&amp; left, const Point&amp; right) { ... } 非静态的全局定义 几个经典的链接错误unresolved external link.. 这个很显然，是链接器发现一个未解决符号，但是在导出符号表里没有找到对应的项。 解决方案就是在某个编译单元里提供这个符号的定义。（注意，这个符号可以是一个变量，也可以是一个函数），也可以看看是不是有什么该链接的文件没有链接。 duplicated external simbols... 这个则是导出符号表里出现了重复项，因此链接器无法确定应该使用哪一个。这可能是使用了重复的名称，也可能有别的原因。 C/C++针对这些而提供的特性：extern：告诉编译器，这个符号在别的编译单元里定义，也就是要把这个符号放到未解决符号表里去。（外部链接） static：如果该关键字位于全局函数或者变量的声明的前面，表明该编译单元不导出这个函数/变量的符号。因此无法在别的编译单元里使用（内部链接）。如果是static局部变量，则该变量的存储方式和全局变量一样，但是仍然不导出符号。 外部链接的利弊：外部链接的符号，可以在整个程序范围内使用（因为导出了符号）。但是同时要求其他的编译单元不能导出相同的符号（不然就是duplicated external simbols) 内部链接的利弊：内部链接的符号，不能在别的编译单元内使用。但是不同的编译单元可以拥有同样名称的内部链接符号。 一些问题的解答 为什么头文件里一般只可以有声明不能有定义？头文件可以被多个编译单元包含，如果头文件里有定义，那么每个包含这个头文件的编译单元就都会对同一个符号进行定义，如果该符号为外部链接，则会导致duplicated external simbols。因此如果头文件里要定义，必须保证定义的符号只能具有内部链接。 为什么类的静态变量不可以就地初始化？所谓就地初始化就是类似于这样，由于class的声明通常是在头文件里，如果允许这样做，其实就相当于在头文件里定义了一个非const变量。 1234 class A&#123; static char msg[] = "aha";&#125;; 为什么公共使用的内联函数要定义于头文件里？因为编译时编译单元之间互相不知道，如果内联函数被定义于.cpp文件中，编译其他使用该函数的编译单元时没有办法找到函数的定义，因此无法对函数进行展开。所以说如果内联函数定义于.cpp文件里，那么就只有这个cpp文件可以使用这个函数。 头文件里的内联函数被拒绝会怎样？记住，内联只是给编译器的一个建议，如果定义于头文件里的内联函数被拒绝，那么编译器会自动在每个包含了该头文件的编译单元里定义这个函数并且不导出符号。 参考[1]. C++编译链接原理简介[2]. C++编译与链接（1）-编译与链接过程]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ STL学习笔记]]></title>
    <url>%2F2018%2F08%2F25%2FC%2B%2B%20STL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[本文记录C++ STL学习过程中的一些知识。 std::pairpair 实质上是一个结构体，其主要的两个成员变量是 first 和 second。 pair的应用std::pair 主要的作用是将两个数据组合成一个数据，两个数据可以是同一类型或者不同类型。当一个函数需要返回2个数据的时候，可以选择 pair。 12345pair&lt;int,float&gt; p1; //使用默认构造函数pair&lt;double,double&gt; p2(1.0, 2.4) //用给定值初始化pair&lt;int, float&gt; p3(p0) //使用拷贝构造函数p1.first = 1;p1.second = 2; make_pair函数初始化一个 pair 可以使用构造函数，也可以使用 std::make_pair 函数，make_pair 函数的定义如下： 1234template pair make_pair(T1 a, T2 b) &#123; return pair(a, b); &#125; 一般 make_pair都使用在需要 pair 做参数的位置，可以直接调用 make_pair 生成 pair 对象。 另一个使用的方面就是 pair 可以接受隐式的类型转换，这样可以获得更高的灵活度。但是这样会出现如下问题：例如有如下两个定义： 12pair&lt;int, float&gt;(1, 1.1);make_pair(1, 1.1); 其中第一个的 second 变量是 float 类型，而 make_pair 函数会将 second 变量都转换成 double 类型。这个问题在编程是需要引起注意。 priority_queue优先队列容器与队列一样，只能从队尾插入，从队首删除。但它在此基础上内部添加了的一个排序，所以出队时，并非按照先进先出的原则进行，而是将当前队列中最大的元素或最小元素出队。 定义priority_queue&lt;Type, Container, Functional&gt;Type 就是数据类型，Container 就是容器类型（Container必须是用数组实现的容器，比如vector,deque等等，但不能用 list。STL里面默认用的是vector），Functional 就是比较的方式，当需要用自定义的数据类型时才需要传入这三个参数，使用基本数据类型时，只需要传入数据类型，默认是大顶堆 123456//升序队列priority_queue &lt;int,vector&lt;int&gt;,greater&lt;int&gt; &gt; q;//降序队列priority_queue &lt;int,vector&lt;int&gt;,less&lt;int&gt; &gt;q;//greater和less是std实现的两个仿函数（就是使一个类的使用看上去像一个函数。其实现就是类中实现一个operator()，这个类就有了类似函数的行为，就是一个仿函数类了)priority_queue&lt;int&gt; a; //等同于 priority_queue&lt;int, vector&lt;int&gt;, less&lt;int&gt; &gt; a; 基本成员函数empty() pop() push() size() top() 优先级声明方式 123456789101112131415161718192021//自定义优先级：struct cmp &#123; operator bool ()(int x, int y) &#123; return x &gt; y; // x小的优先级高 //也可以写成其他方式，如： return p[x] &gt; p[y];表示p[i]小的优先级高 &#125;&#125;;priority_queue&lt;int, vector&lt;int&gt;, cmp&gt; q; //定义方法//结构体声明方式：struct node &#123; int x, y; friend bool operator &lt; (node a, node b) &#123; return a.x &gt; b.x; //结构体中，x小的优先级高 &#125;&#125;;priority_queue&lt;node&gt;q; //定义方法//在该结构中，y为值, x为优先级。//通过自定义operator&lt;操作符来比较元素中的优先级。//在重载”&lt;”时，最好不要重载”&gt;”，可能会发生编译错误]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ Primer：面向对象]]></title>
    <url>%2F2018%2F08%2F25%2FC%2B%2B_Primer_Object_Oriented_Programming%2F</url>
    <content type="text"></content>
      <categories>
        <category>C++</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯]]></title>
    <url>%2F2018%2F08%2F20%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯(naive Bayes) 法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。 基本模型假设我们的输入空间 $\chi \subseteq {R^n}$ 为n维向量的集合，输出空间为类标记集合 $\psi = \{ c_1, c_2, \dots, c_k \}$ ，输入特征向量 $ x \in {R^n} $，输出为类标记 $ y \in \psi $，$X$ 是定义在输入空间 $\chi$ 上的随机向量，$Y$ 是定义在输出空间上 $\psi$ 上的随机变量，朴素贝叶斯通过训练数据集学习联合概率分布 $P(X,Y)$。训练数据集为： T = \{ (x_{1}, y_{1}),(x_{2}, y_{2}),...,(x_{N}, y_{N}) \}具体地， 学习数据的先验概率: P(Y=c_k) \quad k=1, 2, \dots,K 和类条件概率分布 P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)}, \dots, X^{(n)}=x^{(n)} |Y=c_k) \quad k=1, 2, \dots,K条件概率分布 $P(X=x|Y=c_{k} )$ 有指数级数量的参数。事实上，假设$x^{(j)}$ 可取值有 $S_{j}$ 个，$j=1,2,…n$，$Y$ 可取值有 $K$ 个，那么参数个数为 $K\prod_{j=1}^{n} S_{j}$ 。 朴素贝叶斯法对条件概率作了条件独立性假设，具体如下： \begin{align*} P(X=x|Y=c_{k} ) &= P(X^{(1)}=x^{(1)} ,...,X^{(n)}=x^{(n)}|Y=c_{k} ) \\ &=\prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_{k} ) \end{align*} 条件独立假设等于说用于分类的特征在类确定的条件下都是独立的，这一假设使得朴素贝叶斯变得简单，但有时会牺牲一定的分类准确性 朴素贝叶斯法分类时，对给定的输入$x$，通过学习到的模型计算后验概率分布 $P(Y=c_{k} |X=x)$，将后验概率最大的类作为输出。后验概率计算根据贝叶斯定理进行： P(Y=c_{k}|X=x )=\frac{P(X=x|Y=c_{k} )P(Y=c_{k} )}{\sum_{k=1}^{K}{P(X=x|Y=c_{k} )P(Y=c_{k} )} }将条件独立性假设代入上式得： P(Y=c_{k}|X=x )=\frac{P(Y=c_{k} )\prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_{k} )}{\sum_{k=1}^{K}{P(Y=c_{k} )\prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_{k} )} }这是朴素贝叶斯分类的基本公式。于是，朴素贝叶斯分类器可表示为： y=f(x)=argmax_{c_{k} } \frac{P(Y=c_{k} )\prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_{k} )}{\sum_{k=1}^{K}{P(Y=c_{k} )\prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_{k} )} }注意到分母对所有 $c_{k}$ 都是相同的，所以 y=argmax_{c_{k} } P(Y=c_{k} )\prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_{k})参数估计朴素贝叶斯参数估计主要包括极大似然估计和贝叶斯估计。 极大似然估计先验概率的极大似然估计先验概率 $P(Y=c_k)$ 的极大似然估计为： P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}, k=1,2, \dots, K证明如下：令参数 $P(Y=c_k)=\theta_k$，其中 $k\in \left\{ 1,2..K \right\}$ 。那么随机变量 $Y$ 的概率可以用参数来表示为: P(Y)=\sum_{k=1}^{K}{\theta_k} I(Y=c_k)$I$ 是指示函数 $Y=c_k$ 成立时，$I=1$；否则 $I=0$。极大似然函数 L(y_1,y_2..y_N; \theta_k)=\prod_{i=1}^{N}P(y_i) =\prod_{k=1}^{K}\theta_k^{N_k}其中 $N$ 为样本总数，$N_k$ 为样本中 $Y=c_k$ 的样本数目，取对数得到 l(\theta_k)=ln(L(\theta))=\sum_{k=1}^{K}{N_k ln\theta_k}要求该函数的最大值，注意到约束条件 $\sum_{k=1}^{K}{\theta_k} =1$ 可以用拉格朗日乘子法，即 l(\theta_k,\lambda)=\sum_{k=1}^{K}{N_k ln\theta_k} +\lambda(\sum_{k=1}^{K}{\theta_k} -1)求导就可以得到： \frac{N_k}{\theta_k}+\lambda=0联立所有的 $k$ 以及约束条件得到 $\theta_k=\frac{N_k}{N}$ 条件概率的极大似然估计条件概率 $P(X^{(j)} = a_{jl}| Y=c_k)$ 的极大似然估计是： P(X^{(j)} = a_{jl}| Y=c_k) = \frac{\sum_{i=1}^{N}I(x_i^{(j)} = a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)}其中，$x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个属性；$a_{jl}$ 是第j个属性可能取l的值；$I$ 是指示函数。 证明如下：令参数 $\mu_{lk} = p(X = a_t | y=c_k)$，$N$ 个互相独立的样本，$N$ 为样本总数： T = \{ (x_{1}, y_{1}),(x_{2}, y_{2}),...,(x_{N}, y_{N}) \}那么极大似然函数为： \begin{align*} L((x_{1}, y_{1}),...,(x_N, y_N); \mu) &=\prod_{i=1}^{N}P(x_i, y_i) \\ &=\prod_{l=1}^{L} \prod_{k=1}^{K} (\mu_{lk} \cdot \theta_k)^{N_{lk}} \end{align*}其中 $N_{lk}$ 表示 $N$ 个样本中，$X = a_l$ 并且 $Y=c_k$ 的个数，对似然函数取对数，得： l(\mu) = \sum_{l=1}^{L} \sum_{k=1}^{K}{N_{lk}(ln \mu_{lk} + ln \theta_k)}同样，$\mu_{lk}$ 满足约束条件：$\sum_{l=1}^{L} \mu_{lk} = 1$，使用拉格朗日乘子法，得： \begin{align*} F(\mu,\lambda) &= l(\mu) + \lambda \sum_{k=1}^{K}({\mu_{lk}} -1) \\ & = \sum_{l=1}^{L} \sum_{k=1}^{K} N_{lk}(ln \mu_{lk} + ln \theta_k) + \lambda(\sum_{l=1}^{L} \mu_{lk} - 1) \end{align*}对 $F(\mu, \lambda)$ 求偏导，得：$-\lambda =\frac{N_{lk}}{\mu_{lk}}$，联立 $\sum_{l=1}^{L} \mu_{lk} = 1$ 得： \mu_{lk} = \frac{N_{lk}}{N_k}贝叶斯估计极大似然估计可能出现估计的概率为0，这会影响到后验概率的计算结果，使分类产生偏差。可以采用贝叶斯估计代替极大似然估计。 P_\color{red}\lambda(Y=c_k)=\frac {\sum_{i=1}^N I(y_i=c_k)+\color{red}\lambda} {N+\color{red}{K\lambda}},\quad k=1,2,\ldots,K \\ P_\color{red}\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac {\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)+\color{red}\lambda} {\sum_{i=1}^N I(y_i=c_k)+\color{red}{S_j\lambda}}式中，$\lambda \ge 0$，等价于在随机变量的各个取值的频数上赋予一个正数 $\lambda &gt; 0$，当 $\lambda = 0$时就是极大似然估计，常取 $\lambda = 1$，这时称为拉普拉斯平滑， 参考文献[1]. 统计学习方法 李航]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python多线程编程]]></title>
    <url>%2F2018%2F08%2F19%2FPython%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Python的threading于提供线程相关的操作，线程是应用程序中工作的最小单元。 Python中线程的两种实现Python中线程的调用有两种方式，分为直接调用和继承式调用。直接调用实例如下：1234567891011121314151617181920# 直接调用import threadingimport timedef sayhi(num): #定义每个线程要运行的函数 print("running on number:%s" %num) time.sleep(3)if __name__ == '__main__': t1 = threading.Thread(target=sayhi,args=(1,)) #生成一个线程实例 t2 = threading.Thread(target=sayhi,args=(2,)) #生成另一个线程实例 t1.start() #启动线程 t2.start() #启动另一个线程 print(t1.getName()) #获取线程名 print(t2.getName()) 继承调用示例如下：12345678910111213141516171819202122# 继承式调用import threadingimport timeclass MyThread(threading.Thread): def __init__(self,num): threading.Thread.__init__(self) self.num = num def run(self):#定义每个线程要运行的函数 print("running on number:%s" %self.num) time.sleep(3)if __name__ == '__main__': t1 = MyThread(1) t2 = MyThread(2) t1.start() t2.start() join()：在该线程对象启动了之后调用线程的join()方法，那么主线程将会阻塞在当前位置直到子线程执行完成才继续往下走，如果所有子线程对象都调用了join()方法，那么主线程将会在等待所有子线程都执行完之后再往下执行。 setDaemon(True)：在子线程对象调用start()方法（启动该线程）之前就调用该方法的话，将会将该子线程设置成守护模式启动。即当子线程还在运行的时候，父线程已经执行完了，如果这个子线程设置是以守护模式启动的，那么随着主线程执行完成退出时，子线程立马也退出，如果没有设置守护启动子线程（也就是正常情况下）的话，主线程执行完成之后，进程会等待所有子线程执行完成之后才退出。 互斥锁互斥锁的产生是因为多线程可能访问同一个变量，当多个线程要修改这个变量，很有可能造成数据的错误覆盖，甚至程序崩溃。互斥锁为threading.Lock()，加锁lock。acquire()，释放lock.release()。 递归锁如果公共的临界资源比较多，并且线程间都使用互斥锁去访问临界资源，有可能出现死锁的情况。解决方法是使用递归锁代替。 递归锁的创建是使用threading.RLock()，它里面其实维护了两个东西，一个是Lock，另一个是counter，counter记录了加锁的次数，每加一把锁，counter就会+1，释放一次锁counter就会减一，直到所有加的锁都被释放掉了之后其他线程才能够访问这把锁获取资源。当然这个限制是对于线程之间的，同一个线程中，只要这个线程抢到了这把锁，那么这个线程就可以对这把锁加多个锁，而不会阻塞自己的执行。这就是递归锁的原理。 条件同步变量事件信号量参考文献[1]. python高级之多线程]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[方差与偏差]]></title>
    <url>%2F2018%2F08%2F19%2F%E6%96%B9%E5%B7%AE%E4%B8%8E%E5%81%8F%E5%B7%AE%2F</url>
    <content type="text"><![CDATA[学习算法的预测误差, 或者说泛化误差(generalization error)可以分解为三个部分: 偏差(bias), 方差(variance) 和噪声(noise). 在估计学习算法性能的过程中, 我们主要关注偏差与方差. 因为噪声属于不可约减的误差 (irreducible error)。 偏差、方差与噪声的含义 偏差：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。 方差：度量了同样大小的不同训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。 噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。 分析过程有了直观感受以后, 下面来用公式推导泛化误差与偏差与方差, 噪声之间的关系。 符号 涵义 $\mathbf{x}$ 测试样本 $D$ 数据集 $y_D$ $\mathbf{x}$在数据中的标记 $y$ $\mathbf{x}$的真实标记 $f$ 训练集$D$学得的模型 $f(\mathbf{x};D)$ 由训练集$D$学得的模型$f$对$\mathbf{x}$的预测输出 $\bar{f}(\mathbf{x})$ 模型$f$对$\mathbf{x}$的期望预测输出 在一个训练集 $D$ 上模型 $f$ 对测试样本 $\mathbf{x}$ 的预测输出为 $f(\mathbf{x};D)$, 那么学习算法 $f$ 对测试样本 $\mathbf {x}$ 的 期望预测为: \begin{equation} \overline{f}(\mathbf{x}) = E_D\left[f\left(\mathbf{x}; D\right)\right] \end{equation}上面的期望预测也就是针对不同数据集 $D$, $f$ 对 $\mathbf{x}$ 的预测值取其期望。 使用样本数相同的不同训练集产生的方差为: \begin{equation} var(\mathbf{x}) = E_D\left[\left( f(\mathbf{x}; D) - \overline{f}(\mathbf{x}) \right)^2\right] \end{equation}噪声为真实标记与数据集中的实际标记间的偏差: \begin{equation} \epsilon^2 = E_D\left[ (y_D - y)^2 \right] \end{equation}期望预测与真实标记的误差称为偏差(bias), 为了方便起见, 我们直接取偏差的平方: \begin{equation} bias^2(\mathbf{x}) = \left( \overline{f}(\mathbf{x}) - y \right)^2 \end{equation}为方便讨论，假定噪声期望为零，即 $\mathbb{E}_D[y_D-y]=0$。算法的期望泛化误差为 \begin{eqnarray*} E(f;D) &=& \mathbb{E}_D[(f(x;D)-y_D)^2]\\ &=& \mathbb{E}_D[(f(x;D)-\bar{f}(x)+\bar{f}(x)-y_D)^2]\\ &=& \mathbb{E}_D[(f(x;D)-\bar{f}(x))^2]+\mathbb{E}_D[(\bar{f}(x)-y_D)^2]+\color{red}{\mathbb{E}_D[2(f(x;D)-\bar{f}(x))(\bar{f}(x)-y_D)]}\\ &=& \mathbb{E}_D[(f(x;D)-\bar{f}(x))^2]+\mathbb{E}_D[(\bar{f}(x)-y_D)^2]\\ &=& \mathbb{E}_D[(f(x;D)-\bar{f}(x))^2]+\mathbb{E}_D[(\bar{f}(x)-y+y-y_D)^2]\\ &=& \mathbb{E}_D[(f(x;D)-\bar{f}(x))^2]+\mathbb{E}_D[(\bar{f}(x)-y)^2]+\mathbb{E}_D[(y-y_D)^2]+\color{red}{\mathbb{E}_D[2(\bar{f}(x)-y)(y-y_D)]}\\ &=& \mathbb{E}_D[(f(x;D)-\bar{f}(x))^2]+(\bar{f}(x)-y)^2+\mathbb{E}_D[(y-y_D)^2] \end{eqnarray*}式中，第一个加红公式等于0，因为$(f(x;D)-\bar{f}(x))$与$(\bar{f}(x)-y_D)$相互独立，所以 \mathbb{E}_D[2(f(x;D)-\bar{f}(x))(\bar{f}(x)-y_D)]=2\mathbb{E}_D[(f(x;D)-\bar{f}(x))]\mathbb{E}_D[\bar{f}(x)-y_D)]根据期望预测公式$\bar{f}(x)=\mathbb{E}_D[f(x;D)]$有$\mathbb{E}_D[(f(x;D)-\bar{f}(x))]=0$。同理第二个加红公式等于0，因为噪声期望为0。于是 E(f;D)={bias}^2(x)+var(x)+\varepsilon^2也就是说，泛化误差可分解为偏差、方差与噪声之和。噪声无法人为控制，所以通常我们认为 E(f;D)={bias}^2(x)+var(x)我们希望偏差与方差越小越好, 但实际并非如此. 一般来说, 偏差与方差是有冲突的, 称为偏差-方差窘境 (bias-variance dilemma)。 给定一个学习任务, 在训练初期, 由于训练不足, 学习器的拟合能力不够强, 偏差比较大, 也是由于拟合能力不强, 数据集的扰动也无法使学习器产生显著变化, 也就是欠拟合的情况; 随着训练程度的加深, 学习器的拟合能力逐渐增强, 训练数据的扰动也能够渐渐被学习器学到; 充分训练后, 学习器的拟合能力已非常强, 训练数据的轻微扰动都会导致学习器发生显著变化, 当训练数据自身的、非全局的特性被学习器学到了, 则将发生过拟合. 典型模型分析 KNN对于KNN算法，k值越大，表示模型的学习能力越弱，因为k越大，它越倾向于从“面”上考虑做出判断，而不是具体地考虑一个样本近身的情况来做出判断，所以，它的偏差会越来越大。 RF对于RF，我们实际上是部分实现了多次训练取均值的效果，每次训练得到的树都是一个很强的学习器，每一个的方差都比较大，但综合起来就会比较小。为什么说是部分实现了多次训练取均值的效果而不是全部呢？因为我们在训练各棵树时，是通过抽样样本集来实现多次训练的，不同的训练集中不可避免地会有重合的情况，此时，就不能认为是独立的多次训练了，各个训练得到的树之间的方差会产生一定的相关性，训练集中重合的样本越多，则两棵树之间的方差的相关性越强，就越难达成方差互相抵消的效果。 GBDT对于GBDT，N棵树之间根本就不是一种多次训练取均值的关系，而是N棵树组成了相关关联，层层递进的超级学习者，可想而知，它的方差一定是比较大的。但由于它的学习能力比较强，所以，它的偏差是很小的，而且树的棵树越多，学习能力就越强，偏差就越小。也就是说，只要学习次数够多，预测的均值会无限接近于目标。简单讲就是GBDT的N棵树实际上是一个有机关联的模型，不能认为是N个模型。 参考文献[1] 《机器学习》周志华[2] Understanding the Bias-Variance Tradeoff[3] 误差（error），偏差（bias），方差（variance）]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>方差</tag>
        <tag>偏差</tag>
        <tag>泛化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo记录]]></title>
    <url>%2F2018%2F08%2F15%2FHexo_Note%2F</url>
    <content type="text"><![CDATA[建博客时候的一些记录。 文章按更新时间排序默认按照文章创建时间排序，现在想改成按照更新时间排序。 找到hexo-generator-index/lib/generator.js 对posts.data（储存着所有文章数据）进行自定义排序 12345678910111213posts.data = posts.data.sort(function(a, b) &#123; if(a.top &amp;&amp; b.top) &#123; if(a.top == b.top) return b.date - a.date; else return b.top - a.top; &#125; else if(a.top &amp;&amp; !b.top) &#123; return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.updated - a.updated; &#125;); 默认为b.date - a.date。]]></content>
      <categories>
        <category>杂七杂八</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双指针解题（持续更新）]]></title>
    <url>%2F2018%2F08%2F15%2FTwo_Pointers_Algorithm_Solutions%2F</url>
    <content type="text"><![CDATA[记录一些双指针题目的解题思路，持续更新。 有效三角形的个数 leetcode 611给定一个包含非负整数的数组，你的任务是统计其中可以组成三角形三条边的三元组个数。输入: [2,2,3,4]输出: 3 解答： 从小到大排序，假定nums[k]是最大边，只需之前的i &lt; j &lt; k满足nums[i] + nums[j] &gt; nums[k]即可，两边之差小于第三边自动满足。 旋转数组 leetocde 188给定一个数组，将数组中的元素向右移动 k 个位置，其中 k 是非负数。输入: [1,2,3,4,5,6,7] 和 k = 3。输出: [5,6,7,1,2,3,4]输入: [-1,-100,3,99] 和 k = 2。输出: [3,99,-1,-100] 解答： 123reverse(nums, 0, nums.length - 1);reverse(nums, 0, k - 1);reverse(nums, k, nums.length - 1);]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>lintcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11新特性]]></title>
    <url>%2F2018%2F08%2F15%2FC%2B%2B%2011%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[lambda表达式利用Lambda表达式，可以方便的定义和创建匿名函数，用以替换独立函数或者函数对象，并且使代码更可读。但是从本质上来讲，lambda表达式只是一种语法糖，因为所有其能完成的工作都可以用其它稍微复杂的代码来实现。但是它简便的语法却给C++带来了深远的影响。 我们定义一个可以输出字符串的lambda表达式，表达式一般都是从方括号[]开始，然后结束于花括号{}，花括号里面就像定义函数那样，包含了lamdba表达式体：1234// 定义简单的lambda表达式auto basicLambda = [] &#123; cout &lt;&lt; "Hello, world!" &lt;&lt; endl; &#125;;// 调用basicLambda(); // 输出：Hello, world! 上面是最简单的lambda表达式，没有参数。如果需要参数，那么就要像函数那样，放在圆括号里面，如果有返回值，返回类型要放在-&gt;后面，即拖尾返回类型，当然你也可以忽略返回类型，lambda会帮你自动推断出返回类型：123456// 指明返回类型auto add = [](int a, int b) -&gt; int &#123; return a + b; &#125;;// 自动推断返回类型auto multiply = [](int a, int b) &#123; return a * b; &#125;;int sum = add(2, 5); // 输出：7int product = multiply(2, 5); // 输出：10 参考[1]. C++11 lambda表达式与函数对象[2]. C++11 Lambda表达式]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】如何让lantern不限流量（改mac地址）]]></title>
    <url>%2F2018%2F08%2F15%2FLinux_Mac_Lantern%2F</url>
    <content type="text"><![CDATA[lantern算是比较好的穿墙工具，可每个月只有500M的不限速流量，用完了就很慢，咋办，改Mac地址啊。 ifconfig 列出你的网卡； ifconfig eth0 down #其中eth0是要修改mac地址网卡的名称； 修改mac地址：ifconfig eth0 hw ether 00:00:00:00:00:00 #将后面的一串0替换成你想改成的mac地址即可； 重新启用网卡：ifconfig eth0 up； 上述修改在重启之后就会失效。 如果计算机每次启动都要修改mac地址 下一步任务是修改开机启动项: 在这里就和其他版本系统差别有点大了ubuntu18.04不再使用init.d管理系统，改用了systemd 使用systemd设置开机启动，systemd默认读取/etc/systemd/system下的配置文件，该目录下的文件会链接/lib/systemd/system/下的文件。一般系统安装完/lib/systemd/system/下会有rc-local.service文件，即我们需要的配置文件。 cat /lib/systemd/system/rc-local.service #查看rc-local.service文件 发现[Unit]区块中有个ConditionFileIsExecutable=/etc/rc.local语句rc.local文件和以前的系统很像 去对应目录下找没找到，应该是系统还没创建额外的启动项 创建/etc/rc.local并编辑文件123sudo touch /etc/rc.localsudo chmod 755 /etc/rc.local #赋予可执行权限sudo gedit /etc/rc.local #使用gedit文本编辑器打开此文件进行编辑 在其中增加123sudo /sbin/ifconfig eth0 downsudo /sbin/ifconfig eth0 hw ether 00:AA:BB:CC:DD:EE （你的MAC地址）sudo /sbin/ifconfig eth0 up 到最后以后他就会每次启动计算机的时候自动修改网卡地址。 本文转自ubuntu18.04修改mac地址]]></content>
      <categories>
        <category>杂七杂八</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C++ Primer：重载]]></title>
    <url>%2F2018%2F08%2F15%2FC%2B%2B_Primer_Overload%2F</url>
    <content type="text"></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划解题（持续更新）]]></title>
    <url>%2F2018%2F08%2F15%2FDynamic_Programming_Algorithm_Solutions%2F</url>
    <content type="text"><![CDATA[记录一些动态规划问题的解题思路，会一直持续更新。 背包问题背包问题是动态规划里面的一个大类，包括01背包，完全背包，多重背包等。N 件物品放入容量为 V 的包里，第 i 件物品占用空间 C_i，价值是 W_i。核心递推公式为 dp[i][j] = max(dp[i-1][j], dp[i-1][j-C_i]+W_i)，基本上所有问题都是由这个公式衍生而来。 能装多满 lintcode 92在n个物品中挑选若干物品装入背包，最多能装多满？背包的大小为m，每个物品的小为A[i]。例如：有4个物品[2, 3, 5, 7]，如果背包的大小为11，可以选择[2, 3, 5]装入背包，最多可以装满10的空间。 解答： 用 dp[i][j] 表示前 i 个物品放入 j 容量大小的包里面能装多满，对于 dp[i][j]，有两种选择，第一是将 A[i] 不放入，此时，dp[i][j]=dp[i-1][j]；第二是将 A[i] 放入，此时 dp[i][j]=dp[i-1][j-A[i]]+A[i]。我么当然选择两者中最大的，所以 dp[i][j]=max(dp[i-1][j], dp[i-1][j-A[i]]+A[i])。 优化： 想办法优化一些空间复杂度。注意递推公式，第 i 轮更新只与上一轮即 i-1 轮的值有关，所以我们可以将空间复杂度优化到O(m)，dp[j] 表示容量为 j 的背包能装多满，则 dp[j] = max(dp[j], dp[j - A[i]] + A[i])。注意此时遍历背包容量的时候要从后往前遍历，因为我们是用上一轮的第 j-A[i] 个元素更新本轮的第 j 个元素，从后往前遍历，则保证没有遍历到某元素的时候，此元素之前的元素保持不变。 能装入的最大价值 lintcode 125题目描述：给出n个物品的体积A[i]和其价值V[i]，将他们装入一个大小为m的背包，最多能装入的总价值有多大？例如：对于物品体积[2, 3, 5, 7]和对应的价值[1, 5, 2, 4], 假设背包大小为10的话，最大能够装入的价值为9。 解答： 跟I问题思路一样，更新公式稍作改变：dp[i][j] = max(dp[i-1][j], dp[i-1][j-C_i]+V_i)， Coin Change leetcode 322给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。输入: coins = [1, 2, 5], amount = 11输出: 3解释: 11 = 5 + 5 + 1 解答： 背包问题。用dp[i]表示钱数为i时需要的硬币数，coin[j]表示硬币面值，那么显然dp[i] = min(dp[i], dp[i - coin[j]]) + 1。可见只需从前往后遍历，依次求出dp[i]，最后一个即为所求。注意初始条件dp[coin[j]] = 1。 Coin Change 2 leetcode 518给定不同面额的硬币和一个总金额。写出函数来计算可以凑成总金额的硬币组合数。假设每一种面额的硬币有无限个。输入: amount = 5, coins = [1, 2, 5]输出: 4解释: 有四种方式可以凑成总金额:5=55=2+2+15=2+1+1+15=1+1+1+1+1 解答： 思路跟背包问题很像。定义子问题 dp[i][j] : 用前 i 种硬币组成金额 j 的总组合数;最终目标：dp[n][amount]求 dp[i][j] 时： 不用第 i 种硬币。 用不了：j &lt; coins[i]; 能用但不用。 两种情况都只用前 i-1 种硬币去组成 j， 此时共有 dp[i-1][j] 种组合； 用第 i 种硬币, 此时因为硬币无限，所以它的前一个状态仍能用 i 种硬币，即此时共有 dp[i][j-coins[i]] 种组合。 因此总的组合数是两种情况的和：dp[i][j] = dp[i-1][j] + dp[i][j-coins[i-1]]。 优化： 优化空间，dp[j] 表示用前 i 种银币组成金额 j 的总组合数，在迭代时 dp[j] = dp[j] + dp[j - coins[i]]。 正则表达式匹配 leetcode 10给定一个字符串 (s) 和一个字符模式 (p)。实现支持 . 和 * 的正则表达式匹配。. 匹配任意单个字符。* 匹配零个或多个前面的元素。 解答： dp[i][j]表示s的i长度跟p的j长度是否使用匹配。则有一下情况。 dp[0][0]: 表示当s和p都为空，此时肯定是匹配的，dp[0][0] = 1; 当s为空&quot;&quot;，p为&#39;a*&#39;时，唯一可能match的情况是p匹配为空串，取决于dp[0][j-2]。如: 12s: &quot; &quot; p: &quot;a*b*c*&quot; 当s[i-1] == p[j-1] || p[j-1] == &#39;.&#39; : dp[i][j] = dp[i-1][j-1]； 当p[j-1] == &#39;*&#39;，分下面两种情况讨论： 如果p[j-2] != s[i-1] &amp;&amp; p[j-2] != &#39;.&#39;: 只能把它匹配成一个空串，即0个字符，dp[i][j] = dp[i][j-2]； 否则，可能匹配0个字符，1个字符，多个字符，即dp[i][j] = (dp[i][j-2] || dp[i][j-1] || dp[i-1][j])。 代码中注意动态规划的初始赋值。 最长有效括号 leetcode 32给定一个只包含 ‘(‘ 和 ‘)’ 的字符串，找出最长的包含有效括号的子串的长度。输入: &quot;)()())&quot;输出: 4解释: 最长有效括号子串为 &quot;()()&quot; 解法一：动态规划dp[i]表示一i索引为结尾的最大匹配长度， s[i]为(时：dp[i] = 0 s[i] == &#39;)&#39; &amp;&amp; s[i-1] == &#39;(&#39;：dp[i] = dp[i - 2] + 2； s[i] == &#39;)&#39; &amp;&amp; s[i-1] == &#39;)&#39; &amp;&amp; s[i- dp[i-1] - 1] == &#39;(&#39;：dp[i] = dp[i - dp[i-1] - 2] + dp[i-1] + 2 解答二：栈维持一个(索引的栈，当遇到)时弹出。同时维护一个最左端可以匹配的索引，初始为left = -1。当遇到)，做以下判断： 栈为空时，就left = i，继续遍历，否则执行2； 栈不为空时，先pop一个index出来，然后判断栈是否为空。栈为空时，nMax = max(nMax, i - nLeft)；栈不空空时，nMax = max(nMax, i - stack.top())。 地下城游戏 leetcode 174最初勇士位于左上角，要走到右下角，每次只能向右或向下。格子中的数字表示在这里会补血或者失血多少。正补负失。当血量小于等于0时勇士立即死去，试问确保骑士能够活着到达右下角所需的最低初始血量。例如下表，最佳路径 右 -&gt; 右 -&gt; 下 -&gt; 下，血量至少为7。 -2 -3 3 -5 -10 1 10 30 -5 解答： 用dp[i][j]表示勇士进入(i,j)时至少需要的点数。 假设一种情况：勇士位于一个比较中间的位置(i, j)。先来决定从右走还是从下走。如果从右边走，进入右边时点数至少为8才能最终到达终点；如果从下边走至少需要3点才能最终到达终点。那毫无疑问，我们肯定选择从下面走。而在当前位置，需要失血6点，所以进入当前位置时，血量至少为3-(-6)=9。 从这个过程我们分析可知，当前位置进入时的血量取决于它后面的位置需要的血量和当前位置补血失血情况，所以需要从后往前计算。具体就是dp[i][j] = min(dp[i+1][j], dp[i][j+1]) - current[i][j]，需要注意的是，如果此时计算出来的dp[i][j] &lt;= 0，那就要置为1。 打家劫舍 leetcode 198你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，能够偷窃到的最高金额。输入: [1,2,3,1]输出: 4 解答： dp[i]表示小偷光顾第i个房屋后总共获取的最大金额。显然，第i个房间可以进行两种选择，偷或者不偷。不偷：dp[i] = dp[i-1]；偷：dp[i] = dp[i-2] + nums[i]。当然我们选择金额最大的。则：dp[i] = max(dp[i-1], dp[i-2] + nums[i])。 优化： 观察递推公式，发现下一个值只和上一个、上两个值相关，所以可以将空间优化到O(1)。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>lintcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode与LintCode上没有的基础算法题目]]></title>
    <url>%2F2018%2F08%2F15%2FLeetCode%E4%B8%8ELintCode%E4%B8%8A%E6%B2%A1%E6%9C%89%E7%9A%84%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[本人一直在leetcode和lintcode上刷题，但有些经典的题目这两个平台都没有，遂将题目记录在博客。 贪心算法 汽车加油 Description一辆汽车加满油后可行驶n公里。旅途中有若干个加油站。设计一个有效算法，指出应在哪些加油站停靠加油，使沿途加油次数最少。Input第一行有2个正整数n和k，表示汽车加满油后可行驶n公里，且旅途中有k个加油站。接下来的1 行中，有k+1个整数，表示第k个加油站与第k-1个加油站之间的距离。第0个加油站表示出发地，汽车已加满油。第k+1个加油站表示目的地。Output将计算出的最少加油次数输出。如果无法到达目的地，则输出”No Solution”。Sample Input7 71 2 3 4 5 1 6 6Sample Output4 12345678910111213141516171819int MinRefuelingNum(int n, vector&lt;int&gt; nums)&#123; int res = 0; int nCanReach = 0; for (int i = 1; i &lt; nums.size(); i++) &#123; if (nums[i] &gt; n) &#123; return -1; &#125; nCanReach += nums[i]; if (nCanReach &gt; n) &#123; res++; nCanReach = nums[i]; &#125; &#125; return res;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划总结]]></title>
    <url>%2F2018%2F08%2F08%2F%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下使用GDB调试]]></title>
    <url>%2F2018%2F08%2F04%2FLinux_Gdb%2F</url>
    <content type="text"><![CDATA[GDB 是Linux下一个用来调试 C 和 C++ 程序的强力调试器，它使你能在程序运行时观察程序的内部结构和内存的使用情况。以下是 gdb 所提供的一些功能: 设置断点 监视程序变量的值 程序的单步执行 修改变量的值 GDB支持C, C++ ,FORTRAN, PACAL, Java, Chill, assembly, Modula2。 一般来说，GDB会根据调试的程序来确定的相应的调试语言，比如说扩展名为.c, GDB should it is a c programme, extern_name is .c, .cc, .cp, .cxx, .cpp, .c++, GDB should they are c++ programme。 在可以使用GDB调试程序之前，必须使用-g选项编译源文件。例如一个名字命名为test.cpp, 编译时使用如下语句：1g++ -o test -g test.cpp -o表示输出的意思 在命令行上键入gdb并按回车键就可以运行GDB了。 gdb命令描述： 启动、退出、帮助 file FILE 装入想要调试的可执行文件 kill 终止正在调试的程序 quit(q) 退出gdb 执行 next(n) 执行一行源代码但不进入函数内部 step(s) 执行一行源代码而且进入函数内部 run(r) 执行当前被调试的程序 continue(c) 继续运行 断点 break(b) 设置断点 delete 删除断点 disable 禁用断点 enable 启用断点1234567break 3 # 第3行设置断点b 3 # 第3行设置断点br 3 # 第3行设置断点delete br # 删除所有断点delete br 3 # 删除编号为3的断点disable br 3 # 禁用编号为3的断点enable br 3 # 启用编号为3的断点 信息、打印、显示 info(i) 查看一些信息 print(p) 打印表达式、变量的值 list(l) 列出附近的代码，继续l或回车列出接下来的代码 display 显示变量的值 undisplay 取消显示123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384info locals # 查看局部变量的值info br # 查看所有的断点print mpNums # 打印mpNums的值l # 列出附近的10行代码l 10, 40 # 列出10-40行的代码l 30 # 列出30行附近的10行代码l - # 列出上一个l命令前的10行代码l FunName # 列出这个函数附近的代码l test.cpp main # 列出一个文件中某个函数附近的代码display mpNums # 每次停下的时候显示mpNums的值undisplay 2 # 取消显示编号为2的display``` # watch backtracewatch通常需要和break，run，continue联合使用。```shell(gdb) b main # 设置断点Breakpoint 15 at 0x555555554f59: file test.cpp, line 37.(gdb) r # 运行Starting program: /home/pengfei/MyCode/test.out Breakpoint 15, main (argc=1, argv=0x7fffffffdca8) at test.cpp:37 # 中断37 &#123;(gdb) watch nTest # 设置观察变量nTestHardware watchpoint 16: nTest(gdb) cContinuing.here begin!!!Hardware watchpoint 16: nTest# 在40行的时候nTest发生变化Old value = &#123;-136423008, 32767, 0, 0, 1431665312&#125;New value = &#123;1, 32767, 0, 0, 1431665312&#125;0x0000555555554f97 in main (argc=1, argv=0x7fffffffdca8) at test.cpp:4040 int nTest[5] = &#123;1, 3, 5, 9, 2&#125;;(gdb) cContinuing.Hardware watchpoint 16: nTestOld value = &#123;1, 32767, 0, 0, 1431665312&#125;New value = &#123;1, 3, 0, 0, 1431665312&#125;0x0000555555554f9e in main (argc=1, argv=0x7fffffffdca8) at test.cpp:4040 int nTest[5] = &#123;1, 3, 5, 9, 2&#125;;(gdb) cContinuing.Hardware watchpoint 16: nTestOld value = &#123;1, 3, 0, 0, 1431665312&#125;New value = &#123;1, 3, 5, 0, 1431665312&#125;0x0000555555554fa5 in main (argc=1, argv=0x7fffffffdca8) at test.cpp:4040 int nTest[5] = &#123;1, 3, 5, 9, 2&#125;;(gdb) cContinuing.Hardware watchpoint 16: nTestOld value = &#123;1, 3, 5, 0, 1431665312&#125;New value = &#123;1, 3, 5, 9, 1431665312&#125;0x0000555555554fac in main (argc=1, argv=0x7fffffffdca8) at test.cpp:4040 int nTest[5] = &#123;1, 3, 5, 9, 2&#125;;(gdb) cContinuing.Hardware watchpoint 16: nTestOld value = &#123;1, 3, 5, 9, 1431665312&#125;New value = &#123;1, 3, 5, 9, 2&#125;main (argc=1, argv=0x7fffffffdca8) at test.cpp:4141 vector&lt;int&gt; vtTest(nTest, nTest + 5);(gdb) cContinuing.fun beginreturn1 2 here end!!!Watchpoint 16 deleted because the program has left the block inwhich its expression is valid.__libc_start_main (main=0x555555554f40 &lt;main(int, char**)&gt;, argc=1, argv=0x7fffffffdca8, init=&lt;optimized out&gt;, fini=&lt;optimized out&gt;, rtld_fini=&lt;optimized out&gt;, stack_end=0x7fffffffdc98) at ../csu/libc-start.c:344344 ../csu/libc-start.c: 没有那个文件或目录. backtrace(bt) 查看函数的栈帧]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>gdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python的垃圾回收机制]]></title>
    <url>%2F2018%2F08%2F02%2FPython%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[python里也同java一样采用了垃圾收集机制。python采用了引用计数机制，使用标记-清除和分代收集两种策略进行垃圾回收。 引用计数机制与标记清除策略：python里每一个东西都是对象，它们的核心就是一个结构体：PyObject12345typedef struct_object &#123; int ob_refcnt; struct_typeobject *ob_type;&#125; PyObject; PyObject是每个对象必有的内容，其中ob_refcnt就是做为引用计数。当一个对象有新的引用时，它的ob_refcnt就会增加，当引用它的对象被删除，它的ob_refcnt就会减少。当引用计数为0时，该对象生命就结束了。 引用计数机制的优点： 简单 实时性：一旦没有引用，内存就直接释放了。不用像其他机制等到特定时机。实时性还带来一个好处：处理回收内存的时间被分摊 引用计数机制的缺点： 维护引用计数消耗资源 循环引用 循环引用例子： 1234l1 = []l2 = []l1.append(l2)l2.append(l1) 导致引用计数+1的情况: 对象被创建，例如a=23 对象被引用，例如b=a 对象被作为参数，传入到一个函数中，例如func(a) 对象作为一个元素，存储在容器中，例如list1=[a,a] 导致引用计数-1的情况 对象的别名被显式销毁，例如del a 对象的别名被赋予新的对象，例如a=24 一个对象离开它的作用域，例如f函数执行完毕时，func函数中的局部变量（全局变量不会） 对象所在的容器被销毁，或从容器中删除对象 一个对象的引用数为零的时候，就要对它进行回收。这个就是标记清除的基本思路。但是对于循环引用的情况，这种方法就遇到困难了，这里就需要分代回收机制。 分代回收分代回收是一种以空间换时间的操作方式，Python将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，Python将内存分为了3“代”，分别为第0代、第1代、第2代，他们对应的是3个链表。 每个代的threshold值表示该代最多容纳对象的个数。对象多到该阈值时，就对这些对象做一次检查和清理，没有被清理的那些就存活下来，进入第1代，第一代检查做若干次后，对1代清理，存活下来的进入第2代，第二代也是如此。这样就实现了分代回收的操作。 参考文献[1]. Python垃圾回收机制—完美讲解![2]. Python垃圾回收机制详解]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>垃圾回收</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[归一化与标准化]]></title>
    <url>%2F2018%2F08%2F02%2F%E5%BD%92%E4%B8%80%E5%8C%96%E4%B8%8E%E6%A0%87%E5%87%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[归一化和标准化经常被搞混。 归一化归一化是指对不同维度进行伸缩变换，把数据映射到[0,1]之间。 x^* = \frac{x-min(x)}{max(x_i) - min(x_i)}标准化标准化是指将不同维度的数值特征缩放成均值为0，标准差为1的状态。 x^* = \frac{x-\mu}{\sigma}其中，其中$\mu$是样本的均值，$\sigma$是样本的标准差。 两个坑 归一化和标准化的相同点都是对某个特征（column）进行缩放（scaling）而不是对某个样本的特征向量（row）进行缩放。对特征向量进行缩放是毫无意义的，比如三列特征：身高、体重、血压。每一条样本（row）就是三个这样的值，对这个row无论是进行标准化还是归一化都毫无意义，因为你不能将身高、体重和血压混到一起去。 在线性代数中，将一个向量除以向量的长度，也被称为标准化。不过这里的标准化是将向量变为长度为1的单位向量，它和我们这里的标准化不是一回事儿，不要搞混。 标准化与归一化的对比分析首先明确，在机器学习中，标准化是更常用的手段，归一化的应用场景是有限的。总结原因有两点： 标准化更好保持了样本间距当样本中有异常点时，归一化有可能将正常的样本“挤”到一起去。比如三个样本，某个特征的值为1,2,10000，假设10000这个值是异常值，用归一化的方法后，正常的1,2就会被“挤”到一起去。如果不幸的是1和2的分类标签还是相反的，那么，当我们用梯度下降来做分类模型训练时，模型会需要更长的时间收敛，因为将样本分开需要更大的努力。而标准化在这方面就做得很好，至少它不会将样本“挤到一起”。 标准化更符合统计学假设对一个数值特征来说，很大可能它是服从正态分布的。标准化其实是基于这个隐含假设，只不过是略施小技，将这个正态分布调整为均值为0，方差为1的标准正态分布而已。 逻辑回归必须要进行标准化吗？这个问题无论你回答必须或者不必须，你都是错的！！！哈哈。真正的答案是，这取决于我们的逻辑回归是不是用正则化项。 如果不用正则，标准化并不是必须的；如果用正则，那么标准化是必须的。 因为不用正则时，我们的损失函数只是仅仅在度量预测与真实的差距，加上正则后，我们的损失函数除了要度量上面的差距外，还要度量参数值是否足够小。而参数值的大小程度或者说大小的级别是与特征的数值范围相关的。 举例来说，我们用体重预测身高，体重用kg衡量时，训练出的模型是：身高 = 体重*x。x就是我们训练出来的参数。当我们的体重用吨来衡量时，x的值就会扩大为原来的1000倍。在上面两种情况下，都用L1正则的话，显然对模型的训练影响是不同的。 假如不同的特征的数值范围不一样，有的是0到0.1，有的是100到10000，那么，每个特征对应的参数大小级别也会不一样，在L1正则时，我们是简单将参数的绝对值相加，因为它们的大小级别不一样，就会导致L1最后只会对那些级别比较大的参数有作用，那些小的参数都被忽略了。 如果不用正则，那么标准化对逻辑回归有什么好处吗？答案是有好处，进行标准化后，我们得出的参数值的大小可以反应出不同特征对样本label的贡献度，方便我们进行特征筛选。如果不做标准化，是不能这样来筛选特征的。 做标准化有什么注意事项吗？最大的注意事项就是先拆分出test集，不要在整个数据集上做标准化，因为那样会将test集的信息引入到训练集中，导致训练集和测试集不独立。正确的做法是记录训练集的标准化方法，独立的对测试集进行标准化。 决策树需要标准化吗？不需要。因为决策树不需要考虑特征的值，只需要考虑划分界限。比如ID3 C4.5 CART 的熵和基尼系数，研究对象只是概率分布，对数值不关心。 参考文献[1]. 回归过程中的数据标准化[2]. 机器学习面试之归一化与标准化]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>归一化</tag>
        <tag>标准化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】注意力机制]]></title>
    <url>%2F2018%2F07%2F29%2F%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[以英语 - 法语翻译为例，给定一对英语输入序列“They”、“are”、“watching”、“.”和法语输出序列“Ils”、“regardent”、“.”。解码器可以在输出序列的时间步 1 使用更集中编码了“They”、“are”信息的背景变量来生成“Ils”，在时间步 2 使用更集中编码了“watching”信息的背景变量来生成“regardent”，在时间步 3 使用更集中编码了“.”信息的背景变量来生成“.”。这看上去就像是在解码器的每一时间步对输入序列中不同时间步编码的信息分配不同的注意力。这也是注意力机制的由来。它最早由 Bahanau 等提出。 设计我们对“编码器—解码器（seq2seq）”里的解码器稍作修改。在时间步 $t$’，设解码器的背景变量为 $\boldsymbol{c}_{t’}$，输出 $y_{t’}$ 的特征向量为 $\boldsymbol{y}_{t’}$。 和输入的特征向量一样，这里每个输出的特征向量也是模型参数。解码器在时间步 $t’$ 的隐藏状态 \boldsymbol{s}_{t'} = g(\boldsymbol{y}_{t'-1}, \boldsymbol{c}_{t'}, \boldsymbol{s}_{t'-1})令编码器在时间步 $t$ 的隐藏状态为 $\boldsymbol{h}_t$，且总时间步数为 $T$。解码器在时间步 $t’$ 的背景变量为 \boldsymbol{c}_{t'} = \sum_{t=1}^T \alpha_{t' t} \boldsymbol{h}_t其中 $\alpha_{t’ t}$ 是权值。也就是说，给定解码器的当前时间步 $t’$，我们需要对编码器中不同时间步 $t$ 的隐藏状态求加权平均。这里的权值也称注意力权重。它的计算公式是 \alpha_{t' t} = \frac{\exp(e_{t' t})}{ \sum_{k=1}^T \exp(e_{t' k}) }其中 $e_{t’ t} \in \mathbb{R}$ 的计算为 e_{t' t} = a(\boldsymbol{s}_{t' - 1}, \boldsymbol{h}_t)上式中的函数 $a$ 有多种设计方法。Bahanau 等使用了多层感知机： e_{t' t} = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_h \boldsymbol{h}_t)其中 $\boldsymbol{v}、\boldsymbol{W}_s、\boldsymbol{W}_h$ 以及编码器与解码器中的各个权重和偏差都是模型参数。 Bahanau 等在编码器和解码器中分别使用了门控循环单元。在解码器中，我们需要对门控循环单元的设计稍作修改。解码器在 $t’$ 时间步的隐藏状态为 \boldsymbol{s}_{t'} = \boldsymbol{z}_{t'} \odot \boldsymbol{s}_{t'-1} + (1 - \boldsymbol{z}_{t'}) \odot \tilde{\boldsymbol{s}}_{t'}其中的重置门、更新门和候选隐含状态分别为 \begin{split}\begin{aligned} \boldsymbol{r}_{t'} &= \sigma(\boldsymbol{W}_{yr} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{sr} \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_{cr} \boldsymbol{c}_{t'} + \boldsymbol{b}_r),\\ \boldsymbol{z}_{t'} &= \sigma(\boldsymbol{W}_{yz} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{sz} \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_{cz} \boldsymbol{c}_{t'} + \boldsymbol{b}_z),\\ \tilde{\boldsymbol{s}}_{t'} &= \text{tanh}(\boldsymbol{W}_{ys} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{ss} (\boldsymbol{s}_{t' - 1} \odot \boldsymbol{r}_{t'}) + \boldsymbol{W}_{cs} \boldsymbol{c}_{t'} + \boldsymbol{b}_s) \end{aligned}\end{split}本文转自注意力机制]]></content>
      <categories>
        <category>深度学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】seq2seq]]></title>
    <url>%2F2018%2F07%2F28%2Fseq2seq%2F</url>
    <content type="text"><![CDATA[在很多应用中，输入和输出都可以是不定长序列。以机器翻译为例，输入可以是一段不定长的英语文本序列，输出可以是一段不定长的法语文本序列，例如 英语输入：“They”、“are”、“watching”、“.” 法语输出：“Ils”、“regardent”、“.” 当输入输出都是不定长序列时，我们可以使用编码器—解码器（encoder-decoder） 或者 seq2seq 模型。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器对应输入序列，解码器对应输出序列。 下图描述了使用编码器—解码器将上述英语句子翻译成法语句子的一种方法。在训练数据集中，我们可以在每个句子后附上特殊符号&lt;eos&gt;（end of sequence）表示序列的终止。编码器每个时间步的输入依次为英语句子中的单词、标点和特殊符号&lt;eos&gt;。下图使用了编码器在最终时间步的隐藏状态作为输入句子的编码信息。解码器在各个时间步中使用输入句子的编码信息和上个时间步的输出以及隐藏状态作为输入。 我们希望解码器在各个时间步能正确依次输出翻译后的法语单词、标点和特殊符号&lt;eos&gt;。 需要注意的是，解码器在最初时间步的输入用到了一个表示序列开始的特殊符号&lt;bos&gt;（beginning of sequence）。 使用编码器—解码器将句子由英语翻译成法语。编码器和解码器分别为循环神经网络。 接下来我们介绍编码器和解码器的定义。 编码器编码器的作用是把一个不定长的输入序列变换成一个定长的背景变量 $\boldsymbol{c}$，并在该背景变量中编码输入序列信息。常用的编码器是循环神经网络。 让我们考虑批量大小为 1 的时序数据样本。假设输入序列是 $x_1,\ldots,x_T$，例如 $x_i$ 是输入句子中的第 $i$ 个词。在时间步 $t$，循环神经网络将输入 $x_t$ 的特征向量 $\boldsymbol{x}_t$ 和上个时间步的隐藏状态 $\boldsymbol{h}_{t-1}$ 变换为当前时间步的隐藏状态 $\boldsymbol{h}_t$。我们可以用函数 $f$ 表达循环神经网络隐藏层的变换： \boldsymbol{h}_t = f(\boldsymbol{x}_t, \boldsymbol{h}_{t-1}).这里特征向量 $\boldsymbol{x}_t$ 既可以是需要学习的词向量，也可以是预训练的词向量。假设输入序列的总时间步数为 $T$。接下来编码器通过自定义函数 $q$ 将各个时间步的隐藏状态变换为背景变量 \boldsymbol{c} = q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T)例如，当选择 $q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T) = \boldsymbol{h}_T$ 时，背景变量是输入序列最终时间步的隐藏状态 $\boldsymbol{h}_T$。 以上描述的编码器是一个单向的循环神经网络，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用双向循环神经网络构造编码器。这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。 解码器刚刚已经介绍，编码器输出的背景变量 $\boldsymbol{c}$ 编码了整个输入序列 $x_1, \ldots, x_T$ 的信息。给定训练样本中的输出序列 $y_1, y_2, \ldots, y_{T’}$，对每个时间步 $t’$，解码器输出 $y_{t’}$ 的条件概率将基于之前的输出序列 $y_1,\ldots,y_{t’-1}$ 和背景变量 $\boldsymbol{c}$，即 $\mathbb{P}(y_{t’} \mid y_1, \ldots, y_{t’-1}, \boldsymbol{c})$。 为此，我们可以使用另一个循环神经网络作为解码器。 在输出序列的时间步 $t^\prime$，解码器将上一时间步的输出 $y_{t^\prime-1}$ 以及背景变量 $\boldsymbol{c}$ 作为输入，并将它们与上一时间步的隐藏状态 $\boldsymbol{s}_{t^\prime-1}$ 变换为当前时间步的隐藏状态 $\boldsymbol{s}_{t^\prime}$。因此，我们可以用函数 $g$ 表达解码器隐藏层的变换： \boldsymbol{s}_{t^\prime} = g(y_{t^\prime-1}, \boldsymbol{c}, \boldsymbol{s}_{t^\prime-1})有了解码器的隐藏状态后，我们可以使用自定义的输出层和 softmax 运算来计算 $\mathbb{P}(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \boldsymbol{c})$，例如基于当前时间步的解码器隐藏状态 $\boldsymbol{s}_{t^\prime}$、上一时间步的输出 $y_{t^\prime-1}$ 以及背景变量 $\boldsymbol{c}$ 来计算当前时间步输出 $y_{t^\prime}$ 的概率分布。 模型训练根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率 \begin{split}\begin{aligned} \mathbb{P}(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T) &= \prod_{t'=1}^{T'} \mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, x_1, \ldots, x_T)\\ &= \prod_{t'=1}^{T'} \mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c}) \end{aligned}\end{split}并得到该输出序列的损失 - \log\mathbb{P}(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T) = -\sum_{t'=1}^{T'} \log \mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})在模型训练中，我们通过最小化这个损失函数来得到模型参数。 本文转自：编码器—解码器（seq2seq）]]></content>
      <categories>
        <category>深度学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】Glove和fastText]]></title>
    <url>%2F2018%2F07%2F25%2FGlove%E5%92%8CfastText%2F</url>
    <content type="text"><![CDATA[在 word2vec 被提出以后，很多其他词嵌入模型也陆续发表。本文介绍其中比较有代表性的两个模型。它们分别是由斯坦福团队发表的 GloVe 和由 Facebook 团队发表的 fastText。 GloVeGloVe 使用了词与词之间的共现（co-occurrence）信息。定义 $\boldsymbol{X}$ 为共现词频矩阵，其中元素 $x_{ij}$ 为词 $j$ 出现在词 $i$ 的背景的次数。这里的“背景”有多种可能的定义。举个例子，在一段文本序列中，如果词 $j$ 出现在词 $i$ 左边或者右边不超过 10 个词的距离，我们可以认为词 $j$ 出现在词 $i$ 的背景一次。令 $x_i = \sum_k x_{ik}$ 为任意词出现在词 $i$ 的背景的次数。那么， p_{ij} = \mathbb{P}(j \mid i) = \frac{x_{ij}}{x_i}为词 $j$ 在词 $i$ 的背景中出现的条件概率。这一条件概率也称词 $i$ 和词 $j$ 的共现概率。 共现概率比值GloVe 论文里展示了以下词对的共现概率与比值： $\mathbb{P}(k \mid \text{ice})：0.00019（k= solid），0.000066（k= gas），0.003（k= water），0.000017（k= fashion）$ $\mathbb{P}(k \mid \text{steam})：0.000022（k= solid），0.00078（k= gas），0.0022（k= water），0.000018（k= fashion）$ $\mathbb{P}(k \mid \text{ice}) / \mathbb{P}(k \mid \text{steam})：8.9（k= solid），0.085（k= gas），1.36（k= water），0.96（k= fashion）$ 我们可以观察到以下现象： 对于与 ice（冰）相关而与 steam（蒸汽）不相关的词 k，例如 k=solid（固体），我们期望共现概率比值 $p_{ik}/p_{jk}$ 较大，例如上面最后一行结果中的值 8.9。 对于与 ice 不相关而与 steam 相关的词 k，例如 k=gas（气体），我们期望共现概率比值 $p_{ik}/p_{jk}$ 较小，例如上面最后一行结果中的值 0.085。 对于与 ice 和 steam 都相关的词 k，例如 k=water（水），我们期望共现概率比值 $p_{ik}/p_{jk}$ 接近 1，例如上面最后一行结果中的值 1.36。 对于与 ice 和 steam 都不相关的词 k，例如 k=fashion（时尚），我们期望共现概率比值 $p_{ik}/p_{jk}$ 接近 1，例如上面最后一行结果中的值 0.96。 由此可见，共现概率比值能比较直观地表达词与词之间的关系。GloVe 试图用有关词向量的函数来表达共现概率比值。 用词向量表达共现概率比值GloVe 的核心思想在于使用词向量表达共现概率比值。而任意一个这样的比值需要三个词 $i、j$ 和 $k$ 的词向量。对于共现概率 $p_{ij} = \mathbb{P}(j \mid i)$，我们称词 $i$ 和词 $j$ 分别为中心词和背景词。我们使用 $\boldsymbol{v}_i 和 \tilde{\boldsymbol{v}}_i$ 分别表示词 $i$ 作为中心词和背景词的词向量。 我们可以用有关词向量的函数 $f$ 来表达共现概率比值： f(\boldsymbol{v}_i, \boldsymbol{v}_j, \tilde{\boldsymbol{v}}_k) = \frac{p_{ik}}{p_{jk}}需要注意的是，函数 $f$ 可能的设计并不唯一。下面我们考虑一种较为合理的可能性。 首先，用向量之差来表达共现概率的比值，并将上式改写成 f(\boldsymbol{v}_i - \boldsymbol{v}_j, \tilde{\boldsymbol{v}}_k) = \frac{p_{ik}}{p_{jk}}由于共现概率比值是一个标量，我们可以使用向量之间的内积把函数 $f$ 的自变量进一步改写，得到 f((\boldsymbol{v}_i - \boldsymbol{v}_j)^\top \tilde{\boldsymbol{v}}_k) = \frac{p_{ik}}{p_{jk}}由于任意一对词共现的对称性，我们希望以下两个性质可以同时被满足： 任意词作为中心词和背景词的词向量应该相等：对任意词 $i$，$\boldsymbol{v}_i = \tilde{\boldsymbol{v}}_i$。 词与词之间共现词频矩阵 $\boldsymbol{X}$ 应该对称：对任意词 $i$ 和 $j$，$x_{ij} = x_{ji}$。 为了满足以上两个性质，一方面，我们令 f((\boldsymbol{v}_i - \boldsymbol{v}_j)^\top \tilde{\boldsymbol{v}}_k) = \frac{f(\boldsymbol{v}_i^\top \tilde{\boldsymbol{v}}_k)}{f(\boldsymbol{v}_j^\top \tilde{\boldsymbol{v}}_k)}并得到 $f(x) = \text{exp}(x)$。以上两式的右边联立 f(\boldsymbol{v}_i^\top \tilde{\boldsymbol{v}}_k) = \exp(\boldsymbol{v}_i^\top \tilde{\boldsymbol{v}}_k) = p_{ik} = \frac{x_{ik}}{x_i}由上式可得 \boldsymbol{v}_i^\top \tilde{\boldsymbol{v}}_k = \log(p_{ik}) = \log(x_{ik}) - \log(x_i)另一方面，我们可以把上式中 $\log(x_i)$ 替换成两个偏差项之和 $b_i + \tilde{b}_k$，得到 \boldsymbol{v}_i^\top \tilde{\boldsymbol{v}}_k = \log(x_{ik}) - b_i - \tilde{b}_k因此，对于任意一对词 $i$ 和 $j$，我们可以用它们的词向量表达共现词频的对数： \boldsymbol{v}_i^\top \tilde{\boldsymbol{v}}_j + b_i + \tilde{b}_j = \log(x_{ij})损失函数GloVe 中的共现词频是直接在训练数据上统计得到的。为了学习词向量和相应的偏差项，我们希望上式中的左边与右边尽可能接近。给定词典 $\mathcal{V}$ 和权重函数 $h(x_{ij})$，GloVe 的损失函数为 \sum_{i \in \mathcal{V}, j \in \mathcal{V}} h(x_{ij}) \left(\boldsymbol{v}_i^\top \tilde{\boldsymbol{v}}_j + b_i + \tilde{b}_j - \log(x_{ij})\right)^2其中权重函数 $h(x)$ 的一个建议选择是，当 $x &lt; c$（例如 c = 100），令 $h(x) = (x/c)^\alpha$（例如 $\alpha = 0.75$），反之令 $h(x) = 1$。由于权重函数的存在，损失函数的计算复杂度与共现词频矩阵 $\boldsymbol{X}$ 中非零元素的数目呈正相关。我们可以从 $\boldsymbol{X}$ 中随机采样小批量非零元素，并使用优化算法迭代共现词频相关词的向量和偏差项。 我们提到过，任意词的中心词向量和背景词向量是等价的。但由于初始化值的不同，同一个词最终学习到的两组词向量可能不同。当学习得到所有词向量以后，GloVe 使用一个词的中心词向量与背景词向量之和作为该词的最终词向量。 fastText之前介绍了 word2vec 的跳字模型和负采样。fastText 以跳字模型为基础，将每个中心词视为子词（subword）的集合，并使用负采样学习子词的词向量。因此，fastText 是一个子词嵌入模型。 举个例子，设子词长度为 3 个字符，where的子词包括&lt;wh、whe、her、ere、re&gt;和特殊子词（整词）&lt;where&gt;。这些子词中的&lt;和&gt;符号是为了将作为前后缀的子词区分出来。并且，这里的子词her与整词&lt;her&gt;也可被区分开。给定一个词 w，我们通常可以把字符长度在 3 到 6 之间的所有子词和特殊子词的并集 $\mathcal{G}_w$ 取出。假设词典中任意子词 $g$ 的子词向量为 $\boldsymbol{z}_g$，我们可以把使用负采样的跳字模型的损失函数 - \text{log} \mathbb{P} (w_o \mid w_c) = -\text{log} \frac{1}{1+\text{exp}(-\boldsymbol{u}_o^\top \boldsymbol{v}_c)} - \sum_{k=1, w_k \sim \mathbb{P}(w)}^K \text{log} \frac{1}{1+\text{exp}(\boldsymbol{u}_{i_k}^\top \boldsymbol{v}_c)}直接替换成 - \text{log} \mathbb{P} (w_o \mid w_c) = -\text{log} \frac{1}{1+\text{exp}(-\boldsymbol{u}_o^\top \sum_{g \in \mathcal{G}_{w_c}} \boldsymbol{z}_g)} - \sum_{k=1, w_k \sim \mathbb{P}(w)}^K \text{log} \frac{1}{1+\text{exp}(\boldsymbol{u}_{i_k}^\top \sum_{g \in \mathcal{G}_{w_c}} \boldsymbol{z}_g)}可以看到，原中心词向量被替换成了中心词的子词向量之和。与 word2vec 和 GloVe 不同，词典以外的新词的词向量可以使用 fastText 中相应的子词向量之和。 fastText 对于一些语言较重要，例如阿拉伯语、德语和俄语。例如，德语中有很多复合词，例如乒乓球（英文 table tennis）在德语中叫“Tischtennis”。fastText 可以通过子词表达两个词的相关性，例如“Tischtennis”和“Tennis”。 由于词是自然语言的基本表义单元，词向量广泛应用于各类自然语言处理的场景中。例如，我们可以在之前介绍的语言模型中使用在更大规模语料上预训练的词向量，从而提升语言模型的准确性。 本文转自 词嵌入：GloVe 和 fastText]]></content>
      <categories>
        <category>深度学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>Glove</tag>
        <tag>fastText</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】word2vec]]></title>
    <url>%2F2018%2F07%2F23%2Fword2vec%2F</url>
    <content type="text"><![CDATA[2013 年，Google 团队发表了 word2vec 工具。word2vec 工具主要包含两个模型：跳字模型（skip-gram）和连续词袋模型（continuous bag of words，简称 CBOW），以及两种近似训练法：负采样（negative sampling）和层序 softmax（hierarchical softmax）。值得一提的是，word2vec 的词向量可以较好地表达不同词之间的相似和类比关系。 词嵌入自然语言是一套用来表达含义的复杂系统。在这套系统中，词是表义的基本单元。词向量是用来表示词的向量，也可被认为是词的特征向量。这通常需要把维数为词典大小的高维空间嵌入到一个更低维数的连续向量空间。把词映射为实数域上向量的技术也叫词嵌入（word embedding）。近年来，词嵌入已逐渐成为自然语言处理的基础知识。那么，我们应该如何使用向量表示词呢？ 为何不用one-hot向量使用 one-hot 词向量通常并不是一个好选择。一个主要的原因是，one-hot 词向量无法表达不同词之间的相似度，例如余弦相似度。由于任意一对向量$\boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^d$的余弦相似度为 \frac{\boldsymbol{x}^\top \boldsymbol{y}}{\|\boldsymbol{x}\| \|\boldsymbol{y}\|} \in [-1, 1]任何一对词的 one-hot 向量的余弦相似度都为 0。 word2vecword2vec 工具主要包含跳字模型和连续词袋模型。下面将分别介绍它们。 跳字模型在跳字模型中，我们用一个词来预测它在文本序列周围的词。举个例子，假设文本序列是“the”、“man”、“loves”、“his”和“son”。以“loves”作为中心词，设时间窗口大小为 2。跳字模型所关心的是，给定中心词“loves”生成与它距离不超过 2 个词的背景词“the”、“man”、“his”和“son”的条件概率。 假设词典索引集 $\mathcal{V}$ 的大小为 $|\mathcal{V}|$，且 $\mathcal{V} = \{0, 1, \ldots, |\mathcal{V}|-1\}$。给定一个长度为 $T$ 的文本序列中，时间步 $t$ 的词为 $w^{(t)}$。当时间窗口大小为 $m$ 时，跳字模型需要最大化给定任一中心词生成所有背景词的概率 \prod_{t=1}^T \prod_{-m \leq j \leq m, j \neq 0} \mathbb{P}(w^{(t+j)} \mid w^{(t)})上式的最大似然估计与最小化以下损失函数等价： -\frac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} \text{log} \mathbb{P}(w^{(t+j)} \mid w^{(t)})我们可以用 $\mathbf{v}$ 和 $\mathbf{u}$ 分别表示中心词和背景词的向量。换言之，对于词典中索引为 $i$ 的词，它在作为中心词和背景词时的向量表示分别是 $\mathbf{v}_i$ 和 $\mathbf{u}_i$。而词典中所有词的这两种向量正是跳字模型所要学习的模型参数。为了将模型参数植入损失函数，我们需要使用模型参数表达损失函数中的给定中心词生成背景词的条件概率。给定中心词，假设生成各个背景词是相互独立的。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_o$ 在词典中索引为 $o$，损失函数中的给定中心词生成背景词的条件概率可以通过 softmax 函数定义为 \mathbb{P}(w_o \mid w_c) = \frac{\text{exp}(\mathbf{u}_o^\top \mathbf{v}_c)}{ \sum_{i \in \mathcal{V}} \text{exp}(\mathbf{u}_i^\top \mathbf{v}_c)}当序列长度 $T$ 较大时，我们通常在每次迭代时随机采样一个较短的子序列来计算有关该子序列的损失。然后，根据该损失计算词向量的梯度并迭代词向量。 作为一个具体的例子，下面我们看看如何计算随机采样的子序列的损失有关中心词向量的梯度。和上面提到的长度为 $T$ 的文本序列的损失函数类似，随机采样的子序列的损失实际上是对子序列中给定中心词生成背景词的条件概率的对数求平均。通过微分，我们可以得到上式中条件概率的对数有关中心词向量 $\mathbf{v}_c$ 的梯度 \frac{\partial \text{log} \mathbb{P}(w_o \mid w_c)}{\partial \mathbf{v}_c} = \mathbf{u}_o - \sum_{j \in \mathcal{V}} \frac{\text{exp}(\mathbf{u}_j^\top \mathbf{v}_c)}{ \sum_{i \in \mathcal{V}} \text{exp}(\mathbf{u}_i^\top \mathbf{v}_c)} \mathbf{u}_j该式也可写作 \frac{\partial \text{log} \mathbb{P}(w_o \mid w_c)}{\partial \mathbf{v}_c} = \mathbf{u}_o - \sum_{j \in \mathcal{V}} \mathbb{P}(w_j \mid w_c) \mathbf{u}_j随机采样的子序列有关其他词向量的梯度同理可得。训练模型时，每一次迭代实际上是用这些梯度来迭代子序列中出现过的中心词和背景词的向量。训练结束后，对于词典中的任一索引为 $i$ 的词，我们均得到该词作为中心词和背景词的两组词向量 $\mathbf{v}_i$ 和 $\mathbf{u}_i$。在自然语言处理应用中，我们会使用跳字模型的中心词向量。 连续词袋模型连续词袋模型与跳字模型类似。与跳字模型最大的不同是，连续词袋模型用一个中心词在文本序列前后的背景词来预测该中心词。举个例子，假设文本序列为“the”、 “man”、“loves”、“his”和“son”。以“loves”作为中心词，设时间窗口大小为 2。连续词袋模型所关心的是，给定与中心词距离不超过 2 个词的背景词“the”、“man”、“his”和“son”生成中心词“loves”的条件概率。 假设词典索引集 $\mathcal{V}$ 的大小为 $|\mathcal{V}|$，且 $\mathcal{V} = \{0, 1, \ldots, |\mathcal{V}|-1\}$。给定一个长度为 $T$ 的文本序列中，时间步 $t$ 的词为 $w^{(t)}$。当时间窗口大小为 $m$ 时，连续词袋模型需要最大化由背景词生成任一中心词的概率 \prod_{t=1}^T \mathbb{P}(w^{(t)} \mid w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)})上式的最大似然估计与最小化以下损失函数等价： -\sum_{t=1}^T \text{log} \mathbb{P}(w^{(t)} \mid w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)})我们可以用 $\mathbf{v}$ 和 $\mathbf{u}$ 分别表示背景词和中心词的向量（注意符号和跳字模型中的不同）。换言之，对于词典中索引为 $i$ 的词，它在作为背景词和中心词时的向量表示分别是 $\mathbf{v}_i$ 和 $\mathbf{u}_i$。而词典中所有词的这两种向量正是连续词袋模型所要学习的模型参数。为了将模型参数植入损失函数，我们需要使用模型参数表达损失函数中的给定背景词生成中心词的概率。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_{o_1}, \ldots, w_{o_{2m}}$ 在词典中索引为 $o_1, \ldots, o_{2m}$，损失函数中的给定背景词生成中心词的概率可以通过 softmax 函数定义为 \mathbb{P}(w_c \mid w_{o_1}, \ldots, w_{o_{2m}}) = \frac{\text{exp}\left(\mathbf{u}_c^\top (\mathbf{v}_{o_1} + \ldots + \mathbf{v}_{o_{2m}}) /(2m) \right)}{ \sum_{i \in \mathcal{V}} \text{exp}\left(\mathbf{u}_i^\top (\mathbf{v}_{o_1} + \ldots + \mathbf{v}_{o_{2m}}) /(2m) \right)}和跳字模型一样，当序列长度 $T$ 较大时，我们通常在每次迭代时随机采样一个较短的子序列来计算有关该子序列的损失。然后，根据该损失计算词向量的梯度并迭代词向量。 通过微分，我们可以计算出上式中条件概率的对数有关任一背景词向量 $\mathbf{v}_{o_i}(i = 1, \ldots, 2m)$的梯度为： \frac{\partial \text{log} \mathbb{P}(w_c \mid w_{o_1}, \ldots, w_{o_{2m}})}{\partial \mathbf{v}_{o_i}} = \frac{1}{2m} \left(\mathbf{u}_c - \sum_{j \in \mathcal{V}} \frac{\text{exp}(\mathbf{u}_j^\top \mathbf{v}_c)}{ \sum_{i \in \mathcal{V}} \text{exp}(\mathbf{u}_i^\top \mathbf{v}_c)} \mathbf{u}_j \right)该式也可写作 \frac{\partial \text{log} \mathbb{P}(w_c \mid w_{o_1}, \ldots, w_{o_{2m}})}{\partial \mathbf{v}_{o_i}} = \frac{1}{2m}\left(\mathbf{u}_c - \sum_{j \in \mathcal{V}} \mathbb{P}(w_j \mid w_c) \mathbf{u}_j\right)随机采样的子序列有关其他词向量的梯度同理可得。和跳字模型一样，训练结束后，对于词典中的任一索引为 $i$ 的词，我们均得到该词作为背景词和中心词的两组词向量 $\mathbf{v}_i$ 和 $\mathbf{u}_i$。在自然语言处理应用中，我们会使用连续词袋模型的背景词向量。 近似训练法我们可以看到，无论是跳字模型还是连续词袋模型，每一步梯度计算的开销与词典 $\mathcal{V}$ 的大小相关。当词典较大时，例如几十万到上百万，这种训练方法的计算开销会较大。因此，我们将使用近似的方法来计算这些梯度，从而减小计算开销。常用的近似训练法包括负采样和层序 softmax。 负采样我们以跳字模型为例讨论负采样。 实际上，词典 $\mathcal{V}$ 的大小之所以会在损失中出现，是因为给定中心词 $w_c$ 生成背景词 $w_o$ 的条件概率 $\mathbb{P}(w_o \mid w_c)$ 使用了 softmax 运算，而 softmax 运算正是考虑了背景词可能是词典中的任一词，并体现在分母上。 不妨换个角度考虑给定中心词生成背景词的条件概率。我们先定义噪声词分布 $\mathbb{P}(w)$，接着假设给定中心词 $w_c$ 生成背景词 $w_o$ 由以下相互独立事件联合组成来近似： 中心词 $w_c$ 和背景词 $w_o$ 同时出现时间窗口。 中心词 $w_c$ 和第 1 个噪声词 $w_1$ 不同时出现在该时间窗口（噪声词 $w_1$ 按噪声词分布 $\mathbb{P}(w)$ 随机生成，且假设一定和 $w_c$ 不同时出现在该时间窗口）。 … 中心词 $w_c$ 和第 $K$ 个噪声词 $w_K$ 不同时出现在该时间窗口（噪声词 $w_K$ 按噪声词分布 $\mathbb{P}(w)$ 随机生成，且假设一定和 $w_c$ 不同时出现在该时间窗口）。 下面，我们可以使用 $\sigma(x) = 1/(1+\text{exp}(-x))$ 函数来表达中心词 $w_c$ 和背景词 $w_o$ 同时出现在该训练数据窗口的概率： \mathbb{P}(D = 1 \mid w_o, w_c) = \sigma(\mathbf{u}_o^\top \mathbf{v}_c)那么，给定中心词 $w_c$ 生成背景词 $w_o$ 的条件概率的对数可以近似为 \text{log} \mathbb{P} (w_o \mid w_c) = \text{log} \left(\mathbb{P}(D = 1 \mid w_o, w_c) \prod_{k=1, w_k \sim \mathbb{P}(w)}^K \mathbb{P}(D = 0 \mid w_k, w_c) \right)假设噪声词 $w_k$ 在词典中的索引为 $i_k$，上式可改写为 \text{log} \mathbb{P} (w_o \mid w_c) = \text{log} \frac{1}{1+\text{exp}(-\mathbf{u}_o^\top \mathbf{v}_c)} + \sum_{k=1, w_k \sim \mathbb{P}(w)}^K \text{log} \left(1-\frac{1}{1+\text{exp}(-\mathbf{u}_{i_k}^\top \mathbf{v}_c)}\right)因此，有关给定中心词 $w_c$ 生成背景词 $w_o$ 的损失是 - \text{log} \mathbb{P} (w_o \mid w_c) = -\text{log} \frac{1}{1+\text{exp}(-\mathbf{u}_o^\top \mathbf{v}_c)} - \sum_{k=1, w_k \sim \mathbb{P}(w)}^K \text{log} \frac{1}{1+\text{exp}(\mathbf{u}_{i_k}^\top \mathbf{v}_c)}假设词典 $\mathcal{V}$ 很大，每次迭代的计算开销由 $\mathcal{O}(|\mathcal{V}|)$ 变为 $\mathcal{O}(K)$。当我们把 $K$ 取较小值时，负采样每次迭代的计算开销将较小。 当然，我们也可以对连续词袋模型进行负采样。有关给定背景词 $w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)}$ 生成中心词 $w_c$ 的损失 -\text{log} \mathbb{P}(w^{(t)} \mid w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)})在负采样中可以近似为 -\text{log} \frac{1}{1+\text{exp}\left(-\mathbf{u}_c^\top (\mathbf{v}_{o_1} + \ldots + \mathbf{v}_{o_{2m}}) /(2m)\right)} - \sum_{k=1, w_k \sim \mathbb{P}(w)}^K \text{log} \frac{1}{1+\text{exp}\left((\mathbf{u}_{i_k}^\top (\mathbf{v}_{o_1} + \ldots + \mathbf{v}_{o_{2m}}) /(2m)\right)}同样，当我们把 $K$ 取较小值时，负采样每次迭代的计算开销将较小。 层序 softmax层序 softmax 是另一种常用的近似训练法。它利用了二叉树这一数据结构。树的每个叶子节点代表着词典 $\mathcal{V}$ 中的每个词。我们以下图为例来描述层序 softmax 的工作机制。 假设 $L(w)$ 为从二叉树的根节点到词 $w$ 的叶子节点的路径（包括根和叶子节点）上的节点数。设 $n(w,j)$ 为该路径上第 $j$ 个节点，并设该节点的向量为 $\mathbf{u}_{n(w,j)}$。以上图为例，$L(w_3) = 4$。设词典中的词 $w_i$ 的词向量为 $\mathbf{v}_i$。那么，跳字模型和连续词袋模型所需要计算的给定词 $w_i$ 生成词 $w$ 的条件概率为： \mathbb{P}(w \mid w_i) = \prod_{j=1}^{L(w)-1} \sigma\left( [\![ n(w, j+1) = \text{leftChild}(n(w,j)) ]\!] \cdot \mathbf{u}_{n(w,j)}^\top \mathbf{v}_i\right)其中 $\sigma(x) = 1/(1+\text{exp}(-x))，\text{leftChild}(n)$ 是节点 $n$ 的左孩子节点，如果判断 $x$ 为真，$[[x]]=1$；反之 $[[x]] = -1$。由于 $\sigma(x)+\sigma(-x) = 1$，给定词 $w_i$ 生成词典 $\mathcal{V}$ 中任一词的条件概率之和为 1 这一条件也将满足： \sum_{w \in \mathcal{V}} \mathbb{P}(w \mid w_i) = 1计算上图中给定词 $w_i$ 生成词 $w_3$ 的条件概率。我们需要将 $w_i$ 的词向量 $\mathbf{v}_i$ 和根节点到 $w_3$ 路径上的非叶子节点向量一一求内积。由于在二叉树中由根节点到叶子节点 $w_3$ 的路径上需要向左、向右、再向左地遍历（加粗的路径），我们得到 \mathbb{P}(w_3 \mid w_i) = \sigma(\mathbf{u}_{n(w_3,1)}^\top \mathbf{v}_i) \cdot \sigma(-\mathbf{u}_{n(w_3,2)}^\top \mathbf{v}_i) \cdot \sigma(\mathbf{u}_{n(w_3,3)}^\top \mathbf{v}_i).在使用 softmax 的跳字模型和连续词袋模型中，词向量和二叉树中非叶子节点向量是需要学习的模型参数。假设词典 $\mathcal{V}$ 很大，每次迭代的计算开销由 $\mathcal{O}(|\mathcal{V}|)$ 下降至 $\mathcal{O}(\text{log}_2|\mathcal{V}|)$。 本文转自词嵌入：word2vec]]></content>
      <categories>
        <category>深度学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python序列化]]></title>
    <url>%2F2018%2F07%2F20%2FPython%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[我们把对象(变量)从内存中变成可存储或传输的过程称之为序列化，在Python中叫pickling，在其他语言中也被称之为serialization，marshalling，flattening等等，都是一个意思。序列化之后，就可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上。反过来，把变量内容从序列化的对象重新读到内存里称之为反序列化，即unpickling。 json：用于实现Python数据类型与通用（JSON）字符串之间的转换。 pickle: 用于实现Python数据类型与Python特定二进制格式之间的转换 ·json只能处理基本数据类型。pickle`能处理所有Python的数据类型。 json用于各种语言之间的字符转换。pickle用于Python程序对象的持久化或者Python程序间对象网络传输，但不同版本的Python序列化可能还有差异。 Python中，序列化模块包括json和pickle。强调注意：damps、damp、loads，这3个用法要掌握，在json和pickle中3者都是一样的用法。 json如果我们要在不同的编程语言之间传递对象，就必须把对象序列化为标准格式，比如XML，但更好的方法是序列化为JSON，因为JSON表示出来就是一个字符串，可以被所有语言读取，也可以方便地存储到磁盘或者通过网络传输。JSON不仅是标准格式，并且比XML更快，而且可以直接在Web页面中读取，非常方便。 dumps 序列化一个对象1234import jsoni = 10json.dumps(i) &#39;10&#39; 12s = 'hello'json.dumps(s) &#39;&quot;hello&quot;&#39; 12t = (1, 2, 3)json.dumps(t) &#39;[1, 2, 3]&#39; 12l = [1, 2, 3]json.dumps(l) &#39;[1, 2, 3]&#39; 123d = &#123;'name': 'LiMing', 'age': 23&#125;data = json.dumps(d)data &#39;{&quot;name&quot;: &quot;LiMing&quot;, &quot;age&quot;: 23}&#39; 123456789101112# 写入f = open('json序列化对象', 'w')f.write(data)f.close()del f# 读出f = open('json序列化对象')new_data = json.loads(f.read())f.close()new_data {&#39;name&#39;: &#39;LiMing&#39;, &#39;age&#39;: 23} dump 序列化并存入文件1234567891011# 写入f = open('json序列化对象', 'w')d = &#123;'name': 'LiMing', 'age': 25&#125;json.dump(d, f)# 读取del ff = open('json序列化对象')new_data = json.loads(f.read())f.close()new_data {&#39;name&#39;: &#39;LiMing&#39;, &#39;age&#39;: 25} pickle12345import pickleperson = &#123;'name': 'alvin', 'age': 23, 'sex': 'male'&#125;j = pickle.dumps(person)j b&#39;\x80\x03}q\x00(X\x04\x00\x00\x00nameq\x01X\x05\x00\x00\x00alvinq\x02X\x03\x00\x00\x00ageq\x03K\x17X\x03\x00\x00\x00sexq\x04X\x04\x00\x00\x00maleq\x05u.&#39;]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的深拷贝与浅拷贝]]></title>
    <url>%2F2018%2F07%2F20%2FPython_Copy_Deepcopy%2F</url>
    <content type="text"><![CDATA[Python默认复制的副本共享可能引发意想不到的结果。 默认做浅复制 =直接赋值，传递的是对象的引用，相当于一个别名，其中一个改变另一个也会跟着变。 构造方法或[:]做的是浅复制（即复制了最外层容器），如果所有元素都是不可变的，那么这样没有问题，还能节省内存。但是，如果有可变的元素，可能就会导致意想不到的问题。 copy模块提供的deepcopy和copy函数能为任意对象做深复制和浅复制。 深复制有时可能太深了。例如，对象可能会引用不该复制的外部资源或单例值。我们可以实现特殊方法 __copy__() 和 __deepcopy__()，控制 copy 和 deepcopy 的行为。 复制列表（或多数内置的可变集合）最简单的方式是使用内置的类型构造方法或[:]。例如： 123456l1 = [3, [66, 55, 44], (7, 8, 9)]l2 = list(l1)l2 == l1 # Truel2 is l1 # Falseid(l1) # 140027414483080id(l2) # 140027413720392 140027413719624 可以看出，副本与源列表相等，但是二者指代不同的对象。 下面看看这段代码的效果： 1234567891011l1 = [3, [66, 55, 44], (7, 8, 9)]l2 = list(l1) # ➊l1.append(100) # ➋l1[1].remove(55) # ➌print('l1:', l1)print('l2:', l2)l2[1] += [33, 22] # ➍l2[2] += (10, 11) # ➎print('l1:', l1)print('l2:', l2) l1: [3, [66, 44], (7, 8, 9), 100] l2: [3, [66, 44], (7, 8, 9)] l1: [3, [66, 44, 33, 22], (7, 8, 9), 100] l2: [3, [66, 44, 33, 22], (7, 8, 9, 10, 11)] ➊ l2 是 l1 的浅复制副本。此时的状态如图。➋ 把 100 追加到 l1 中，对 l2 没有影响。➌ 把内部列表 l1[1] 中的 55 删除。这对 l2 有影响，因为 l2[1] 绑定的列表与 l1[1]是同一个。➍ 对可变的对象来说，如 l2[1] 引用的列表， += 运算符就地修改列表。这次修改在l1[1] 中也有体现，因为它是 l2[1] 的别名。➎ 对元组来说， += 运算符创建一个新元组，然后重新绑定给变量 l2[2]。现在， l1 和 l2 中最后位置上的元组不是同一个对象。 为任意对象做深复制和浅复制12345678910111213141516171819import copyclass Bus: def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = list(passengers) def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) bus1 = Bus(['Alice', 'Bill', 'David'])bus2 = copy.copy(bus1)bus3 = copy.deepcopy(bus1)id(bus1), id(bus2), id(bus3) # ➊ (140027413487568, 140027413487400, 140027413569776) 1id(bus1.passengers), id(bus2.passengers), id(bus3.passengers) # ➋ (140027413574216, 140027413574216, 140027413866632) 1bus1.drop('Bill') 1bus2.passengers # ❸ [&#39;Alice&#39;, &#39;David&#39;] 1bus3.passengers # ❹ [&#39;Alice&#39;, &#39;Bill&#39;, &#39;David&#39;] ➊ 使用 copy 和 deepcopy，创建 3 个不同的 Bus 实例。➋ 审查 passengers 属性后发现，bus1 和 bus2 共享同一个列表对象，因为 bus2 是bus1 的浅复制副本。❸ bus1 中的 ‘Bill’ 下车后，bus2 中也没有他了。❹ bus3 是 bus1 的深复制副本，因此它的 passengers 属性指代另一个列表。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大熵模型【转】]]></title>
    <url>%2F2018%2F07%2F17%2F%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[最大熵模型（Maximum Entropy Model，以下简称MaxEnt），MaxEnt 是概率模型学习中一个准则，其思想为：在学习概率模型时，所有可能的模型中熵最大的模型是最好的模型；若概率模型需要满足一些约束，则最大熵原理就是在满足已知约束的条件集合中选择熵最大模型。 最大熵原理指出，当我们需要对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。（不做主观假设这点很重要。） 在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫“最大熵模型” 。我们常说，不要把所有的鸡蛋放在一个篮子里，其实就是最大熵原理的一个朴素的说法，因为当我们遇到不确定性时，就要保留各种可能性。 ——《数学之美》 特征函数接下来以统计建模的形式来描述 MaxEnt 模型，给定训练数据 $\left \{ (x_i,y_i)\right\}_{i=1}^N$ ，现在要通过Maximum Entrop 来建立一个概率判别模型，该模型的任务是对于给定的 $X = x$ 以条件概率分布 $P(Y|X = x)$ 预测 $Y$ 的取值。根据训练语料能得出 $(X,Y)$ 的经验分布， 得出部分 $(X,Y)$ 的概率值，或某些概率需要满足的条件，即问题变成求部分信息下的最大熵或满足一定约束的最优解，约束条件是靠特征函数来引入的。 特征函数 $f(x,y)$ 描述 $x$ 与 $y$ 之间的某一事实，其定义如下： f(x,y) = \left \{ \begin{aligned} 1, & \ 当 \ x、y \ 满足某一事实.\\ 0, & \ 不满足该事实.\\ \end{aligned}\right .特征函数 $f(x,y)$ 是一个二值函数， 当 $x$ 与 $y$ 满足事实时取值为 1 ，否则取值为 0 。比如对于如下数据集： 数据集中，第一列为 $Y$ ，右边为 $X$ ，可以为该数据集写出一些特征函数，数据集中得特征函数形式如下： f(x,y) = \left \{ \begin{aligned} 1, & \ if \ x= Cloudy \ and \ y=Outdoor.\\ 0, & \ else. \end{aligned}\right.为每个 对 都做一个如上的特征函数，用来描述数据集数学化。 约束条件接下来看经验分布，现在把训练数据当做由随机变量 $(X,Y)$ 产生,则可以根据训练数据确定联合分布的经验分布 $\widetilde{P}(X,Y)$ 与边缘分布的经验分布 $\widetilde{P}(X)$ : \begin{aligned} \widetilde{P}(X = x,Y = y) &= \frac{count(X=x,Y= y)}{N}\\ \widetilde{P}(X = x) &= \frac{count(X=x)}{N} \end{aligned}用 $E _{\widetilde{P}}(f)$ 表示特征函数 $f(x,y)$ 关于经验分布 $\widetilde{P}(X ,Y)$ 的期望，可得： E _{\widetilde{P}}(f) = \sum_{x,y}\widetilde{P}(x ,y)f(x,y) = \frac{1}{N} \sum_{x,y}f(x,y)用 $E _{P}(f)$ 表示特征函数 $f(x,y)$ 关于模型 $P(Y|X)$ 与经验分布 $\widetilde{P}(X)$ 的期望，可得： E_P(f) = \sum_{x,y}\widetilde{P}(x)P(y|x)f(x,y)$\widetilde{P}(x ,y)$ 前面已经得到了，数数 $f(x,y)$ 的次数就可以了，如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等， 即 $E_P(f) = E _{\widetilde{P}}(f)$ ： \sum_{x,y}\widetilde{P}(x)p(y|x)f(x,y) = \sum_{x,y}\widetilde{P}(x ,y)f(x,y)上式便为 $MaxEnt$ 中需要满足的约束，给定 $n$ 个特征函数 $f_i(x,y)$ ，则有 $n$ 个约束条件，用 $C$ 表示满足约束的模型集合： C = \left\{ P \ | \ E_P(f_i) = E _{\widetilde{P}}(f_i) ,I = 1,2,…,n \right \}从满足约束的模型集合 $C$ 中找到使得 $P(Y|X)$ 的熵最大的即为 MaxEnt 模型了。 最大熵模型关于条件分布 $P(Y|X)$ 的熵为： H(P) = –\sum_{x,y}\widetilde{P}(x)P(y|x)logP(y|x)首先满足约束条件然后使得该熵最大即可，MaxEnt 模型 $P^*$ 为： P^* = arg\max_{P \in C} H(P) \ \ 或 \ \ P^* = arg\min_{P \in C} -H(P)综上给出形式化的最大熵模型： 给定数据集 $\left \{ (x_i,y_i)\right\}_{i=1}^N$，特征函数 $f_i(x,y)，i= 1,2…,n$，根据经验分布得到满足约束集的模型集合 $C$ ： \begin{aligned} & \min_{P \in C} \ \ \sum_{x,y} \widetilde{P}(x)P(y|x)logP(y|x) \\ & \ s.t. \ \ \ E_p(f_i) = E _{\widetilde{P}}(f_i) \\ & \ \ \ \ \ \ \ \ \ \sum_yP(y|x) = 1 \end{aligned} MaxEnt 模型的求解MaxEnt 模型最后被形式化为带有约束条件的最优化问题，可以通过拉格朗日乘子法将其转为无约束优化的问题，引入拉格朗日乘子：$w_0,w_1,…,w_n$， 定义朗格朗日函数 $L(P,w)$: \begin{aligned} L(P,w) &= -H(P) + w_0\left (1-\sum_yP(y|x) \right ) + \sum^n_{i=1}w_i(E _{\widetilde{P}}(f_i) - E_p(f_i))\\ &=\sum_{x,y} \widetilde{P}(x)P(y|x)logP(y|x) + w_0\left (1-\sum_yP(y|x) \right ) + \sum^n_{i=1}w_i\left (\sum_{x,y}\widetilde{P}(x ,y)f(x,y) -\sum_{x,y}\widetilde{P}(x)p(y|x)f(x,y) \right ) \end{aligned}现在问题转化为: $\min_{P \in C}L(P,w)$ ，拉格朗日函数 $L(P,w)$ 的约束是要满足的 ，如果不满足约束的话，只需另 $w_i \rightarrow +\infty$ ，则可得 $L(P,w) \rightarrow +\infty$ ，因为需要得到极小值，约束必须要满足，满足约束后可得： $L(P,w) = \max L(P,w)$ ，现在问题可以形式化为便于拉格朗日对偶处理的极小极大的问题： \min_{P \in C} \max_w L(P,w)由于 $L(P,w)$ 是关于 $P$ 的凸函数，根据拉格朗日对偶可得 $L(P,w)$ 的极小极大问题与极大极小问题是等价的： \max_w \min_{P \in C} L(P,w)现在可以先求内部的极小问题 $\min_{P \in C} L(P,w)$ ，$\min_{P \in C} L(P,w)$ 得到的解为关于 $w$ 的函数，可以记做 $\Psi(w)$ ： \Psi(w) = \min_{P \in C} L(P,w) = L(P_w,w)上式的解 $P_w$ 可以记做： P_w = arg \min_{P \in C}L(P,w) = P_w(y|x)由于求解 $P$ 的最小值 $P_w$ ,只需对于 $P(y|x)$ 求导即可,令导数等于 $0$ 即可得到 $P_w(y|x)$ ： \begin{aligned} \frac{\partial L(P,w) }{\partial P(y|x)} &= \sum_{x,y}\widetilde{P}(x)(logP(y|x)+1)-\sum_yw_0-\sum_{x,y}\left ( \widetilde{P}(x)\sum_{i=1}^nw_if_i(x,y) \right ) \\ &= \sum_{x,y}\widetilde{P}(x)\left ( logP(y|x)+1-w_0-\sum_{i=1}^n w_if_i(x,y) \right ) = 0 \\ \Rightarrow \\ P(y|x) &= exp \left ( \sum_{i=1}^n w_if_i(x,y) +w_0-1 \right ) = \frac{exp(\sum\limits_{i=1}^{M}w_if_i(x,y))}{exp(1-w_0)} \end{aligned}由于 $\sum_yP(y|x) = 1$，可得: \sum_yP(y|x) = 1 \Rightarrow \frac {1} {exp(1-w_0)} \sum _y exp \left ( \sum_{i=1}^n w_if_i(x,y) \right ) = 1进而可以得到： exp(1-w_0) = \sum _y exp \left ( \sum_{i=1}^n w_if_i(x,y) \right )这里 $exp(1-w_0)$ 起到了归一化的作用，令 $Z_w(x)$ 表示 $exp(1-w_0)$ ，便得到了 $MaxEnt$ 模型 ： \begin{aligned} P_w(y|x) &= \frac{1}{Z_w(x) }exp \left ( \sum_{i=1}^n w_if_i(x,y) \right ) \\ Z_w(x) &=\sum _y exp \left ( \sum_{i=1}^n w_if_i(x,y) \right ) \end{aligned}这里 $f_i(x,y)$ 代表特征函数，$w_i$ 代表特征函数的权值， $P_w(y|x)$ 即为 $MaxEnt$ 模型的解，它具有指数形式，其中 $w_1,…,w_n$ 为参数，分布对应 $f_1, f_2, …,f_n$ 的权重， $w_i$ 越大，表示对应的特征越重要。注意，此时已经不再包含 $w_0$ 了。 现在内部的极小化求解得到关于 $w$ 的函数，现在求其对偶问题的外部极大化即可，将最优解记做 $w^*$: w^* = arg \max_w \Psi(w)所以现在最大熵模型转为求解 $\Psi(w)$ 的极大化问题，求解最优的 $w^*$ 后， 便得到了所要求的$MaxEnt$ 模型。 极大似然估计下面证明对偶函数的极大化等价于最大熵模型的极大似然估计。 这里有训练数据得到经验分布 $\widetilde{P}(x,y)$ ， 待求解的概率模型 $P(Y|X)$ 的似然函数为： L_{\widetilde{P}}(P_w) = \prod_{i=1}^{N}P(y_i|x_i) = \prod_{x,y}P(y|x)^{v(x,y)}指数除以样本总数$N$后，不改变$w$。 L_{\widetilde{P}}(P_w) = \prod_{x,y}P(y|x)^{\frac{v(x,y)}{N}} = \prod_{x,y}P(y|x)^{\widetilde{P}(x,y)}写成对数形式： L_{\widetilde{P}}(P_w) = log\prod_{x,y}P(y|x)^{\widetilde{P}(x,y)} = \sum_{x,y}\widetilde{P}(x,y)logP(y|x)将 $P_w(y|x)$ 带入以下公式可以得到： \begin{aligned} L_{\widetilde{P}}(P_w) &= \sum_{x,y}\widetilde{P}(x,y)logP(y|x)\\ &= \sum_{x,y}\widetilde{P}(x,y)\left ( \sum_{i=1}^n w_if_i(x,y) -logZ_w(x)\right )\\ &= \sum_{x,y}\widetilde{P}(x,y)\sum_{i=1}^n w_if_i(x,y) - \sum_{x,y}\widetilde{P}(x,y)logZ_w(x)\\ &= \sum_{x,y}\widetilde{P}(x,y)\sum_{i=1}^n w_if_i(x,y) - \sum_{x}\widetilde{P}(x)logZ_w(x)\\ \end{aligned}再来看看对偶函数$\Psi(w)$： 可见，拉格朗日对偶得到的结果与极大似然得到的结果是等价的，现在只需极大化似然函数即可，顺带优化目标中可以加入正则项，这是一个凸优化问题，一般的梯度法、牛顿法都可解之，专门的算法有GIS IIS 算法。 最优化算法IIS具体见李航统计学习方法。 其中部分推导如下 参考文献[1]. 最大熵模型 Maximum Entropy Model[2]. Maximum Entropy Model最大熵模型]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>最大熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启发式算法【转】]]></title>
    <url>%2F2018%2F07%2F05%2FHeuristic_Algorithm%2F</url>
    <content type="text"><![CDATA[讲一下什么是启发式算法。 转自启发式算法（Heuristic Algorithm）]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>启发式算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像风格迁移]]></title>
    <url>%2F2018%2F06%2F24%2F%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[基于神经网络的图像风格迁移在2015年由Gatys et al. 在两篇论文中提出：Gatys et al., 2015a和Gatys et al., 2015b。 使用神经网络进行样式迁移的过程如下图所示。在图中我们选取一个有三个卷积层的神经网络为例，来提取特征。对于样式图片，我们选取第一和第三层输出作为样式特征。对于内容图片则选取第二层输出作为内容特征。给定一个合成图片的初始值，我们通过不断的迭代直到其与样式图片输入到同一神经网络时，第一和第三层输出能很好地匹配样式特征，并且合成图片与初始内容图片输入到神经网络时在第二层输出能匹配到内容特征。 抽取特征论文使用的 VGG 19 模型来抽取特征。VGG 使用了五个卷积块来构建网络，块之间使用最大池化层来做间隔。论文中使用每个卷积块的第一个卷积层输出来匹配样式（称之为样式层），和第四块中的最后一个卷积层来匹配内容（称之为内容层）。 样式层和内容层有多种选取方法。通常越靠近输入层越容易匹配内容和样式的细节信息，越靠近输出则越倾向于语义的内容和全局的样式。选取比较靠后的内容层的目的是避免合成图片过于保留内容图片细节。使用多个位置的样式层来匹配局部和全局样式。 损失函数在训练时，我们需要定义如何比较合成图片和内容图片的内容层输出（内容损失函数），以及比较和样式图片的样式层输出（样式损失函数）。内容损失函数可以使用回归用的均方误差。 对于样式，我们可以简单将它看成是像素点在每个通道的统计分布。例如要匹配两张图片的样式，我们可以匹配这两张图片在 RGB 这三个通道上的直方图。更一般的，假设卷积层的输出格式是$c×h×w$，即（通道，高，宽）。那么我们可以把它变形成$c×hw$的二维数组，并将它看成是一个维度为 c 的随机变量采样到的 hw 个点。所谓的样式匹配就是使得两个 c 维随机变量统计分布一致。匹配统计分布常用的做法是冲量匹配，就是说使得他们有一样的均值，协方差，和其他高维的冲量。例如匹配二阶信息就是匹配协方差。 当使用靠近输出层的神经层输出来匹配时，经常可以观察到学到的合成图片里面有大量高频噪音，即有特别亮或者暗的颗粒像素。一种常用的降噪方法是总变差降噪（total variation denoising）。假设 $x_{i,j}$ 表示像素 $(i,j)$ 的值，总变差损失使得邻近的像素值相似： \sum_{i,j} \left|x_{i,j} - x_{i+1,j}\right| + \left|x_{i,j} - x_{i,j+1}\right|训练中将上述三个损失函数加权求和。通过调整权重值可以控制学到的图片是否保留更多样式，更多内容，还是更加干净。此外注意到样式层里有五个神经层，我们对靠近输入的有较少的通道数的层给予比较大的权重。 训练训练的主要不同在于我们只对输入x进行更新。此外我们将x的梯度除以了它的绝对平均值来降低对学习率的敏感度，而且每隔一定的批量我们减小一次学习率。 参考文献[1]. 图像风格迁移(Neural Style)简史[2]. 样式迁移]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>风格迁移</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测之YOLO V2]]></title>
    <url>%2F2018%2F06%2F24%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BYOLO9000%2F</url>
    <content type="text"><![CDATA[YOLO版本论文全名叫“YOLO9000: Better, Faster, Stronger”，主要有两个大方面的改进： 使用了一系列的方法对原来的YOLO多目标检测框架做了大量改进，在保持原有速度优势的基础上，精度上得以提升。 提出了一种目标分类与检测的联合训练方法，通过这种方法，YOLO9000可以同时在COCO和ImageNet数据集中进行训练，训练后的模型可以实现多达9000种物体的实时检测。 Better先总体看一下YOLOv2用了什么技巧： Batch Normalization对于深度神经网络来说，每一层里都对输入乘以权重后得到输出。当很多层这样的相乘累计在一起时，一个输入数据较小的改变都可能导致输出产生巨大变化，从而带来不稳定性，大大降低网络的训练速度和泛化能力。YOLOv2通过在每一个卷积层后添加batch normalization，极大的改善了收敛速度同时减少了对其它正则化方法的依赖（舍弃了Dropout优化后依然没有过拟合），使得mAP获得了2%的提升。 High Resolution Classifier目前的目标检测方法中，基本上都会使用ImageNet预训练过的模型（classifier）来提取特征，从AlexNet开始，多数分类器都把输入图像resize到$256\times 256$，这会容易丢失一些小物体的信息，给检测带来困难。 YOLOv1先使用$256\times 256$的分辨率来训练分类网络，在训练检测网络的时候再切换到$448\times 448$的分辨率，这意味着YOLOv1的卷积层要重新适应新的分辨率同时YOLOv1的网络还要学习检测网络。 对于YOLOv2，作者首先对分类网络（自定义的darknet）进行了fine tune，分辨率改成$448\times 448$，在ImageNet数据集上训练10轮（10 epochs），训练后的网络就可以适应高分辨率的输入了。然后，作者对检测网络部分（也就是后半部分）也进行fine tune。这样通过提升输入的分辨率，mAP获得了4%的提升。 Convolutional With Anchor BoxesYOLOv1使用全连接层数据进行bounding box预测（要把$14701$的全链接层reshape为$77*30$的最终特征），这会丢失较多的空间信息导致定位不准。YOLOv2借鉴了Faster RCNN中的anchor的思想：在卷积特征图上进行滑窗操作，每个中心预测9种不同大小和比例的建议框，这样很好的保留的空间信息。 去掉全连接层。 输入图片分辨率变为为$416\times416$，而不是$448\times448$。这一步的目的是为了让后面产生的卷积特征图宽高都为奇数（$7\times7$ v.s. $8\times8$ grid cell），这样就可以产生一个center cell。因为大物体通常占据了图像的中间位置，这样就可以只用中心的一个cell来预测这些物体的位置。 去掉一个池化层，使网络卷积层输出具有更高的分辨率。这样最终得到$13\times13$的卷积特征图。 把预测类别的机制从空间位置（Cell）中解耦，由anchor Box同时预测类别和坐标。因为YOLO是由每个cell来负责预测类别，每个cell对应的2个bounding box 负责预测坐标。YOLOv2中，不再让类别的预测与每个Cell（空间位置）绑定一起，而是让全部放到Anchor Box中。下面是特征维度示意图（仅作示意并非完全正确）。 加入了anchor boxes后，可以预料到的结果是召回率上升，准确率下降。YOLOv1每张图像只预测98个边界框，但是使用锚盒的模型预测超过1000。假设每个cell预测9个建议框，那么总共会预测$13\times13\times9= 1521$个boxes。但准确率只有小幅下降，而召回率则提升了7%。 Dimension Clusters（维度聚类）使用anchor时，作者发现Faster RCNN中anchor boxes的个数和宽高维度往往是手动选择的先验框（hand-picked priors)，带有一定主观性。设想能否一开始就选择了更好的、更有代表性的先验boxes维度，那么网络就更容易学到准确的预测位置。解决办法就是统计学习中的k-means聚类方法，通过对数据集中的ground true box做聚类，找到ground true box的统计规律。以聚类个数k为anchor boxs个数，以k个聚类中心box的宽高维度为anchor box的维度。 传统的K-means聚类方法使用的是欧氏距离函数，也就意味着较大的boxes会比较小的boxes产生更多的error，聚类结果可能会偏离。为此，作者采用的评判标准是IOU得分（也就是boxes之间的交集除以并集），这样error就和box的尺度无关，最终的距离函数为： d(\text{box}, \text{centroid}) = 1 - \text{IOU}(\text{box}, \text{centroid})可以看到，平衡复杂度和IOU之后，最终得到k值为5，意味着作者选择了5种大小的box维度来进行定位预测，这与手动选择的box维度不同。结果中扁长的框较少，而瘦高的框更多（这符合行人的特征），这种结论如不通过聚类实验恐怕是发现不了的。 使用聚类方法，仅仅5种boxes的召回率就和Faster R-CNN的9种相当。说明K-means方法的引入使得生成的boxes更具有代表性，为后面的检测任务提供了便利。 Direct Location Prediction（直接位置预测）在YOLO上使用anchor boxes会遇到一个问题：模型不稳定。尤其是在早期迭代中。论文认为模型不稳定的原因来自于预测bbox的$(x,y)$。 x=(t_x \times w_a)+x_a \\ y=(t_y \times h_a)+y_a在Faster R-CNN的inference时，偏移因子$t_x,t_y$是没有限制的，使得无论在什么位置进行预测，任何anchor boxes可以在图像中任意一点。模型随机初始化后，需要花很长一段时间才能稳定预测敏感的物体位置。而正确做法应该是每一个anchor只负责检测周围的一个部分区域。 所以YOLO的方法改为预测相对于网格单元位置的位置坐标，并使用Sigmoid函数将偏移量限制在0-1(这里的尺度是针对grid cell)，计算公式如下： b_x=\sigma(t_x)+c_x \\ b_y=\sigma(t_y)+c_y \\ b_w=p_we^{t_w} \\ b_h=p_he^{t_h} \\ Pr(object)\times IOU(b,object) = \sigma(t_o) \\Sigmoid实际意义是使anchor只负责周围的box，有利于提升效率和网络收敛。$\sigma(t_x)$是bounding box的中心相对栅格左上角的横坐标，$\sigma(t_y)$是纵坐标，$\sigma(t_o)$是bounding box的confidence score。$b_x,b_y,b_w,b_h$是预测的bbox的中心点坐标和宽高，中心点坐标的尺度是相对于grid cell。如下图： 使用Dimension Clusters和Direct location prediction这两项anchor boxes改进方法，mAP获得了5%的提升。 Fine-Grained Features网络最后的feature map尺寸为$13\times13$，对于检测大尺寸的目标是够的。如果是要检测小尺寸的物体，就有点勉强。Faser RCNN和SSD都在不同层次的特征图上产生区域建议（SSD直接就可看得出来这一点），获得了多尺度的适应性。这里使用了一种不同的方法，简单添加了一个转移层（ passthrough layer），这一层要把分辨率为$26\times26\times512$的浅层特征图reshape成$13\times13\times2048$，然后连接到深层特征图。它可以拥有更好的细粒度特征，使得模型的性能获得了1%的提升。如下图所示（下图应为$26\times26\times512$和$13\times13\times2048$，直接copy别人的图，勿喷）： Multi-Scale TrainingYOLOv1网络使用固定的$448\times448$图片作为输入，加入anchor boxes后，输入变成了$416\times416$。网络只用到了卷积层和池化层，所以可输入任意大小的图片。作者希望YOLOv2对不同尺寸图片具有良好的鲁棒性，因此在训练的时候也考虑了这一点。 不同于固定输入网络的图片尺寸的方法，YOLOv2在几次迭代后就会微调网络。每经过10个epoch，就会随机选择新的图片尺寸。YOLO网络使用的降采样参数为32，那么就使用32的倍数进行尺度池化${320,352，…，608}$。最终最小的尺寸为$320\times320$，最大的尺寸为$608\times608$，在新的输出尺寸上进行训练。 这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务。在小尺寸图片检测中，YOLOv2成绩很好，输入为$228\times228$的时候，帧率达到90FPS，mAP几乎和Faster RCNN的水准相同。使得其在低性能GPU、高帧率视频、多路视频场景中更加适用。在大尺寸图片检测中，YOLOv2达到了先进水平，VOC2007 上mAP为78.6%，仍然高于平均水准。 FasterYOLO一向是速度和精度并重。大多数检测网络有赖于VGG-16作为特征提取部分，VGG-16的确是一个强大而准确的分类网络，但是复杂度有些冗余。$224\times224$的图片进行一次前向传播，卷积层就需要多达306.9亿次浮点数运算。 YOLOv2使用的是基于Googlenet的定制网络，比VGG-16更快，一次前向传播仅需85.2亿次运算。但它的精度要略低于VGG-16，单张$224\times224$取前五个预测概率的对比成绩为88%和90%（低一点点也是可以接受的）。 Darknet-19YOLOv2使用了一个新的分类网络作为特征提取部分，使用了较多的$3\times3$卷积核，在每一次池化操作后把通道数翻倍。借鉴network in network的思想，网络使用了全局平均池化（global average pooling），把$1\times1$的卷积核置于$3\times3$的卷积核之间，用来压缩特征。也使用了batch normalization稳定模型训练。 最终得出的基础模型就是Darknet-19，如下图，其包含19个卷积层、5个最大值池化层（maxpooling layers ），下图展示网络具体结构。 Training for classification论文以Darknet-19为模型在ImageNet上用SGD跑了160epochs。 参数 数值 learning rate 0.1 polynomial rate decay 4 weight decay 0.00005 momentum 0.9 data augmentation数据增强 random crops, rotations等tricks 跑完了160 epochs后，把输入尺寸从$224\times224$上调为$448\times448$，这时候lr调到0.001，再跑了10 epochs，这时候DarkNet达到了top-1准确率76.5%，top-5准确率93.3%。 Training for dectection在上面训练好的DarkNet-19的基础上，把分类网络改成detect网络，去掉原先网络的最后一个卷积层，取而代之的是使用3个$3\times3\times1024$的卷积层，并且每个新增的卷积层后面接$1\times1$的卷积层，输出维度是检测所需的数量。对于VOC数据集，预测5种boxes大小，每个box包含5个坐标值和20个类别，所以总共是$5\times(5+20)=125$个输出维度。同时也添加了转移层（passthrough layer），从最后那个$3\times3\times512$的卷积层连到倒数第二层，使模型有了细粒度特征。 参数 数值 训练次数 160 epochs learning rate 起始0.001,在60和90 epochs时衰减10倍 weight decay 0.0005 momentum 0.9 data augmentation random crops,color shifting,etc data augmentation数据增强 random crops, rotations等tricks Stronger论文提出了一种联合训练的机制：使用detection数据集训练模型detection相关parts，使用classification数据集训练模型classification相关parts。 这样训练会有一些问题:detection datasets的标签更为“上层”,例如狗，船啊啥的。而对应的classification datasets的标签就“下层”了很多，比如狗就有很多种，例如“Norfolk terrier”, “Yorkshire terrier”, and “Bedlington terrier”等等。 而我们一般在模型中分类使用的是softmax，softmax计算所有种类的最终的概率分布。softmax会假设所有种类之间是互斥的，但是，实际过程中，“上层”和“下层”之间是有对应的关系的。(例如中华田园犬，博美都属于狗)，照着这样的思路，论文整出了一个层次性的标签结构。 Hierarchical classificationImageNet的标签的来源是WordNet一种结构化概念及概念之间关系的语言数据库)。例如： WordNet是一个有向图结构（而非树结构），因为语言是复杂的（例如“dog”既是“canine”又是“domestic animal”），为了简化问题，作者从ImageNet的概念中构建了一个层次树结构（hierarchical tree）来代替图结构方案。 创建层次树的步骤是： 遍历ImageNet的所有视觉名词 对每一个名词，在WordNet上找到从它所在位置到根节点（“physical object”）的路径。 许多同义词集只有一条路径。所以先把这些路径加入层次树结构。 然后迭代检查剩下的名词，得到路径，逐个加入到层次树。路径选择办法是：如果一个名词有两条路径到根节点，其中一条需要添加3个边到层次树，另一条仅需添加一条边，那么就选择添加边数少的那条路径。 最终结果是一颗 WordTree （视觉名词组成的层次结构模型）。用WordTree执行分类时，预测每个节点的条件概率。例如： 在“terrier”节点会预测： Pr(\text{Norfolk terrier} | \text{terrier}) \\ Pr(\text{Yorkshire terrier} | \text{terrier}) \\ Pr(\text{Bedlington terrier} | \text{terrier})\\ …\\如果想求得特定节点的绝对概率，只需要沿着路径做连续乘积。例如 如果想知道一张图片是不是“Norfolk terrier ”需要计算： Pr(\text{Norfolk terrier}) = Pr(\text{Norfolk terrier} | \text{terrier})\\ \times Pr(\text{terrier} | \text{hunting dog}) \\ \times \ldots \times \\ \times Pr(\text{mammal} | Pr(\text{animal})\\ \times Pr(\text{animal} | \text{physical object})分类时假设 图片包含物体：Pr(physical object) = 1。 为了验证这种方法作者在WordTree（用1000类别的ImageNet创建）上训练了Darknet-19模型。为了创建WordTree1k作者添加了很多中间节点，把标签由1000扩展到1369。训练过程中ground truth标签要顺着向根节点的路径传播：例如 如果一张图片被标记为“Norfolk terrier”它也被标记为“dog” 和“mammal”等。为了计算条件概率，模型预测了一个包含1369个元素的向量，而且基于所有“同义词集”计算softmax，其中“同义词集”是同一概念的下位词。 使用相同的训练参数，层次式Darknet-19获得71.9%的top-1精度和90.4%top-5精度。尽管添加了369个额外概念，且让网络去预测树形结构，精度只有略微降低。按照这种方式执行分类有一些好处，当遇到新的或未知物体类别，预测精确度降低的很温和（没有突然巨幅下降）。例如：如果网络看到一张狗的图片，但是不确定狗的类别，网络预测为狗的置信度依然很高，但是，狗的下位词（“xx狗”）的置信度就比较低。 这个策略野同样可用于检测。不再假设每一张图片都包含物体，取而代之使用YOLOv2的物体预测器（objectness predictor）得到Pr(physical object)的值。检测器预测一个bounding box和概率树（WordTree）。沿着根节点向下每次都走置信度最高的分支直到达到某个阈值，最终预测物体的类别为最后的节点类别。 Dataset Combination With WordTree可以使用WordTree把多个数据集整合在一起。只需要把数据集中的类别映射到树结构中的同义词集合（synsets）。使用WordTree整合ImageNet和COCO的标签如下图： Joint Classification And Detection使用WordTree整合了数据集之后就可以在“分类-检测”数据上训练联合模型。作者想要训练一个检测类别很大的检测器所以使用COCO检测数据集和全部ImageNet的前9000类创造一个联合数据集。为了评估使用的方法，也从ImageNet detection challenge 中向整合数据集添加一些还没有存在于整合数据集的类别。相应的WordTree有9418个类别。由于ImageNet是一个非常大的数据集，所以通过oversampling COCO数据集来保持平衡，使ImageNet：COCO = 4：1。 使用上面的数据集训练YOLO9000。采用基本YOLOv2的结构，anchor box数量由5调整为3用以限制输出大小。 当网络遇到一张检测图片就正常反向传播。其中对于分类损失只在当前及其路径以上对应的节点类别上进行反向传播。 当网络遇到一张分类图片仅反向传播分类损失。在该类别对应的所有bounding box中找到一个置信度最高的（作为预测坐标），同样只反向传播该类及其路径以上对应节点的类别损失。反向传播objectness损失基于如下假设：预测box与ground truth box的重叠度至少0.31IOU。 采用这种联合训练，YOLO9000从COCO检测数据集中学习如何在图片中寻找物体，从ImageNet数据集中学习更广泛的物体分类。 作者在ImageNet detection task上评估YOLO9000。ImageNet detection task和COCO有44个物体类别是相同的。这意味着YOLO9000只从大多数测试数据集中看到过分类数据而非检测数据。最终整体精度为19.7mAP，在从未见过的156个物体检测数据类别上精度为16.0mAP。这个结果高于DPM，但是YOLO9000是在不同数据集上进行半监督训练。而且YOLO9000可以同时实时检测9000多种其它物体类别。 作者也分析了YOLO9000在ImageNet上的性能，发现可以学习新的动物表现很好，但是学习衣服和设备这类物体则不行。因为从COCO数据集上动物类别那里学习到的物体预测泛化性很好。但是COCO数据集并没有任何衣服类别的标签数据（只有”人”类别），所以YOLO9000很难对“太阳镜”，“游泳裤”这些类别建模。 参考文献[1]. YOLO9000: Better, Faster, Stronger[2]. YOLO9000, Better, Faster, Stronger论文翻译[3]. Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3[4]. YOLOv2 论文笔记[5]. 目标检测之YOLOv2[6]. Object Detection — 论文YOLO2(YOLO9000:Better, Faster, Stronger)解读[7]. 知乎专栏：YOLO2]]></content>
      <categories>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测之YOLO]]></title>
    <url>%2F2018%2F06%2F24%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BYOLO%2F</url>
    <content type="text"><![CDATA[YOLO（You Only Look Once: Unified, Real-Time Object Detection），是Joseph Redmon和Ali Farhadi等人于2015年提出的基于单个神经网络的目标检测系统。在2017年CVPR上，Joseph Redmon和Ali Farhadi又发表的YOLO 2，进一步提高了检测的精度和速度。 首先将输入图片resize到448x448，然后送入CNN网络，最后处理网络预测结果得到检测的目标。相比R-CNN算法，其是一个统一的框架，其速度更快，而且Yolo的训练过程也是end-to-end的。 网络结构 上图展示了YOLO的网络结构。相比Faster RCNN，YOLO结构简单，包含24个卷积层+2个全连接层。其架构受GoogleNet影响，前20层与GoogleNet相似，不同的是，YOLO未使用Inception Module，而是使用$1\times1$卷积层（此处1*1$1\times1$卷积层的存在是为了跨通道信息整合）+$3\times3$卷积层简单替代。直接用预训练的模型来初始化参数。后面的4个卷积层是新加的。 训练 预训练分类网络：在 ImageNet 1000 class Dataset上预训练一个分类网络，这个网络是前文网络结构中的前20个卷机网络+Average Pooling Layer+Fully Connected Layer。此时网络输入是$224\times224$。 训练检测网络：YOLO添加4个卷积层和2个全链接层，随机初始化权重。把网络输入从$224\times224$变成$448\times448$。上面几张图片展示了最后形成的 $7\times 7\times 30$ 的详细信息。直接剪切Deepsystems.io团队的PPT。下面详细解释。(1). 将原图划分为$S\times S$的网格。论文中是$7\times 7$，原图经过网络之后变成 $7\times 7\times 30$ 的输出，和前面的网格相对应。如果一个目标的中心落入某个网格，后面的输出中这个网格对应的输出维度就负责检测该目标。(2). 每个网格要预测$B$个bounding boxes，以及$C$个类别概率Pr(classi|object)。$C$是网络分类总数。每个格子只有一个类别。即每个网格产生$B$个bounding boxes，这些bounding box预测的类别都相同。(3). 每个bounding box除了要回归自身的位置之外，还要附带预测一个confidence值。这个confidence代表了所预测的box中含有目标的置信度和这个bounding box预测的有多准两重信息： confidence = \Pr(\textrm{Object}) \cdot \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}如果有目标中心落在格子则confidence为iou；否则confidence为0。 每个边界框包含5个预测：$x,y,w,h,confidence$。$(x，y)$坐标表示边界框相对于网格单元边界框的中心。$(w, h)$是相对于整张图像的比例。需要说明，这里的$x, y, w$和$h$都是经过归一化的。(4). 由于输入图像被分为$S\times S$网格，每个网格包括5个预测量：$(x, y, w, h, confidence)$和$C$个类别分数，所以网络输出是$S\times S\times (5\times B + C)$大小。论文中由于有20类，所以为 $7\times 7\times 30$。 确定损失函数论文中比较难理解的是Loss。表达式如下： \begin{multline} \lambda_\textbf{coord} \sum_{i = 0}^{S^2} \sum_{j = 0}^{B} \mathbb{1}_{ij}^{\text{obj}} \left[ \left( x_i - \hat{x}_i \right)^2 + \left( y_i - \hat{y}_i \right)^2 \right] \\ + \lambda_\textbf{coord} \sum_{i = 0}^{S^2} \sum_{j = 0}^{B} \mathbb{1}_{ij}^{\text{obj}} \left[ \left( \sqrt{w_i} - \sqrt{\hat{w}_i} \right)^2 + \left( \sqrt{h_i} - \sqrt{\hat{h}_i} \right)^2 \right] \\ + \sum_{i = 0}^{S^2} \sum_{j = 0}^{B} \mathbb{1}_{ij}^{\text{obj}} \left( C_i - \hat{C}_i \right)^2 \\ + \lambda_\textrm{noobj} \sum_{i = 0}^{S^2} \sum_{j = 0}^{B} \mathbb{1}_{ij}^{\text{noobj}} \left( C_i - \hat{C}_i \right)^2 \\ + \sum_{i = 0}^{S^2} \mathbb{1}_i^{\text{obj}} \sum_{c \in \textrm{classes}} \left( p_i(c) - \hat{p}_i(c) \right)^2 \end{multline}其中，$\mathbb{1}_i^{\text{obj}}$表示目标是否出现在grid cell中，$\mathbb{1}_{ij}^{\text{obj}}$表示第$i$个grid cell中第$j$个bounding box对预测负责。其他$B-1$个bounding box作为$\mathbb{1}_i^{\text{noobj}}$进行计算。也就是说，假如一张图片中只有三个物体，那么就只有3个bounding box计算$\mathbb{1}_i^{\text{obj}}$，其他的都计算$\mathbb{1}_i^{\text{noobj}}$。但这样负例太多，样本严重不均衡，所以加入$\lambda_\textrm{noobj}$做均衡。这里跟SSD和Faster RCNN的负例采样不同。 这个损失函数中：(1). classification error只惩罚对预测负责的bbox。(2). 更重视8维的坐标预测，给这些损失前面赋予更大的Loss Weight, 记为$\lambda_{coord}$,在Pascal VOC训练中取5。(3). 对没有object的bbox的confidence Loss，赋予小的Loss Weight，记为$\lambda_{noobj}$，在Pascal VOC训练中取0.5。(4). 有object的bbox的confidence Loss和类别的Loss的Loss Weight正常取1。(5). 将bbox的width和height取平方根代替原本的height和width。 检测 得到类别score在检测目标的时候，每个网格预测的类别条件概率和bounding box预测的confidence信息相乘，就得到每个bounding box的class-specific confidence score: \Pr(\textrm{Class}_i | \textrm{Object}) * \Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}} = \Pr(\textrm{Class}_i)*\textrm{IOU}_{\textrm{pred}}^{\textrm{truth}} 给每一类做非极大值抑制对于每一类都做如下操作，最终得到的有很多0元素的矩阵：例如，对于cat类，先将小于阈值的score置为0，然后按49个cat的score大小排序，做非极大值抑制，得到很多0。 按照score大小得到最终类别 参考文献[1]. You Only Look Once: Unified, Real-Time Object Detection[2]. 目标检测之YOLO，SSD[3]. YOLO PPT by Deepsystems.io[4]. 知乎：YOLO笔记]]></content>
      <categories>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>SSD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习常用的LossFunction小结]]></title>
    <url>%2F2018%2F06%2F23%2F%E5%B8%B8%E7%94%A8%E7%9A%84LossFunction%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文介绍深度学习中常用的损失函数。会不断更新。 均方误差的问题理想情况下，我们希望神经网络能够快速地从错误中学习，并且错误越大，下降速度越快。但有时候采用均方误差时loss很大，下降速率却很慢。戳此演示 演示 考虑神经元的学习方式：假设Loss是 $C$ ，通过计算代价函数的偏导 $\partial C/\partial w$ 和 $\partial C / \partial b$ 来改变权重和偏置。那么「学习速度很慢」其实上是在说偏导很小。那么问题就是为何偏导很小。为了解释这个问题，我们先来计算一下偏导。使用了均方代价函数，即： \begin{eqnarray} C = \frac{(y-a)^2}{2} \end{eqnarray}这里 $a$ 是输入 $x=1$ 时神经元的输出，$y=0$ 是期望输出。下面我们用权重和偏置来重写这个式子。$a = \sigma(z)$，$z = wx+b$。运用链式法则得到： \begin{eqnarray} \frac{\partial C}{\partial w} & = & (a-y)\sigma'(z) x = a \sigma'(z) \\ \frac{\partial C}{\partial b} & = & (a-y)\sigma'(z) = a \sigma'(z) \end{eqnarray}为了理解这些表达式的行为，我们画出 $\sigma$ 函数的图像如下。 从图像看出当神经元输出接近1时，曲线变得非常平坦，因此 $\sigma’(z)$ 就会变得非常小。$\partial C / \partial w$ 和 $\partial C / \partial b$ 会变得很小。这就是学习速度变慢的根源。 交叉熵损失那么如何来避免这种减速？事实证明我们可以用不同的代价函数比如交叉熵（cross-entropy）代价函数来替代平方代价函数。为了理解交叉熵，我们假设要训练一个拥有多个输入变量的神经元：输入 $x_1, x_2, \ldots$，权重 $w_1, w_2, \ldots$，偏置 $b$： 神经元的输出为 $a = \sigma(z)$，这里 $z = \sum_j w_j x_j+b$。我们定义这个神经元的交叉熵代价函数为： \begin{eqnarray} C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right] \tag{1} \end{eqnarray}这里 $n$ 是训练数据的个数，这个加和覆盖了所有的训练输入 $x$，$y$ 是期望输出。 交叉熵有两个特性能够合理地解释为何它能作为代价函数。 首先，它是非负的，也就是说，$C&gt;0$。需要注意到：1. 等式 $(1)$加和里的每一项都是负的，因为这些数是0到1，它们的对数是负的；2. 整个式子的前面有一个负号。 其次，如果对于所有的训练输入 $x$，如果这个神经元的实际输出值都能很接近期望输出，那么交叉熵将会非常接近0。为了说明这个，假设有一些输入样例 $x$ 得到的输出是 $y = 0$，$a \approx 0$。这些都是一些比较好的输出。我们会发现等式 $(1)$ 的第一项将会消掉，与此同时，第二项 $-\ln (1-a) \approx 0$。同理，当 $y=1$ 或 $a \approx 1$ 时也如此分析。那么如果实际输出接近期望输出，代价函数的分布就会很低。 总结一下，交叉熵是正的，并且当所有输入 $x$ 的输出都能接近期望输出 $y$ 的话，交叉熵的值将会接近0。这两个特征在直觉上我们都会觉得它适合做代价函数。事实上，均方代价函数也同时满足这两个特征。 而且交叉熵有另一个均方代价函数不具备的特征，它能够避免学习速率降低的情况。为了理解这个，我们计算一下交叉熵关于权重的偏导。用$a = \sigma(z)$代替等式（1），并且运用链式法则，得到： \begin{eqnarray} \frac{\partial C}{\partial w_j} & = & -\frac{1}{n} \sum_x \left( \frac{y }{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \right) \frac{\partial \sigma}{\partial w_j} \\ & = & -\frac{1}{n} \sum_x \left( \frac{y}{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \right)\sigma'(z) x_j \end{eqnarray}通分化简之后得到： \begin{eqnarray} \frac{\partial C}{\partial w_j} & = & \frac{1}{n} \sum_x \frac{\sigma'(z) x_j}{\sigma(z) (1-\sigma(z))} (\sigma(z)-y) \end{eqnarray}利用sigmoid函数的定义，$\sigma(z) = 1/(1+e^{-z})$，得到 $\sigma’(z) = \sigma(z)(1-\sigma(z))$。可以看到 $\sigma’(z)$和$\sigma(z)(1-\sigma(z))$ 这一项在上式中消除了，它被简化成： \begin{eqnarray} \frac{\partial C}{\partial w_j} = \frac{1}{n} \sum_x x_j(\sigma(z)-y) \end{eqnarray}这是一个非常优美的表达式。它告诉我们权重的学习速率可以被 $\sigma(z)-y$ 控制，也就是被输出结果的误差所控制。误差越大我们的神经元学习速率越大。这正是我们直觉上所期待的那样。另外它能避免学习减速，这是 $\sigma’(z)$ 一项导致的。当我们使用交叉熵时，$\sigma’(z)$ 这一项会被抵消掉，因此我们不必担心它会变小。 同样，我们能够计算偏置的偏导： \begin{eqnarray} \frac{\partial C}{\partial b} = \frac{1}{n} \sum_x (\sigma(z)-y) \end{eqnarray}同理，它也能够避免 $\sigma’(z)$ 这一项带来的学习减速。 mxnet中的损失函数下面介绍mxnet中常用的损失函数。 SigmoidBinaryCrossEntropyLossIf from_sigmoid is False (default), this loss computes: prob = \frac{1}{1 + \exp(-{pred})} \\ L = - \sum_i {label}_i * \log({prob}_i) +(1 - {label}_i) * \log(1 - {prob}_i)If from_sigmoid is True, this loss computes: L = - \sum_i {label}_i * \log({prob}_i) +(1 - {label}_i) * \log(1 - {prob}_i)HuberLossHuber loss是为了增强平方误差损失函数（squared loss function）对噪声（或叫离群点，outliers）的鲁棒性提出的。定义如下： f(x) = \begin{cases} (\sigma x)^2/2,& \text{if }x < 1/\sigma^2\\ |x|-0.5/\sigma^2,& \text{otherwise} \end{cases}图像如下： SoftmaxCrossEntropyLossIf sparse_label is True (default), label should contain integer category indicators: \DeclareMathOperator{softmax}{softmax} \\ p = \softmax({pred})\\L = -\sum_i \log p_{i,{label}_i}If sparse_label is False, label should contain probability distribution and label‘s shape should be the same with pred: p = \softmax({pred}) \\ L = -\sum_i \sum_j {label}_j \log p_{ij}参考文献[1]. Neural Networks and Deep Learning[2]. 交叉熵代价函数[3]. 动手学深度学习]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】循环神经网络(RNN)、GRU、LSTM]]></title>
    <url>%2F2018%2F06%2F23%2F%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(RNN)%2F</url>
    <content type="text"><![CDATA[现实生活中，有很多事件由序列的信息决定。如：语言，基因，时间序列等。如果我们试图从这类数据得到有用的输出，就需要一个这样的网络：能够访问一些关于数据的历史知识，以便完全理解这些数据。循环神经网络（RNN）正是解决这个问题。 循环神经网络的基本结构如下： 在上图中，块A连接着输入 $X_t$ 和输出 $h_t$，其中有一个循环允许这一时刻的信息传递到网络的下一个时刻。一个循环神经网络可以看成是一个同一网络的多个副本，每一个副本都传递信息给下一个副本。可以展开如上图右边所以。 在过去几年间，RNN已经被大量的成功应用于解决很多问题：语音识别，语言建模，翻译，图像字幕。 基本模型设时间步 $t$ 的输入为 $\boldsymbol{x}_t \in \mathbb{R}^d$，标签为 $y_t$，隐藏状态 $\boldsymbol{h}_t \in \mathbb{R}^h$ 的计算表达式为 \boldsymbol{h}_t = \phi(\boldsymbol{W}_{hx} \boldsymbol{x}_t + \boldsymbol{W}_{hh} \boldsymbol{h}_{t-1} + b_h)其中 $\boldsymbol{W}_{hx} \in \mathbb{R}^{h \times d}$ 和 $\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$ 是隐藏层权重参数，$b_h$是偏差。设输出层权重参数 $\boldsymbol{W}_{yh} \in \mathbb{R}^{q \times h}$，时间步$t$ 的输出层变量 $\boldsymbol{o}_t \in \mathbb{R}^q$ 计算为 \boldsymbol{o}_t = \boldsymbol{W}_{yh} \boldsymbol{h}_{t} + b_y设时间步 $t$ 的损失为 $\ell(\boldsymbol{o}_t, y_t)$。时间步数为 $T$ 的损失函数 $L$ 定义为 L = \frac{1}{T} \sum_{t=1}^T \ell (\boldsymbol{o}_t, y_t)我们将 $L$ 叫做有关给定时间步数的数据样本的目标函数，简称目标函数。 模型计算图简单起见，我们将偏差 $b$ 忽略，且激活函数的输入输出相同。为了可视化模型变量和参数之间在计算中的依赖关系，绘制模型计算图，如图所示。例如，时间步3的隐藏状态 $\boldsymbol{h}_3$ 的计算依赖模型参数 $\boldsymbol{W}_{hx}, \boldsymbol{W}_{hh}$、上一时间步隐藏状态 $\boldsymbol{h}_2$ 以及当前时间步输入 $\boldsymbol{x}_3$。 梯度不稳定问题RNN容易出现梯度不稳定问题。这要从梯度的反向传播说起。 训练模型通常需要模型参数的梯度 $\partial L/\partial \boldsymbol{W}_{hx}$、$\partial L/\partial \boldsymbol{W}_{hh}$ 和 $\partial L/\partial \boldsymbol{W}_{yh}$。 上图中各数据的shape为$h_t:(h\times 1) \quad \boldsymbol{W}_{hx}:(h\times x) \quad \boldsymbol{o}_t:(y\times 1) \quad \boldsymbol{W}_{yh}:(y\times h) \quad \boldsymbol{W}_{hh}:(h\times h)$ 首先，目标函数有关各时间步输出层变量的梯度 $\partial L/\partial \boldsymbol{o}_t \in \mathbb{R}^q$可以很容易地计算： \frac{\partial L}{\partial \boldsymbol{o}_t} = \frac{\partial \ell (\boldsymbol{o}_t, y_t)}{T \cdot \partial \boldsymbol{o}_t}下面，计算目标函数有关模型参数 $\boldsymbol{W}_{yh}$ 的梯度 $\partial L/\partial \boldsymbol{W}_{yh} \in \mathbb{R}^{q \times h}$。根据模型图，$L$ 通过 $\boldsymbol{o}_1, \ldots, \boldsymbol{o}_T$ 依赖 $\boldsymbol{W}_{yh}$。依据链式法则 \frac{\partial L}{\partial \boldsymbol{W}_{yh}} = \sum_{t=1}^T \text{prod}(\frac{\partial L}{\partial \boldsymbol{o}_t}, \frac{\partial \boldsymbol{o}_t}{\partial \boldsymbol{W}_{yh}}) = \sum_{t=1}^T \frac{\partial L}{\partial \boldsymbol{o}_t} \boldsymbol{h}_t^\top其次，注意到隐藏状态之间也有依赖关系。 在上图，$L$ 只通过 $\boldsymbol{o}_T$ 依赖最终时间步 $T$ 的隐藏状态 $\boldsymbol{h}_T$。因此，我们先计算目标函数有关最终时间步隐藏状态的梯度 $\partial L/\partial \boldsymbol{h}_T \in \mathbb{R}^h$。依据链式法则，我们得到 \frac{\partial L}{\partial \boldsymbol{h}_T} = \text{prod}(\frac{\partial L}{\partial \boldsymbol{o}_T}, \frac{\partial \boldsymbol{o}_T}{\partial \boldsymbol{h}_T} ) = \boldsymbol{W}_{yh}^\top \frac{\partial L}{\partial \boldsymbol{o}_T}接下来，对于时间步 $t &lt; T$，$L$ 通过 $\boldsymbol{h}_{t+1}$ 和 $\boldsymbol{o}_t$ 依赖 $\boldsymbol{h}_t$。依据链式法则， 目标函数有关时间步 $t &lt; T$ 的隐藏状态的梯度 $\partial L/\partial \boldsymbol{h}_t \in \mathbb{R}^h$ 需要按照时间步从晚到早依次计算： \begin{align*} \frac{\partial L}{\partial \boldsymbol{h}_t} &= \text{prod}(\frac{\partial L}{\partial \boldsymbol{h}_{t+1}}, \frac{\partial \boldsymbol{h}_{t+1}}{\partial \boldsymbol{h}_t} ) + \text{prod}(\frac{\partial L}{\partial \boldsymbol{o}_t}, \frac{\partial \boldsymbol{o}_t}{\partial \boldsymbol{h}_t} ) \\ &= \boldsymbol{W}_{hh}^\top \frac{\partial L}{\partial \boldsymbol{h}_{t+1}} + \boldsymbol{W}_{yh}^\top \frac{\partial L}{\partial \boldsymbol{o}_t} \end{align*}将上面的递归公式展开，对任意时间步 $1 \leq t \leq T$，我们可以得到目标函数有关隐藏状态梯度的通项公式 \frac{\partial L}{\partial \boldsymbol{h}_t} = \sum_{i=t}^T {(\boldsymbol{W}_{hh}^\top)}^{T-i} \boldsymbol{W}_{yh}^\top \frac{\partial L}{\partial \boldsymbol{o}_{T+t-i}}由上式中的指数项可见，当时间步数 $T$ 较大或者时间步 $t$ 较小，目标函数有关隐藏状态的梯度较容易出现衰减和爆炸。这也会影响其他计算中包含 $\partial L / \partial \boldsymbol{h}_t$ 的梯度，例如隐藏层中模型参数的梯度 $\partial L / \partial \boldsymbol{W}_{hx} \in \mathbb{R}^{h \times d}$ 和 $\partial L / \partial \boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$。 在计算图中，$L$ 通过 $\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T$ 依赖这些模型参数。 依据链式法则，我们有 \begin{aligned} \frac{\partial L}{\partial \boldsymbol{W}_{hx}} &= \sum_{t=1}^T \text{prod}(\frac{\partial L}{\partial \boldsymbol{h}_t}, \frac{\partial \boldsymbol{h}_t}{\partial \boldsymbol{W}_{hx}}) \\ &= \sum_{t=1}^T \frac{\partial L}{\partial \boldsymbol{h}_t} \boldsymbol{x}_t^\top,\ \frac{\partial L}{\partial \boldsymbol{W}_{hh}} \\ &= \sum_{t=1}^T \text{prod}(\frac{\partial L}{\partial \boldsymbol{h}_t}, \frac{\partial \boldsymbol{h}_t}{\partial \boldsymbol{W}_{hh}}) = \sum_{t=1}^T \frac{\partial L}{\partial \boldsymbol{h}_t} \boldsymbol{h}_{t-1}^\top \end{aligned}每次迭代中，上述各个依次计算出的梯度会被依次存储或更新。这是为了避免重复计算。例如，由于隐藏状态梯度 $\partial L/\partial \boldsymbol{h}_t$ 被计算存储，之后的模型参数梯度 $\partial L/\partial \boldsymbol{W}_{hx}$ 和 $\partial L/\partial \boldsymbol{W}_{hh}$ 的计算可以直接读取 $\partial L/\partial \boldsymbol{h}_t$ 的值，而无需重复计算。 此外，反向传播对于各层中变量和参数的梯度计算可能会依赖通过正向传播计算出的各层变量的当前值。举例来说，参数梯度 $\partial L/\partial \boldsymbol{W}_{hh}$ 的计算需要依赖隐藏状态在时间步 $t = 0, \ldots, T-1$ 的当前值 $\boldsymbol{h}_t$（$\boldsymbol{h}_0$ 是初始化得到的）。这些值是通过从输入层到输出层的正向传播计算并存储得到的。 解决梯度爆炸问题为了应对梯度爆炸，我们可以裁剪梯度（clipping gradient）。假设我们把所有模型参数梯度的元素拼接成一个向量 $\boldsymbol{g}$，并设裁剪的阈值是 $\theta$。 \min\left(\frac{\theta}{|\boldsymbol{g}|}, 1\right)\boldsymbol{g}裁剪后梯度的 $L_2$ 范数不超过 $\theta$。 深度循环网络给定时间步 $t$ 的小批量输入$\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$（样本数为$n$，输入个数为$d$）。在深度循环神经网络中， 设该时间步第$l$隐藏层的隐藏状态为$\boldsymbol{H}_t^{(l)} \in \mathbb{R}^{n \times h}$（隐藏单元个数为$h$），输出层变量为$\boldsymbol{O}_t \in \mathbb{R}^{n \times q}$（输出个数为$q$），隐藏层的激活函数为$\phi$。第一隐藏层的隐藏状态和之前的计算一样： \boldsymbol{H}_t^{(1)} = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh}^{(1)} + \boldsymbol{H}_{t-1}^{(1)} \boldsymbol{W}_{hh}^{(1)} + \boldsymbol{b}_h^{(1)})其中权重$\boldsymbol{W}_{xh}^{(1)} \in \mathbb{R}^{d \times h}, \boldsymbol{W}_{hh}^{(1)} \in \mathbb{R}^{h \times h}$和偏差 $\boldsymbol{b}_h^{(1)} \in \mathbb{R}^{1 \times h}$分别为第一隐藏层的模型参数。 假设隐藏层个数为$L$，当$1 &lt; l \leq L$时，第$l$隐藏层的隐藏状态的表达式为 \boldsymbol{H}_t^{(l)} = \phi(\boldsymbol{H}_t^{(l-1)} \boldsymbol{W}_{xh}^{(l)} + \boldsymbol{H}_{t-1}^{(1)} \boldsymbol{W}_{hh}^{(l)} + \boldsymbol{b}_h^{(l)}),其中权重$\boldsymbol{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h}, \boldsymbol{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h}$和偏差 $\boldsymbol{b}_h^{(l)} \in \mathbb{R}^{1 \times h}$分别为第$l$隐藏层的模型参数。 最终，输出层的输出只需基于第$L$隐藏层的隐藏状态： \boldsymbol{O}_t = \boldsymbol{H}_t^{(L)} \boldsymbol{W}_{hy} + \boldsymbol{b}_y其中权重$\boldsymbol{W}_{hy} \in \mathbb{R}^{h \times q}$和偏差$\boldsymbol{b}_y \in \mathbb{R}^{1 \times q}$为输出层的模型参数。 深度循环神经网络的架构如下图所示。隐藏状态的信息不断传递至当前层的下一时间步和当前时间步的下一层。 门控循环单元(GRU)循环神经网络的梯度可能会衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但无法解决梯度衰减的问题。给定一个时间序列，例如文本序列，循环神经网络在实际中较难捕捉时间步距离较大的词之间的依赖关系。 门控循环神经网络（gated recurrent neural network）的提出，是为了更好地捕捉时间序列中时间步距离较大的依赖关系。其中，门控循环单元（gated recurrent unit，简称GRU）是一种常用的门控循环神经网络。 重置门和更新门假设隐藏单元个数为$h$，给定时间步$t$的小批量输入$\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$（样本数为$n$，输入个数为$d$）和上一时间步隐藏状态$\boldsymbol{H}_{t-1} \in \mathbb{R}^{n \times h}$。重置门（reset gate）$\boldsymbol{R}_t \in \mathbb{R}^{n \times h}$和更新门（update gate）$\boldsymbol{Z}_t \in \mathbb{R}^{n \times h}$的计算如下： \begin{aligned} \boldsymbol{R}_t = \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xr} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hr} + \boldsymbol{b}_r) \\ \boldsymbol{Z}_t = \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xz} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hz} + \boldsymbol{b}_z) \end{aligned}其中$\boldsymbol{W}_{xr}, \boldsymbol{W}_{xz} \in \mathbb{R}^{d \times h}$和$\boldsymbol{W}_{hr}, \boldsymbol{W}_{hz} \in \mathbb{R}^{h \times h}$是权重参数，$\boldsymbol{b}_r, \boldsymbol{b}_z \in \mathbb{R}^{1 \times h}$是偏移参数。激活函数$\sigma$是sigmoid函数。sigmoid函数可以将元素的值变换到0和1之间。因此，重置门$\boldsymbol{R}_t$和更新门$\boldsymbol{Z}_t$中每个元素的值域都是$[0, 1]$。 我们可以通过元素值域在$[0, 1]$的更新门和重置门来控制隐藏状态中信息的流动：通常可以应用按元素乘法符$\odot$。 候选隐藏状态接下来，时间步$t$的候选隐藏状态$\tilde{\boldsymbol{H}}_t \in \mathbb{R}^{n \times h}$的计算使用了值域在$[-1, 1]$的tanh函数做激活函数。它在之前描述的循环神经网络隐藏状态表达式的基础上，引入了重置门和按元素乘法： \tilde{\boldsymbol{H}}_t = \text{tanh}(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{R}_t \odot \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh} + \boldsymbol{b}_h)其中$\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}$和$\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$是权重参数，$\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$是偏移参数。需要注意的是，候选隐藏状态使用了重置门，从而控制包含时间序列历史信息的上一个时间步的隐藏状态如何流入当前时间步的候选隐藏状态。如果重置门近似0，上一个隐藏状态将被丢弃。因此，重置门可以丢弃与预测未来无关的历史信息。 隐藏状态最后，隐藏状态$\boldsymbol{H}_t \in \mathbb{R}^{n \times h}$的计算使用更新门$\boldsymbol{Z}_t$来对上一时间步的隐藏状态$\boldsymbol{H}_{t-1}$和当前时间步的候选隐藏状态$\tilde{\boldsymbol{H}}_t$做组合： \boldsymbol{H}_t = \boldsymbol{Z}_t \odot \boldsymbol{H}_{t-1} + (1 - \boldsymbol{Z}_t) \odot \tilde{\boldsymbol{H}}_t值得注意的是，更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新。假设更新门在时间步$t^\prime$到$t$（$t^\prime &lt; t$）之间一直近似1。那么，在时间步$t^\prime$到$t$之间的输入信息几乎没有流入时间步$t$的隐藏状态$\boldsymbol{H}_t$。 实际上，这可以看作是较早时刻的隐藏状态$\boldsymbol{H}_{t^\prime-1}$一直通过时间保存并传递至当前时间步$t$。 这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。 对门控循环单元的设计稍作总结： 重置门有助于捕捉时间序列里短期的依赖关系。 更新门有助于捕捉时间序列里长期的依赖关系。 长短期记忆(LSTM)另一种常用的可以解决梯度衰减的门控循环神经网络：长短期记忆（long short-term memory，简称LSTM）。它比门控循环单元的结构稍微更复杂一点。 输入门、遗忘门和输出门假设隐藏单元个数为$h$，给定时间步$t$的小批量输入$\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$（样本数为$n$，输入个数为$d$）和上一时间步隐藏状态$\boldsymbol{H}_{t-1} \in \mathbb{R}^{n \times h}$。 时间步$t$的输入门（input gate）$\boldsymbol{I}_t \in \mathbb{R}^{n \times h}$、遗忘门（forget gate）$\boldsymbol{F}_t \in \mathbb{R}^{n \times h}$和输出门（output gate）$\boldsymbol{O}_t \in \mathbb{R}^{n \times h}$分别计算如下： \begin{aligned} \boldsymbol{I}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xi} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hi} + \boldsymbol{b}_i) \\ \boldsymbol{F}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xf} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hf} + \boldsymbol{b}_f) \\ \boldsymbol{O}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xo} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{ho} + \boldsymbol{b}_o) \end{aligned}其中的$\boldsymbol{W}_{xi}, \boldsymbol{W}_{xf}, \boldsymbol{W}_{xo} \in \mathbb{R}^{d \times h}$和$\boldsymbol{W}_{hi}, \boldsymbol{W}_{hf}, \boldsymbol{W}_{ho} \in \mathbb{R}^{h \times h}$是权重参数，$\boldsymbol{b}_i, \boldsymbol{b}_f, \boldsymbol{b}_o \in \mathbb{R}^{1 \times h}$是偏移参数。激活函数$\sigma$是sigmoid函数。 和门控循环单元中的重置门和更新门一样，这里的输入门、遗忘门和输出门中每个元素的值域都是$[0, 1]$。 候选记忆细胞和门控循环单元中的候选隐藏状态一样，时间步$t$的的候选记忆细胞$\tilde{\boldsymbol{C}}_t \in \mathbb{R}^{n \times h}$也使用了值域在$[-1, 1]$的tanh函数做激活函数。它的计算和不带门控的循环神经网络的隐藏状态的计算没什么区别： \tilde{\boldsymbol{C}}_t = \text{tanh}(\boldsymbol{X}_t \boldsymbol{W}_{xc} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hc} + \boldsymbol{b}_c)其中的$\boldsymbol{W}_{xc} \in \mathbb{R}^{d \times h}$和$\boldsymbol{W}_{hc} \in \mathbb{R}^{h \times h}$是权重参数，$\boldsymbol{b}_c \in \mathbb{R}^{1 \times h}$是偏移参数。 记忆细胞我们可以通过元素值域在$[0, 1]$的输入门、遗忘门和输出门来控制隐藏状态中信息的流动：这通常可以应用按元素乘法符$\odot$。当前时间步记忆细胞$\boldsymbol{C}_t \in \mathbb{R}^{n \times h}$的计算组合了上一时间步记忆细胞和当前时间步候选记忆细胞的信息，并通过遗忘门和输入门来控制信息的流动： \boldsymbol{C}_t = \boldsymbol{F}_t \odot \boldsymbol{C}_{t-1} + \boldsymbol{I}_t \odot \tilde{\boldsymbol{C}}_t需要注意的是，如果遗忘门一直近似1且输入门一直近似0，过去的记忆细胞将一直通过时间保存并传递至当前时间步。 这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时序数据中间隔较大的依赖关系。 隐藏状态有了记忆细胞以后，接下来我们还可以通过输出门来控制从记忆细胞到隐藏状态$\boldsymbol{H}_t \in \mathbb{R}^{n \times h}$的信息的流动： \boldsymbol{H}_t = \boldsymbol{O}_t \odot \text{tanh}(\boldsymbol{C}_t)这里的tanh函数确保隐藏状态元素值在-1到1之间。需要注意的是，当输出门近似1，记忆细胞信息将传递到隐藏状态供输出层使用；当输出门近似0，记忆细胞信息只自己保留。 输出层在时间步$t$，长短期记忆的输出层计算和之前描述的循环神经网络输出层计算一样：我们只需将该时刻的隐藏状态$\boldsymbol{H}_t$传递进输出层，从而计算时间步$t$的输出。 本文转自动手深度学习并且参考理解循环神经网络]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>RNN</tag>
        <tag>GRU</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全卷积网络(FCN)]]></title>
    <url>%2F2018%2F06%2F22%2F%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%88FCN%EF%BC%89%2F</url>
    <content type="text"><![CDATA[CNN能够对图片进行分类，可是怎么样才能识别图片中特定部分的物体，在2015年之前还是一个未解难题。Jonathan Long发表了《Fully Convolutional Networks for Semantic Segmentation》为图像语义分割指明了方向。 核心思想FCN包含三个核心思想： 不含全连接层(fc)的全卷积(fully conv)网络。可适应任意尺寸输入。 增大数据尺寸的反卷积(deconv)层。能够输出精细的结果。 结合不同深度层结果的跳级(skip)结构。同时确保鲁棒性和精确性。 网络结构主体网络可以使用AlexNet、VGG、ResNet等，以往分类的网络通常会在最后使用全连接层，将原来二维特征图转换成一维的固定长度的特征向量，最后输出一个特定长度的向量，表示输入图像属于每一类的概率，以此作为分类的标签。但这样会丢失关键的空间信息，无法应用在像素级别的分类上。 与传统CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类（全连接层＋softmax）不同，FCN可以接受任意尺寸的输入图像，然后通过卷积转置层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在与输入图等大小的特征图上对每个像素进行分类，逐像素地用softmax分类计算损失,相当于每个像素对应一个训练样本。 转置卷积层此处的转置卷积即是上采样（upsample）。当然关于这个名字不同框架不同，Caffe和Kera里叫Deconvolution，而tensorflow里叫conv_transpose。CS231n这门课中说，叫conv_transpose更为合适。 众所诸知，普通的池化会缩小图片的尺寸，比如VGG16 五次池化后图片被缩小了32倍。为了得到和原图等大的分割图，我们需要上采样。 上采样和卷积类似，都是相乘相加的运算。只不过后者是多对一，前者是一对多。而反卷积的前向和后向传播，只用颠倒卷积的前后向传播即可。所以无论优化还是后向传播算法都是没有问题。图解如下： 可以理解upsample就是使大小比原图像小得多的特征图变大，使其大小为原图像大小。 跳级结构对网络最后一层（第5层）的输出（32倍缩小）上采样到原图大小，得到的结果还是不够精确，一些细节无法恢复。于是Jonathan将第4层的输出和第3层的输出也依次反卷积，分别需要16倍和8倍上采样，结果就精细一些了。 训练 第一阶段，训练一个完整的分类网络。 从特征小图（16*16*4096）预测分割小图（16*16*21），之后直接上采样到原图尺寸。上采样（橙色）的步长为32，这个网络称为FCN-32s。 在第二次升采样前，把第4个pooling层（绿色）的预测结果（蓝色）融合进来。使用跳级结构提升精确性。第二次上采样步长为16，这个网络称为FCN-16s。 进一步融合了第3个pooling层的预测结果。第三次上采样步长为8，记为FCN-8s。 参考文献[1]. FCN 简单梳理[2]. 图像分割[3]. Fully Convolutional Networks for Semantic Segmentation[4]. Convolution arithmetic]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>FCN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mxnet源码解读之Dataset与DataLoader]]></title>
    <url>%2F2018%2F06%2F22%2FGluon%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B9%8BDataLoader%2F</url>
    <content type="text"><![CDATA[本文介绍mxnet中数据加载的两个类Dataset与DataLoader。 Dataset基类 所有的DataSet类继承自Dataset抽象类，且必须实现__getitem__和__len__方法。 transform(self, fn, lazy=True)。在原数据集上应用fn方法得到一个新的数据集。如果lazy为False，立即返回一个新的数据集，如果lazy为True，返回的是一个含有转换方法的类，在真正使用数据的时候再进行转换。fn是转换函数，针对每一个样本。 transform_first(self, fn, lazy=True)，只对第一个元素做transform操作，例如只对data做操作，而保持label不变123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172class Dataset(object): """Abstract dataset class. All datasets should have this interface. Subclasses need to override `__getitem__`, which returns the i-th element, and `__len__`, which returns the total number elements. .. note:: An mxnet or numpy array can be directly used as a dataset. """ def __getitem__(self, idx): raise NotImplementedError def __len__(self): raise NotImplementedError def transform(self, fn, lazy=True): """Returns a new dataset with each sample transformed by the transformer function `fn`. Parameters ---------- fn : callable A transformer function that takes a sample as input and returns the transformed sample. lazy : bool, default True If False, transforms all samples at once. Otherwise, transforms each sample on demand. Note that if `fn` is stochastic, you must set lazy to True or you will get the same result on all epochs. Returns ------- Dataset The transformed dataset. """ trans = _LazyTransformDataset(self, fn) if lazy: return trans return SimpleDataset([i for i in trans]) def transform_first(self, fn, lazy=True): """Returns a new dataset with the first element of each sample transformed by the transformer function `fn`. This is useful, for example, when you only want to transform data while keeping label as is. Parameters ---------- fn : callable A transformer function that takes the first elemtn of a sample as input and returns the transformed element. lazy : bool, default True If False, transforms all samples at once. Otherwise, transforms each sample on demand. Note that if `fn` is stochastic, you must set lazy to True or you will get the same result on all epochs. Returns ------- Dataset The transformed dataset. """ def base_fn(x, *args): if args: return (fn(x),) + args return fn(x) return self.transform(base_fn, lazy) def _fork(self): """Protective operations required when launching multiprocess workers.""" # for non file descriptor related datasets, just skip pass ImageFolderDataset派生类 root是存放图片的根目录。如root/car/0001.jpg root/car/xxxa.jpg root/car/yyyb.jpg flag表示要不要转换为3通道，If 0, always convert loaded images to greyscale (1 channel). If 1, always convert loaded images to colored (3 channels).1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class ImageFolderDataset(dataset.Dataset): """A dataset for loading image files stored in a folder structure like:: root/car/0001.jpg root/car/xxxa.jpg root/car/yyyb.jpg root/bus/123.jpg root/bus/023.jpg root/bus/wwww.jpg Parameters ---------- root : str Path to root directory. flag : &#123;0, 1&#125;, default 1 If 0, always convert loaded images to greyscale (1 channel). If 1, always convert loaded images to colored (3 channels). transform : callable, default None A function that takes data and label and transforms them: :: transform = lambda data, label: (data.astype(np.float32)/255, label) Attributes ---------- synsets : list List of class names. `synsets[i]` is the name for the integer label `i` items : list of tuples List of all images in (filename, label) pairs. """ def __init__(self, root, flag=1, transform=None): self._root = os.path.expanduser(root) self._flag = flag self._transform = transform self._exts = ['.jpg', '.jpeg', '.png'] self._list_images(self._root) def _list_images(self, root): self.synsets = [] self.items = [] for folder in sorted(os.listdir(root)): path = os.path.join(root, folder) if not os.path.isdir(path): warnings.warn('Ignoring %s, which is not a directory.'%path, stacklevel=3) continue # 给每个文件夹从0开始生成一个label，将对应的文件夹名称放到synsets里面 label = len(self.synsets) self.synsets.append(folder) # 将文件夹中所有的filename加入到self.items中 for filename in sorted(os.listdir(path)): filename = os.path.join(path, filename) # 得到扩展名，判断下是不是支持的扩展名 ext = os.path.splitext(filename)[1] if ext.lower() not in self._exts: warnings.warn('Ignoring %s of type %s. Only support %s'%( filename, ext, ', '.join(self._exts))) continue self.items.append((filename, label)) def __getitem__(self, idx): img = image.imread(self.items[idx][0], self._flag) label = self.items[idx][1] if self._transform is not None: return self._transform(img, label) return img, label def __len__(self): return len(self.items) DataLoader dataset：一个Dataset的对象。 batch_size：批量大小。 shuffle：是否打乱次序。 last_batch：当最后一个批量大小不足batch_size时，如何处理。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788class DataLoader(object): """Loads data from a dataset and returns mini-batches of data. Parameters ---------- dataset : Dataset Source dataset. Note that numpy and mxnet arrays can be directly used as a Dataset. batch_size : int Size of mini-batch. shuffle : bool Whether to shuffle the samples. sampler : Sampler The sampler to use. Either specify sampler or shuffle, not both. last_batch : &#123;'keep', 'discard', 'rollover'&#125; How to handle the last batch if batch_size does not evenly divide `len(dataset)`. keep - A batch with less samples than previous batches is returned. discard - The last batch is discarded if its incomplete. rollover - The remaining samples are rolled over to the next epoch. batch_sampler : Sampler A sampler that returns mini-batches. Do not specify batch_size, shuffle, sampler, and last_batch if batch_sampler is specified. batchify_fn : callable Callback function to allow users to specify how to merge samples into a batch. Defaults to `default_batchify_fn`:: def default_batchify_fn(data): if isinstance(data[0], nd.NDArray): return nd.stack(*data) elif isinstance(data[0], tuple): data = zip(*data) return [default_batchify_fn(i) for i in data] else: data = np.asarray(data) return nd.array(data, dtype=data.dtype) num_workers : int, default 0 The number of multiprocessing workers to use for data preprocessing. `num_workers &gt; 0` is not supported on Windows yet. """ def __init__(self, dataset, batch_size=None, shuffle=False, sampler=None, last_batch=None, batch_sampler=None, batchify_fn=None, num_workers=0): self._dataset = dataset if batch_sampler is None: if batch_size is None: raise ValueError("batch_size must be specified unless " \ "batch_sampler is specified") if sampler is None: if shuffle: sampler = _sampler.RandomSampler(len(dataset)) else: sampler = _sampler.SequentialSampler(len(dataset)) elif shuffle: raise ValueError("shuffle must not be specified if sampler is specified") batch_sampler = _sampler.BatchSampler( sampler, batch_size, last_batch if last_batch else 'keep') elif batch_size is not None or shuffle or sampler is not None or \ last_batch is not None: raise ValueError("batch_size, shuffle, sampler and last_batch must " \ "not be specified if batch_sampler is specified.") self._batch_sampler = batch_sampler self._num_workers = num_workers if num_workers &gt;= 0 else 0 if batchify_fn is None: if num_workers &gt; 0: self._batchify_fn = default_mp_batchify_fn else: self._batchify_fn = default_batchify_fn else: self._batchify_fn = batchify_fn def __iter__(self): if self._num_workers == 0: generator = lambda: [(yield self._batchify_fn([self._dataset[idx] for idx in batch])) for batch in self._batch_sampler] return generator() # multi-worker return _MultiWorkerIter(self._num_workers, self._dataset, self._batchify_fn, self._batch_sampler) def __len__(self): return len(self._batch_sampler)]]></content>
      <categories>
        <category>深度学习</category>
        <category>mxnet</category>
      </categories>
      <tags>
        <tag>mxnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测之SSD]]></title>
    <url>%2F2018%2F06%2F20%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BSSD%2F</url>
    <content type="text"><![CDATA[SSD：Single Shot MultiBox Detector，ECCV 2016 的一篇文章，作者是UNC Chapel Hill（北卡罗来纳大学教堂山分校）的Wei Liu大神。是截至目前是主要的检测框架之一。相比Faster RCNN有明显的速度优势，相比YOLO又有明显的mAP优势。： 总体概述SSD模型的示意图如下。给定输入图片，其首先使用主要由卷积层组成的模块来进行特征抽取。在其特征输出上，我们以每个像素为中心构建多个锚框，然后使用softmax来对每个锚框判断其包含的物体类别，以及用卷积直接预测它到真实物体边界框的距离。卷积层的输出同时被输入到一个高宽减半模块（往上的箭头）来缩小图片尺寸。这个模块的输入将进入到另一个卷积模块抽取特征，并构建锚框来预测物体类别和边界框。这样设计的目的是在不同的尺度上进行目标检测，例如前一层的锚框主要检测图片中尺寸较小的物体，而后一层则检测尺寸稍大的物体。重复这一过程多次以保证在多种不同的尺度下检测物体。 网络结构主体网络用来从原始图像抽取特征，一般会选择常用的深度卷积神经网络。论文中使用了VGG16。在conv5_3的38*38*512feature map之后，经过3*3*1024的卷积，再经过1*1*1024的卷积得到19*19*1024的卷积，后续网络也是这样。最终做分类预测，非极大值抑制。网络结构中Classifer有两个并列的3*3的卷积核，一个是用来做分类一个用来做回归。 金字塔模型YOLO和SSD都使用基础网络（如 VGG-16）提取feature map。但YOLO只对一个feature map获取信息，而SSD对基础网络产生的feature map后接继续卷积层，产生另外一个feature map，后面再加卷积层又有一个feature map，这样每个feature map都可以产生预测值，由此加入了多尺度的概念。所以上图中SSD结构的倒数第二列的数字8732综合了各个feature map上所有的prior box，是这么来的38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4=8732 训练训练主要涉及匹配策略、难例挖掘和数据曾广。 匹配策略在训练过程中，需要确定哪些默认边界框对应实际边界框的检测，并相应地训练网络。我们首先将每个实际边界框与具有最高的IOU的边界框相匹配。与MultiBox不同的是，我们将默认边界框匹配到重叠高于阈值（0.5）的任何实际边界框，当做正例，否则为负例。这简化了学习问题，允许网络为多个重叠的默认边界框预测高分，而不是要求它只挑选具有最大重叠的一个边界框。 卷积预测YOLO中对feature map产生预测使用的是全连接层，参数较多，SSD改为使用卷积产生预测，卷积核一般是$3\times3\times p$，其中 $p=k\times (Classes+1)$ 或$p=k\times4$，k是anchors的个数。$3\times3$是经验值，它可能不能覆盖锚框定义的区域。所以我们需要保证前面的卷积层能有效的将较大的锚框区域的特征浓缩到一个$3\times3$的窗口里。 使用一个保持宽高比的$3\times3$的卷积层预测类别，每个pixel上生成num_anchors*(num_classes+1)个值。 对每个锚框我们需要预测如何将其变换到真实的物体边界框。变换由一个长为4的向量来描述，分别表示左下和右上的x、y轴坐标偏移。与类别预测类似，这里我们同样使用一个保持高宽的$3\times3$卷积层来输出偏移预测。 损失函数和Faster RCNN的基本一样，由分类和回归两部分组成，可以参考Faster RCNN。回归部分的loss是希望预测的box和prior box的差距尽可能跟ground truth和prior box的差距接近，这样预测的box就能尽量和ground truth一样。 SSD训练目标函数来自于MultiBox目标，但扩展到处理多个目标类别。设$x_{ij}^p = \lbrace 1,0 \rbrace$是第$i$个默认边界框匹配到类别 $p$ 的第$j$个实际边界框的指示器，即只有正例进行惩罚。在上面的匹配策略中，我们有$\sum_i x_{ij}^p \geq 1$。总体目标损失函数是定位损失（loc）和置信度损失（conf）的加权和： L(x, c, l, g) = \frac{1}{N}(L_{conf}(x, c) + \alpha L_{loc}(x, l, g))其中$N$是匹配的默认边界框的数量。如果$N=0$，则将损失设为0。定位损失是预测框(l)与真实框(g)参数之间的Smooth L1损失。类似于Faster RCNN，我们回归默认边界框$(d)$的中心偏移量$(cx, cy)$和其宽度$(w)$、高度$(h)$的偏移量。 L_{loc}(x,l,g) = \sum_{i \in Pos}^N \sum_{m \in \lbrace cx, cy, w, h \rbrace} x_{ij}^k \mathtt{smooth}_{L1}(l_{i}^m - \hat{g}_j^m) \\ \hat{g}_j^{cx} = (g_j^{cx} - d_i^{cx}) / d_i^w \quad \quad \hat{g}_j^{cy} = (g_j^{cy} - d_i^{cy}) / d_i^h \\ \hat{g}_j^{w} = \log\Big(\frac{g_j^{w}}{d_i^w}\Big) \quad \quad \hat{g}_j^{h} = \log\Big(\frac{g_j^{h}}{d_i^h}\Big)置信度损失是在多类别置信度$(c)$上的softmax损失。 L_{conf}(x, c) = - \sum_{i\in Pos}^N x_{ij}^p log(\hat{c}_i^p) - \sum_{i\in Neg} log(\hat{c}_i^0)\quad \mathtt{where}\quad\hat{c}_i^p = \frac{\exp(c_i^p)}{\sum_p \exp(c_i^p)}置信度损失是分类预测损失。对于每个正例预测，我们根据相应类的置信度得分来惩罚损失。对于负匹配预测，我们根据类“0”的置信度得分来惩罚损失：类“0”表示没有检测到对象。通过交叉验证权重项$\alpha$设为1。 难例挖掘在匹配步骤之后，大多数默认边界框为负例。这在正例和负例之间引入了显著的不平衡。我们不使用所有负例，而是按照每个默认边界框的最高置信度损失来排序，并挑选损失最高的负例，以便负例和正例之间的比例最多为3:1。这会带来更快的优化和更稳定的训练。 默认anchors与正负样本在金字塔模型中，每个feature map有不同的尺寸，假设有m层金字塔。Default box与输入图片比例为 $s_k$： s_k=s_{min}+\frac{s_{max}-s_{min}}{m-1}(k-1),\qquad k\in[1,m]其中 $s_{min}=0.2,s_{max}=0.95$ 表示 default box 的边长最小占整幅图片的20%，最大占95%。Default box边长的长款比例为 $a_r\in\{1,2,3,\frac{1}{2},\frac{1}{3}\}$，并且对长宽比是 1 的增加一个 $s_k’=\sqrt{s_ks_{k+1}}$。所以每个网格对应 6 个 default boxes。于是我们有 边框中心为 $(\frac{i+0.5}{|f_k|},\frac{j+0.5}{|f_k|})$，$|f_k|$ 为第 $k$ 层 feature map 的尺寸，$i,j\in[0,|f_k|]$； 边框边长为 $(w_k^a=s_k\sqrt{a_r},h_k^a=\frac{s_k}{\sqrt{a_r}})$ 例如有四个default anchors时 如上图所示，在训练期间，SSD仅需要每个目标的输入图像和真实边界框，首先将这些默认边界框与grount truth box按照iou进行匹配。匹配成功则这个prior box就是positive example（正样本），如果匹配不上，就是negative example（负样本）。显然这样产生的负样本的数量要远远多于正样本。这里将负样本按照置信度进行排序，选择最高的num_sel个prior box使得最后正、负样本的比例在 1：3 左右。 数据增广每一张训练图像，随机的进行如下几种选择： 使用整个原始输入图像。 采样一个图像块，使得与目标之间的最小Jaccard重叠为0.1，0.3，0.5，0.7或0.9。 随机采样一个图像块。 每个采样图像块的大小是原始图像大小的$[0.1，1]$，长宽比在$\frac {1} {2}$和2之间。如果实际边界框的中心在采用的图像块中，就保留实际边界框与采样图像块的重叠部分。在上述采样步骤之后，将每个采样图像块resize到固定尺寸并以0.5的概率进行水平翻转。 这样一个样本被诸多batch_sampler采样器采样后会生成多个候选样本，然后从中随机选一个样本送人网络训练。 参考文献[1]. 动手深度学习[2]. 卷积神经网络(CNN)反向传播算法[3]. SSD: Single Shot MultiBox Detector[4]. SSD论文翻译[5]. SSD论文笔记[6]. SSD object detection: Single Shot MultiBox Detector for real-time processing]]></content>
      <categories>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>SSD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测之Faster RCNN]]></title>
    <url>%2F2018%2F06%2F20%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8BFaster%20RCNN%2F</url>
    <content type="text"><![CDATA[本文介绍目标检测算法Faster RCNN。 网络结构网络结构如图所示。 Conv LayersFaster R-CNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。 Region Proposal Networks(RPN)RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得比较精确的proposals。 ROI Pooling该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。 Classification利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。 首先对于一副任意大小 $P \times Q$ 的图像，首先缩放至固定大小 $M\times N$ ，然后将 $M\times N$ 图像送入网络；卷积层可以使用ResNet或者VGG提取特征；RPN网络首先经过 $3\times 3$ 卷积，再分别生成foreground anchors与bounding box regression偏移量，然后计算出proposals；而ROI Pooling则从proposals提取固定大小的proposal feature送入后续全连接和softmax网络作classification。 特征提取一般使用VGG或ResNet进行特征提取。本文以VGG为例。 VGG这里以 VGG16 为例。 VGG16 图片分类时，输入为 $224×224×3$ 的张量。网络结构最后采用FC层(而不是Conv层)得到固定长度的向量，以进行图片分类。对最后一个卷积层的输出拉伸为rank 1的张量，然后送入FC层。 Faster RCNN 采用 VGG16 的中间卷积层的输出，即最后一个Max Pooling前的部分，因此不用关心输入的尺寸。 卷积特征图保持相对于原始图片所编码“things”的位置。 例如，如果在图片的左上角存在一个红色正方形，而且卷积层有激活响应，那么该红色正方形的信息被卷积层编码后，仍在卷积特征图的左上角。但其空间维度降低，即分辨率比原图小得多。 ResNetResNet结构逐渐取代VGG作为基础网络，用于提取特征。ResNet相对于VGG的明显优势是，网络更大，因此具有更强的学习能力，而且训练比较容易。 区域建议区域建议为后续网络生成候选框。RCNN、Fast RCNN中的selective search都很耗时，而Faster RCNN则抛弃了这种方法，直接使用RPN生成检测框，极大提升检测框的生成速度。结构如下。 上图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals，并选择部分送入ROI layer。 Anchors这里有必要对Anchors（锚框）介绍一下。所谓anchors，实际上就是一组由rpn生成的矩形。假设输入数据的空间尺寸为 $h$ 和 $w$，那么大小为 $s\in (0,1]$ 和比例为 $r &gt; 0$ 的锚框形状是 \left( ws\sqrt{r}, \ \frac{hs}{\sqrt{r}}\right),论文中使用三种不同的areas $128\times128, 256\times256, 512\times421$ 和三种不同的比例 $1:1,1:2,2:1$，即每一个中心点可以产生9个anchors。遍历feature maps，为每一个点都配备这9种anchors作为初始的检测框。 原文中，Conv Layers生成的feature map有256个通道，相当于feature map每个点都是256 dimensions，$3\times 3$卷积之后的通道还是256d，假设每个点上有k个anchor（默认为9），而每个anhcor要分foreground和background，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有[x, y, w, h]对应4个坐标，所以reg=4k coordinates。 具体是由$1\times 1$ 卷积来做，如之前的图示，经过该卷积的输出大小为 $W\times H \times 18$，这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是foreground和background，所有这些信息都保存 $W\times H\times (9\cdot2)$ 大小的矩阵。后面接softmax分类获得foreground anchors。在softmax前后的reshape layer其实只是为了方便Caffe保存数据，不属于网络的内容。 需要理解的是，虽然anchors是基于卷积特征图定义的，但最终的anchos是相对于原始图片的。 由于只有卷积层和pooling层，特征图的维度是与原始图片的尺寸成比例关系的。 如果图片尺寸 $w×h$，特征图的尺寸是 $w/r×h/r$。 其中，$r$ 是下采样率(subsampling ratio)，这样，原图中的框和特征图上的锚框是对应的，只不过分辨率下降。 Bounding Box回归生成的box可能对目标有正确的分类，但定位不准。所以我们希望采用一种方法box进行微调，使得foreground anchors和GT更加接近。 对于窗口一般使用四维向量 $(x, y, w, h)$ 表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框P代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口 $G$ 更接近的回归窗口 $G’$ ，即给定：anchor $(P_x, P_y, P_w, P_h)$ 和 $GT=[G_{x}, G_{y}, G_{w}, G_{h}]$，寻找一种变换F，使得：$F(P_x, P_y, P_w, P_h)=(G_{x}’, G_{y}’, G_{w}’, G_{h}’)$，其中$(G_{x}’, G_{y}’, G_{w}’, G_{h}’)≈(G_{x}, G_{y}, G_{w}, G_{h})$ 。 那么经过何种变换 $F$ 才能从anchor $P$ 变为 $G’$？ 思路就是: 先做平移 $(\Delta x, \Delta y)$， $\Delta x = P_w d_x(P), \Delta y = P_h d_y(P)$ ，RCNN论文中： G_{x}' = P_w d_x(P) + P_x \\ G_{y}' = P_h d_y(P) + P_y 然后再做尺度缩放 $(S_w, S_h), S_w = exp(d_w(P)), S_h = exp(d_h(P))$，对应论文中： G_{x}'= P_w exp(d_w(P) ) \\ G_{y}'= P_h exp(d_h(P) ) bounding box regression就是学习 $d_x(P), d_y(P), d_w(P), d_h(P)$ 这四个变换。当输入的anchor P与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调。 接下来的问题就是如何通过线性回归获得 $d_x(P), d_y(P), d_w(P), d_h(P)$ 了。线性回归就是给定输入的特征向量 $X$, 学习一组参数 $W$, 使得经过线性回归后的值跟真实值 $Y$ 非常接近，即 $Y=WX$。对于该问题，输入 $X$ 是cnn feature map，定义为 $\phi$；同时还有训练传入P与GT之间的变换量，即 $(t_{x}, t_{y}, t_{w}, t_{h})$。输出是 $d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)$ 四个变换。那么目标函数可以表示为： d_*(P) = w_*^T\phi(P)其中 $\phi(P)$ 是对应anchor的feature map组成的特征向量，$w$ 是需要学习的参数，$d(P)$ 是得到的预测值（$*$表示 $x，y，w，h$，也就是每一个变换对应一个上述目标函数）。为了让预测值 $(t_{x}, t_{y}, t_{w}, t_{h})$ 与真实值差距最小，设计损失函数： Loss = \sum_i^N(t_*^i - \hat w_*^T\phi_5(P^i))^2函数优化目标为： W_* = argmin_{w_*} \sum_i^N(t_*^i - \hat w_*^T\phi(P^i))^2 + \lambda || \hat w_*||^2需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。说完原理，对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 $(t_x, t_y)$ 与尺度因子 $(t_w, t_h)$ 如下： t_x = (G_x - P_x) / P_w \\ t_y = (G_y - P_y) / P_h \\ t_w = \log (G_w / P_w) \\ t_h = \log(G_h / P_h)对于训练bouding box regression网络回归分支，输入是cnn feature $\phi$，监督信号是Anchor与GT的差距 $(t_x, t_y, t_w, t_h)$，即训练目标是：输入 $\phi$ 的情况下使网络输出与监督信号尽可能接近。那么当bouding box regression工作时，再输入 $\phi$ 时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 $(t_x, t_y, t_w, t_h)$，显然即可用来修正Anchor位置了。 在Faster RCNN中，通过 $1\times1$ 的卷积来得到 $[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$ 变换量。输出为36，表示9个anchors，每个有4个变换值 Proposal LayerProposal Layer负责综合所有 [d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)] 变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer。 Proposal Layer 做了以下事情： 利用 $[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$ 对所有的anchors做bbox regression回归； 将超出原图像素范围的anchors的clip到原图像素范围； 剔除非常小（width&lt;threshold or height&lt;threshold）的foreground anchors； 做nms。会按照iou剔除冗余的框，并且得分从高到低排序； 在做nms后的结果中，提取前n(e.g. 2000)个再加上gt_box的结果进行sampler，具体是通过与gt_box的iou得出哪些是前景foreground，哪些是background，哪些需要ignore，并且按score选出前m个(e.g. 128, 128)作为ROI pooling layer的输入。 ROI池化对于传统的CNN，由于全连接的存在，输入的图像尺寸必须是固定值。如果输入图像大小不定，有2种解决办法：1. 从图像中crop一部分传入网络；2.将图像warp成需要的大小后传入网络。但这两种方法要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。RPN网络生成的proposals也存在上述问题。 Faster R-CNN中提出了RoI Pooling解决这个问题。RoI Pooling是从Spatial Pyramid Pooling发展而来。首先将proposals映射回feature map的尺度，之后将每个proposal水平和竖直分为pooled_w和pooled_h份，对每一份都进行max pooling处理。这样处理后，即使大小不同的proposal，输出结果都是 $pooled_w\times pooled_h$ 大小，实现了fixed-length output。原文中的输出空间尺寸是 $7\times7$。 RCNN分类与边框预测R-CNN 是 Faster R-CNN 框架中的最后一个步骤，采用FC层来输出每个可能的 object 类别class 的score。例如对于$512\times7\times7$ 的feature map，首先做一个全局平均，就拉平成 $512\times1\times1$ 的向量，然后再接两个不同的全连接层处理： 一个全连接层有 $N+1$ 个神经单元，其中 $N$ 是类别class的总数。再通过softmax得到类别。 一个全连接层有 $4N$ 个神经单元。回归预测输出，得到 $N$ 个可能的类别classes分别预测 $Δcenterx,Δcentery,Δwidth,Δheight$。 训练训练过程中是降低Loss的过程。Loss有两个方面：RPN的Loss和RCNN的Loss。 RPN LossRPN有两种类型的预测值输出：foreground与background分类和边界框回归调整。 训练时，对所有的anchors分类为三种类别，foreground，background，ignore，与gt_box的iou大于0.7的anchors作为foreground；大于0小于0.3的作为background，其余的ignore。并且采用动态batches，因为即使已经尝试保持background和foreground的anchors的平衡比例，也不总是可行的。根据图片中gt_box和anchors的尺度与比例，很有可能得不到foreground。 这种情况时，将采用与gt_box具有最大iou的anchors. 这与理想情况相差很远，但实际中一般总能有foreground样本和要学习目标. 然后，随机采样anchors来生成batchsize=256的mini-batch，尽可能的保持foreground和background的比例平衡。 对mini-batch内的所有anchors计算分类loss，采用SigmoidBinaryCrossEntropyLoss。 只对mini-batch内标记为foreground的box计算回归loss。Faster RCNN没有采用简单的L1或L2 loss用于回归误差，而是采用Smooth L1 loss。 将两个loss相加就是rpn网络的loss。 RCNN LossRCNN网络也有两种预测输出：类别预测和边界框调整。 对于输入roi pooling的proposal，计算它与任意gt_box的iou，大于0.7就认为是正确的box，0.1到0.7之间的设为background。边界框回归的目标计算的是正例proposal与其对应的gt_box间的偏移和缩放。 分类loss使用SoftmaxCrossEntropyLoss，边框回归使用Smooth L1 loss。最后将两个loss加在一起。 参考文献[1]. 一文读懂Faster RCNN[2]. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks[3]. Faster R-CNN - 目标检测详解[4]. Faster R-CNN: Down the rabbit hole of modern object detection]]></content>
      <categories>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>RCNN</tag>
        <tag>Faster RCNN</tag>
        <tag>ROI</tag>
        <tag>RPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fine Tuning]]></title>
    <url>%2F2018%2F06%2F20%2F%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一般来说我们自己需要做的方向，比如在一些特定的领域的识别分类中，我们很难拿到大量的数据。因为像在ImageNet上毕竟是一个千万级的图像数据库，通常我们可能只能拿到几千张或者几万张某一特定领域的图像，比如识别衣服啊、标志啊、生物种类等等。在这种情况下重新训练一个新的网络是比较复杂的，而且参数不好调整，数据量也不够，因此fine-tuning微调就是一个比较理想的选择。 所谓fine tune就是用别人训练好的模型，加上我们自己的数据，来训练新的模型。fine tune相当于使用别人的模型的前几层，来提取浅层特征，然后在最后再落入我们自己的分类中。 fine tune的好处在于不用完全重新训练模型，从而提高效率，因为一般新训练模型准确率都会从很低的值开始慢慢上升，但是fine tune能够让我们在比较少的迭代次数之后得到一个比较好的效果。在数据量不是很大的情况下，fine tune会是一个比较好的选择。但是如果你希望定义自己的网络结构的话，就需要从头开始了。 我们可以在ImageNet上1000类分类训练好的参数的基础上，根据我们的分类识别任务进行特定的微调。 微调由下面四步构成： 在源数据（例如 ImageNet）上训练一个神经网络 A 。 创建一个新的神经网络 B，它复制了A上除了输出层外的所有模型参数。我们假设这些模型参数含有源数据上学习到的知识，且这些知识同样适用于目标数据集。但最后的输出层跟源数据标注紧密相关，所以不被重用。 为B添加一个输出大小为目标数据集类别数目（例如一百类椅子）的输出层，并将其权重初始化成随机值。 在目标数据集（例如椅子数据集）上训练B。我们将从头开始学习输出层，但其余层都是基于源数据上的模型参数进行微调。 参考文献[1]. 微调[2]. Caffe 使用记录（三）finetune]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>迁移学习</tag>
        <tag>Fine Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux遇到的问题]]></title>
    <url>%2F2018%2F06%2F19%2FLinux_Problems_And_Solutions%2F</url>
    <content type="text"><![CDATA[windows与linux互传文件的便捷方法使用winscp，具体参考Windows和linux(ubuntu)互传文件简便快捷的方法。 可能会遇到拒绝连接的情况，这说明没有建立好通信。操作系统要想使用ssh协议进行通信，就要提供ssh客户端服务和ssh服务器端服务。默认情况下，ubuntu中只提供了ssh客户端服务，而没有提供ssh服务器端服务。所以windows和它通信就不成功。我们只需要进入到linux操作系统ubuntu中安装好ssh服务器端服务就行了。 具体参考使用WinSCP软件在windows和ubuntu中进行文件传输。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GluonCV中Faster-RCNN源码解读]]></title>
    <url>%2F2018%2F06%2F17%2FGluonCV%E4%B8%ADFaster-RCNN%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[本文梳理GluonCV中Faster RCNN源码。 RPN网络RPN网络由RPN这个类实现。主要包括生成anchors，得到score，bounding box回归，非极大值抑制。 预生成大量的的anchors。在RPNAnchorGenerator里面实现。 根据feature map的尺寸slice相应数量的anchors。 使用 $1\times1$ 的卷积和sigmoid函数得到每个anchor是foregroud的分数。注意：原论文中对于每一个anchor会产生两个值，分别为foreground和background的分数，然后再用softmax，这点与Gluon-CV中的实现不一样，但效果一样。 使用 $1\times1$ 的卷积得到每个anchor的偏移值，以便之后做bounding box回归。 做非极大值抑制，按score从大到小的顺序得到前n个anchors的分数和坐标等信息。由RPNProposal这个类实现。 RPN类的代码如下。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118class RPN(gluon.HybridBlock): r"""Region Proposal Network. Parameters ---------- channels : int Channel number used in convolutional layers. stride : int Feature map stride with respect to original image. This is usually the ratio between original image size and feature map size. base_size : int The width(and height) of reference anchor box. ratios : iterable of float The aspect ratios of anchor boxes. We expect it to be a list or tuple. scales : iterable of float The areas of anchor boxes. We use the following form to compute the shapes of anchors: .. math:: width_&#123;anchor&#125; = size_&#123;base&#125; \times scale \times \sqrt&#123; 1 / ratio&#125; height_&#123;anchor&#125; = size_&#123;base&#125; \times scale \times \sqrt&#123;ratio&#125; alloc_size : tuple of int Allocate size for the anchor boxes as (H, W). Usually we generate enough anchors for large feature map, e.g. 128x128. Later in inference we can have variable input sizes, at which time we can crop corresponding anchors from this large anchor map so we can skip re-generating anchors for each input. nms_thresh : float, default is 0.7 IOU threshold for NMS. It is used to remove overlapping proposals. train_pre_nms : int, default is 12000 Filter top proposals before NMS in training. train_post_nms : int, default is 2000 Return top proposal results after NMS in training. test_pre_nms : int, default is 6000 Filter top proposals before NMS in testing. test_post_nms : int, default is 300 Return top proposal results after NMS in testing. min_size : int, default is 16 Proposals whose size is smaller than ``min_size`` will be discarded. stds : tuple of float Standard deviation to be multiplied from encoded regression targets. These values must be the same as stds used in RPNTargetGenerator. weight_initializer : mxnet.initializer, default is mx.init.Normal(0.01) Weight intializer for RPN convolutional layers. """ def __init__(self, channels, stride, base_size=16, ratios=(0.5, 1, 2), scales=(8, 16, 32), alloc_size=(128, 128), nms_thresh=0.7, train_pre_nms=12000, train_post_nms=2000, test_pre_nms=6000, test_post_nms=300, min_size=16, stds=(1., 1., 1., 1.), weight_initializer=None, **kwargs): super(RPN, self).__init__(**kwargs) if weight_initializer is None: weight_initializer = mx.init.Normal(0.01) with self.name_scope(): self.anchor_generator = RPNAnchorGenerator( stride, base_size, ratios, scales, alloc_size) anchor_depth = self.anchor_generator.num_depth self.region_proposaler = RPNProposal( nms_thresh, train_pre_nms, train_post_nms, test_pre_nms, test_post_nms, min_size, stds) self.conv1 = nn.HybridSequential() self.conv1.add( nn.Conv2D(channels, 3, 1, 1, weight_initializer=weight_initializer)) self.conv1.add(nn.Activation('relu')) # use sigmoid instead of softmax, reduce channel numbers self.score = nn.Conv2D(anchor_depth, 1, 1, 0, weight_initializer=weight_initializer) self.loc = nn.Conv2D(anchor_depth * 4, 1, 1, 0, weight_initializer=weight_initializer) # pylint: disable=arguments-differ def hybrid_forward(self, F, x, img): """Forward RPN. The behavior during traing and inference is different. Parameters ---------- x : mxnet.nd.NDArray or mxnet.symbol Feature tensor. img : mxnet.nd.NDArray or mxnet.symbol The original input image. Returns ------- (rpn_score, rpn_box) Returns predicted scores and regions which are candidates of objects. """ # 生成anchors anchors = self.anchor_generator(x) # 经过3*3的卷积，通道和宽高都没变 x = self.conv1(x) # 经过1*1的卷积，通道变为anchor_depth，即为ratios*scales，然后再调整为batch*anchors*1 # 然后经过sigmoid，得到是前景的分数，0-1之间 raw_rpn_scores = self.score(x).transpose(axes=(0, 2, 3, 1)).reshape((0, -1, 1)) rpn_scores = F.sigmoid(raw_rpn_scores) # 经过1*1卷积，通道变为anchor_depth*4, 然后再调整为batch*ahchors*4 # 得到的是bounding box regression的四个变换量 rpn_box_pred = self.loc(x).transpose(axes=(0, 2, 3, 1)).reshape((0, -1, 4)) # 得到真正的符合条件的roi的score，坐标。在RPNProposal这个类里面实现（做了bounding box偏移和nms）。 # self.region_proposaler的功能： # 1.做bounding box偏移 # 2.将超出图像像素的框clip到图像像素范围 # 3.滤掉宽高小于一定阈值的box # 4.做nms # 5.从nms的结果中得到前n个box的scor和坐标 # 所以加入输入的有m个box，但输出的只有n个，训练是是2000个 rpn_score, rpn_box = self.region_proposaler( anchors, rpn_scores, rpn_box_pred, img) # 如果是训练，返回（给sampler的box的分数，给sampler的box的坐标，原始anchor的score，预测的anchors偏移，原始的anchors） if autograd.is_training(): # return raw predictions as well in training for bp return rpn_score, rpn_box, raw_rpn_scores, rpn_box_pred, anchors # 如果是做预测，返回roi的score和坐标 return rpn_score, rpn_box 生成anchorsanchors生成的总体思路是，初始化的时候先在 $128\times128$ 尺寸的特征图上预生成所有的anchors。之后在计算输入图像的anchors时，只需slice相应的尺寸，这样避免做不必要的重复生成操作。 预生成操作的代码为anchors = self._generate_anchors(stride, base_size, ratios, scales, alloc_size)。slice操作的代码为a = F.slice_like(anchors, x * 0, axes=(2, 3))。 例如，anchors有5种像素，3种宽高比，则特征图上每个像素会生成15个anchors，大小为 $15\times4$。$128\times128$的像素中，预生成的anchors尺寸大小是 $128\times128\times60$。如果真正图像的feature map的空间尺寸为 $38\times50$，那么只需在预生成的anchors中slice $38\times50$ 大小的anchors即可，即 $38\times50\times60$。 需要注意的是，初始化中有一个参数stride，它的具体含义是原图空间尺寸和feature map的空间尺寸之比。例如原图的尺寸是 $512\times512$，经过CNN网络之后，feature map的大小为 $32\times32$，缩放了16倍，则stride为16。这样做的目的是为了可以从feature map对应到原始图像。 RPNProposal类详解RPNProposal这个类主要是用来产生ROI（Region of interest）。 先做一个bounding box regression，但是这里没有单独的loss，实际上只是通过前面CNN生成的bbox_pred，将anchors进行一个平移加缩放。代码为roi = self._box_decoder(bbox_pred, self._box_to_center(anchor))。 将超出原图像素范围的anchors的clip到原图像素范围。例如某个anchor为[-33, -12, 877, 912]，而原图像素为 $600\times800$，则把anchor变为[0, 0, 799, 599]，代码为roi = F.Custom(roi, img, op_type=&#39;bbox_clip_to_image&#39;)。 将宽度或者高度小于某个阈值的anchors的score变为0，坐标变为-1，目的是剔除这些anchors。 做nms。会将小于阈值的anchors的score变为-1，坐标也变为-1，并且按照score从大到小排序。 从nms得到的anchors中找出取前n个，返回其score和坐标。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106class RPNProposal(gluon.HybridBlock): """Proposal generator for RPN. RPNProposal takes RPN anchors, RPN prediction scores and box regression preditions. It will transform anchors, apply NMS to get clean foreground proposals. Parameters ---------- nms_thresh : float, default is 0.7 IOU threshold for NMS. It is used to remove overlapping proposals. train_pre_nms : int, default is 12000 Filter top proposals before NMS in training. train_post_nms : int, default is 2000 Return top proposal results after NMS in training. test_pre_nms : int, default is 6000 Filter top proposals before NMS in testing. test_post_nms : int, default is 300 Return top proposal results after NMS in testing. min_size : int, default is 16 Proposals whose size is smaller than ``min_size`` will be discarded. stds : tuple of float Standard deviation to be multiplied from encoded regression targets. These values must be the same as stds used in RPNTargetGenerator. """ def __init__(self, nms_thresh=0.7, train_pre_nms=12000, train_post_nms=2000, test_pre_nms=6000, test_post_nms=300, min_size=16, stds=(1., 1., 1., 1.)): super(RPNProposal, self).__init__() self._box_to_center = BBoxCornerToCenter() self._box_decoder = NormalizedBoxCenterDecoder(stds=stds) # self._clipper = BBoxClipToImage() # self._compute_area = BBoxArea() self._nms_thresh = nms_thresh self._train_pre_nms = max(1, train_pre_nms) self._train_post_nms = max(1, train_post_nms) self._test_pre_nms = max(1, test_pre_nms) self._test_post_nms = max(1, test_post_nms) self._min_size = min_size #pylint: disable=arguments-differ def hybrid_forward(self, F, anchor, score, bbox_pred, img): """ Generate proposals. Limit to batch-size=1 in current implementation. """ if autograd.is_training(): pre_nms = self._train_pre_nms post_nms = self._train_post_nms else: pre_nms = self._test_pre_nms post_nms = self._test_post_nms with autograd.pause(): # restore bounding boxes # self._box_to_center将anchor从(xmin, ymin, xmax, ymax)的形式转换为(center_x, center_y, width, height)的形式 # self._box_decoder做平移和缩放，即bounding box regression roi = self._box_decoder(bbox_pred, self._box_to_center(anchor)) # clip rois to image's boundary # 将超出原图像素范围的anchors的clip到原图像素范围 roi = F.Custom(roi, img, op_type='bbox_clip_to_image') # roi = self._clipper(roi, width, height) # remove bounding boxes that don't meet the min_size constraint # by setting them to (-1, -1, -1, -1) # width = roi.slice_axis(axis=-1, begin=2, end=3) # height = roi.slice_axis(axis=-1, begin=3, end=None) xmin, ymin, xmax, ymax = roi.split(axis=-1, num_outputs=4) # 计算所有anchors的宽高 width = xmax - xmin height = ymax - ymin # TODO:(zhreshold), there's im_ratio to handle here, but it requires # add' info, and we don't expect big difference # 找出所有宽或高小于self._min_size的anchors，用&gt;0表示，其他的都是0 invalid = (width &lt; self._min_size) + (height &lt; self._min_size) # # remove out of bound anchors # axmin, aymin, axmax, aymax = F.split(anchor, axis=-1, num_outputs=4) # # it's a bit tricky to get right/bottom boundary in hybridblock # wrange = F.arange(0, 2560).reshape((1, 1, 1, 2560)).slice_like( # img, axes=(3)).max().reshape((1, 1, 1)) # hrange = F.arange(0, 2560).reshape((1, 1, 2560, 1)).slice_like( # img, axes=(2)).max().reshape((1, 1, 1)) # invalid = (axmin &lt; 0) + (aymin &lt; 0) + F.broadcast_greater(axmax, wrange) + \ # F.broadcast_greater(aymax, hrange) # 每个invalid对应的anchors的score变为0 score = F.where(invalid, F.zeros_like(invalid), score) # 将invalid复制四份，即最后一个维度为4，因为roi的最后一个维度也是4 invalid = F.repeat(invalid, axis=-1, repeats=4) # 每个invalid对应的anchors的roi变为-1，（注意：不能变成0，因为roi里面本来就有0元素） roi = F.where(invalid, F.ones_like(invalid) * -1, roi) # Non-maximum suppression # 将score和roi拼接在一起，最后一个维度上，第一个元素代表score，后面四个元素代表坐标 pre = F.concat(score, roi, dim=-1) # 做非极大值抑制，按score从大到小排列，并且会将小于阈值的的score和坐标变为-1 tmp = F.contrib.box_nms(pre, overlap_thresh=self._nms_thresh, topk=pre_nms, coord_start=1, score_index=0, id_index=-1, force_suppress=True) # slice post_nms number of boxes # 选前post_nms个候选框 result = F.slice_axis(tmp, axis=1, begin=0, end=post_nms) # 得到这些候选框的score rpn_scores = F.slice_axis(result, axis=-1, begin=0, end=1) # 得到这些候选框的坐标 rpn_bbox = F.slice_axis(result, axis=-1, begin=1, end=None) return rpn_scores, rpn_bbox RCNNTargetSampler#得到每个box最大iou的索引，并且要大于self._threshold，阈值默认0.7，如果小于就为-1。例如，输入：x=[[0, 0, 0.3], [0.8, 0, 0.1], [0.2, 0.1, 0.9]]，输出：[-1, 0, 2]123456789101112131415161718192021222324class MaximumMatcher(gluon.HybridBlock): """A Matcher implementing maximum matching strategy. Parameters ---------- threshold : float Matching threshold. """ def __init__(self, threshold): super(MaximumMatcher, self).__init__() self._threshold = threshold def hybrid_forward(self, F, x): # x保存的是每个roi与每个gt_box的iou，例如两个roi，3个gt_box，则为[[0, 0, 0.3], [0.6, 0, 0.1]] # argmax得到与那个gt_box的iou最大，对应的索引，如返回[2, 0] argmax = F.argmax(x, axis=-1) # F.pick(x, argmax, axis=-1)按照argmax的索引获取对应的值 # &gt;= self._threshold=0.7 与上一步得到的值比较，得到0 1矩阵 # F.where()按照上一步得到的0 1矩阵，得到每个roi和哪个gt_box的iou最大，如果是0就复制为-1 # match最终得到roi和哪个gt_box的iou最大，如果iou小于阈值，则对应位置的值为-1 match = F.where(F.pick(x, argmax, axis=-1) &gt;= self._threshold, argmax, F.ones_like(argmax) * -1) return match 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172class RCNNTargetSampler(gluon.HybridBlock): """A sampler to choose positive/negative samples from RCNN Proposals Parameters ---------- num_sample : int, default is 128 Number of samples for RCNN targets. pos_iou_thresh : float, default is 0.5 Proposal whose IOU larger than ``pos_iou_thresh`` is regarded as positive samples. neg_iou_thresh_high : float, default is 0.5 Proposal whose IOU smaller than ``neg_iou_thresh_high`` and larger than ``neg_iou_thresh_low`` is regarded as negative samples. Proposals with IOU in between ``pos_iou_thresh`` and ``neg_iou_thresh`` are ignored. neg_iou_thresh_low : float, default is 0.0 See ``neg_iou_thresh_high``. pos_ratio : float, default is 0.25 ``pos_ratio`` defines how many positive samples (``pos_ratio * num_sample``) is to be sampled. """ def __init__(self, num_sample=128, pos_iou_thresh=0.5, neg_iou_thresh_high=0.5, neg_iou_thresh_low=0.0, pos_ratio=0.25): super(RCNNTargetSampler, self).__init__() self._num_sample = num_sample self._pos_iou_thresh = pos_iou_thresh self._neg_iou_thresh_high = neg_iou_thresh_high self._neg_iou_thresh_low = neg_iou_thresh_low self._pos_ratio = pos_ratio self._matcher = MaximumMatcher(pos_iou_thresh) #pylint: disable=arguments-differ def hybrid_forward(self, F, roi, gt_box): """ Only support batch_size=1 now. """ with autograd.pause(): # cocnat rpn roi with ground truths # 去掉axis=0这个单维度，并且合并roi和gtbox all_roi = F.concat(roi.squeeze(axis=0), gt_box.squeeze(axis=0), dim=0) # calculate ious between (N, 4) anchors and (M, 4) bbox ground-truths # ious is (N, M) # 计算all_roi与每个gt_box的ious， ious = F.contrib.box_iou(all_roi, gt_box, format='corner').transpose((1, 0, 2)) # 得到每个roi最大的iou值，具体里面会有一个阈值，iou小于阈值的roi赋值为-1 matches = self._matcher(ious) # 得到哪些是负样本-1，哪些是正样本1，哪些需要忽略0 samples = F.Custom(matches, ious, op_type='quota_sampler', num_sample=self._num_sample, pos_thresh=self._pos_iou_thresh, neg_thresh_high=self._neg_iou_thresh_high, neg_thresh_low=self._neg_iou_thresh_low, pos_ratio=self._pos_ratio) # 上一步得到的是类似这样的矩阵[[-1, 0, 1, 0, ...]]， 把第一个单维度去掉 samples = samples.squeeze(axis=0) # remove batch axis # 之前的matchs得到的是这样的矩阵[[-1, -1, -1, 3, 4, -1,...]]，把第一个维度去掉 matches = matches.squeeze(axis=0) # shuffle and argsort, take first num_sample samples # 将samples中为0的值（需要忽略的roi）变为-999，并且赋值给sf_samples sf_samples = F.where(samples == 0, F.ones_like(samples) * -999, samples) # F.argsort得到sf_samples从小到大排序的索引，slice_axis得到前self._num_sample个index indices = F.argsort(sf_samples, is_ascend=False).slice_axis( axis=0, begin=0, end=self._num_sample) # 从all_roi中将对应indices的元素拿出来，再加上一个维度 new_roi = all_roi.take(indices).expand_dims(0) # 从samples中将对应indices的元素拿出来，再加上一个维度 new_samples = samples.take(indices).expand_dims(0) # 从matchs中得到对应indices的元素，再加上一个维度 new_matches = matches.take(indices).expand_dims(0) return new_roi, new_samples, new_matches 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204class FasterRCNN(RCNN): r"""Faster RCNN network. Parameters ---------- features : gluon.HybridBlock Base feature extractor before feature pooling layer. top_features : gluon.HybridBlock Tail feature extractor after feature pooling layer. train_patterns : str Matching pattern for trainable parameters. scales : iterable of float The areas of anchor boxes. We use the following form to compute the shapes of anchors: .. math:: width_&#123;anchor&#125; = size_&#123;base&#125; \times scale \times \sqrt&#123; 1 / ratio&#125; height_&#123;anchor&#125; = size_&#123;base&#125; \times scale \times \sqrt&#123;ratio&#125; ratios : iterable of float The aspect ratios of anchor boxes. We expect it to be a list or tuple. classes : iterable of str Names of categories, its length is ``num_class``. roi_mode : str ROI pooling mode. Currently support 'pool' and 'align'. roi_size : tuple of int, length 2 (height, width) of the ROI region. stride : int, default is 16 Feature map stride with respect to original image. This is usually the ratio between original image size and feature map size. rpn_channel : int, default is 1024 Channel number used in RPN convolutional layers. rpn_train_pre_nms : int, default is 12000 Filter top proposals before NMS in training of RPN. rpn_train_post_nms : int, default is 2000 Return top proposal results after NMS in training of RPN. rpn_test_pre_nms : int, default is 6000 Filter top proposals before NMS in testing of RPN. rpn_test_post_nms : int, default is 300 Return top proposal results after NMS in testing of RPN. nms_thresh : float, default is 0.3. Non-maximum suppression threshold. You can speficy &lt; 0 or &gt; 1 to disable NMS. nms_topk : int, default is 400 Apply NMS to top k detection results, use -1 to disable so that every Detection result is used in NMS. post_nms : int, default is 100 Only return top `post_nms` detection results, the rest is discarded. The number is based on COCO dataset which has maximum 100 objects per image. You can adjust this number if expecting more objects. You can use -1 to return all detections. num_sample : int, default is 128 Number of samples for RCNN targets. pos_iou_thresh : float, default is 0.5 Proposal whose IOU larger than ``pos_iou_thresh`` is regarded as positive samples. neg_iou_thresh_high : float, default is 0.5 Proposal whose IOU smaller than ``neg_iou_thresh_high`` and larger than ``neg_iou_thresh_low`` is regarded as negative samples. Proposals with IOU in between ``pos_iou_thresh`` and ``neg_iou_thresh`` are ignored. neg_iou_thresh_low : float, default is 0.0 See ``neg_iou_thresh_high``. pos_ratio : float, default is 0.25 ``pos_ratio`` defines how many positive samples (``pos_ratio * num_sample``) is to be sampled. """ def __init__(self, features, top_features, scales, ratios, classes, roi_mode, roi_size, stride=16, rpn_channel=1024, rpn_train_pre_nms=12000, rpn_train_post_nms=2000, rpn_test_pre_nms=6000, rpn_test_post_nms=300, num_sample=128, pos_iou_thresh=0.5, neg_iou_thresh_high=0.5, neg_iou_thresh_low=0.0, pos_ratio=0.25, **kwargs): super(FasterRCNN, self).__init__( features, top_features, classes, roi_mode, roi_size, **kwargs) self.stride = stride self._max_batch = 1 # currently only support batch size = 1 self._max_roi = 100000 # maximum allowed ROIs self._target_generator = set([RCNNTargetGenerator(self.num_class)]) with self.name_scope(): self.rpn = RPN(rpn_channel, stride, scales=scales, ratios=ratios, train_pre_nms=rpn_train_pre_nms, train_post_nms=rpn_train_post_nms, test_pre_nms=rpn_test_pre_nms, test_post_nms=rpn_test_post_nms) self.sampler = RCNNTargetSampler(num_sample, pos_iou_thresh, neg_iou_thresh_high, neg_iou_thresh_low, pos_ratio) @property def target_generator(self): """Returns stored target generator Returns ------- mxnet.gluon.HybridBlock The RCNN target generator """ return list(self._target_generator)[0] # pylint: disable=arguments-differ def hybrid_forward(self, F, x, gt_box=None): """Forward Faster-RCNN network. The behavior during traing and inference is different. Parameters ---------- x : mxnet.nd.NDArray or mxnet.symbol The network input tensor. gt_box : type, only required during training The ground-truth bbox tensor with shape (1, N, 4). Returns ------- (ids, scores, bboxes) During inference, returns final class id, confidence scores, bounding boxes. """ feat = self.features(x) # RPN proposals if autograd.is_training(): # rpn网络的输出，例如 # rpn_box shape 1*2000*4 # raw_rpn_score shape 1*28500*1 # raw_rpn_box shape 1*28500*4 # anchors shape 1*28500*4 _, rpn_box, raw_rpn_score, raw_rpn_box, anchors = self.rpn( feat, F.zeros_like(x)) # sample 128 roi assert gt_box is not None # rpn_box：加上样本lebel之后选出的rpn_box，例如shape=1*128*4 # samples：得到rpn_box是正样本1，还是负样本-1，还是忽略0 例如 shape=1*128 # matches：rpn_box匹配哪个样本，-1代表忽略的，其他值是真是的 例如 shape=1*128 rpn_box, samples, matches = self.sampler(rpn_box, gt_box) else: _, rpn_box = self.rpn(feat, F.zeros_like(x)) # create batchid for roi with autograd.pause(): # 例如self._max_batch是1，self._max_roi是3，则产生如下[[0, 0, 0]] roi_batchid = F.arange( 0, self._max_batch, repeat=self._max_roi).reshape( (-1, self._max_roi)) # roi_batchid变成和rpn_box数量一致，例如shape 1*128 roi_batchid = F.slice_like(roi_batchid, rpn_box * 0, axes=(0, 1)) # rpn_roi把roi_batchid和rpn_box连接在一起，例如 shape为128*5，这样是之后接口中rpn_box第二个维度上有五个值 rpn_roi = F.concat(*[roi_batchid.reshape((-1, 1)), rpn_box.reshape((-1, 4))], dim=-1) # ROI features # 做roi pooling layer，得到固定空间大小的特征输出，如有128个roi，feature map的通道是1024，空间大小是14*14，那么得到的输出shape是128*1024*14*14 if self._roi_mode == 'pool': pooled_feat = F.ROIPooling(feat, rpn_roi, self._roi_size, 1. / self.stride) elif self._roi_mode == 'align': pooled_feat = F.contrib.ROIAlign(feat, rpn_roi, self._roi_size, 1. / self.stride) else: raise ValueError("Invalid roi mode: &#123;&#125;".format(self._roi_mode)) # RCNN prediction # 再次用卷积网络提取特征，例如shape从128*1024*14*14变为128*2048*7*7的shape top_feat = self.top_features(pooled_feat) # top_feat = F.Pooling(top_feat, global_pool=True, pool_type='avg', kernel=self._roi_size) # 在top_feat上面做一个全局平均，例如shape变为128*2048*1*1 top_feat = self.global_avg_pool(top_feat) # 接全连接网络预测类别，例如shape变为128*21 cls_pred = self.class_predictor(top_feat) # 接全连接网络预测bounding box，例如shape变为20*128*4 box_pred = self.box_predictor(top_feat).reshape( (-1, self.num_class, 4)).transpose((1, 0, 2)) # no need to convert bounding boxes in training, just return # 训练过程中，返回如下 # cls_pred：全连接网络最后的预测类型 # box_pred: 全连接网络最后的box偏移 # rpn_box：加上lebal的所有box偏移 # samples：rpn_box是正样本1，还是负样本-1，还是忽略0 # matches：rpn_box匹配哪个样本，-1代表忽略的，其他值是真实类别 # raw_rpn_score：原始产生的boxscore # raw_rpn_box：原始产生的box坐标偏移 # anchors：原始产生的anchors if autograd.is_training(): box_pred = box_pred.transpose((1, 0, 2)) return (cls_pred, box_pred, rpn_box, samples, matches, raw_rpn_score, raw_rpn_box, anchors) # translate bboxes bboxes = self.box_decoder(box_pred, self.box_to_center(rpn_box)).split( axis=0, num_outputs=self.num_class, squeeze_axis=True) cls_ids, scores = self.cls_decoder(F.softmax(cls_pred, axis=-1)) results = [] for i in range(self.num_class): cls_id = cls_ids.slice_axis(axis=-1, begin=i, end=i+1) score = scores.slice_axis(axis=-1, begin=i, end=i+1) # per class results per_result = F.concat(*[cls_id, score, bboxes[i]], dim=-1) results.append(per_result) result = F.concat(*results, dim=0).expand_dims(0) if self.nms_thresh &gt; 0 and self.nms_thresh &lt; 1: result = F.contrib.box_nms( result, overlap_thresh=self.nms_thresh, topk=self.nms_topk, id_index=0, score_index=1, coord_start=2) if self.post_nms &gt; 0: result = result.slice_axis(axis=1, begin=0, end=self.post_nms).squeeze(axis=0) ids = F.slice_axis(result, axis=-1, begin=0, end=1) scores = F.slice_axis(result, axis=-1, begin=1, end=2) bboxes = F.slice_axis(result, axis=-1, begin=2, end=6) return ids, scores, bboxes target怎么来在FasterRCNNDefaultTrainTransform这个类的call方法里面。 先将img resize到输入网络的尺寸，如600*800；然后img随机翻转，gt_box也做相应翻转； 输入中有net的信息，就可以知道img经过net之后的feature map大小，如1024*38*50； 从预生成的anchors中slice相应的anchors； 计算anchors与gt_box之间的iou； 根据iou信息最终得到 box_target：正样本box跟gt_box之间要学习的平移和缩放距离；box_mask：box_target对应的mask；cls_target：哪些是正样本，哪些是负样本，哪些需要忽略。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114class FasterRCNNDefaultTrainTransform(object): """Default Faster-RCNN training transform. Parameters ---------- short : int, default is 600 Resize image shorter side to ``short``. max_size : int, default is 1000 Make sure image longer side is smaller than ``max_size``. net : mxnet.gluon.HybridBlock, optional The faster-rcnn network. .. hint:: If net is ``None``, the transformation will not generate training targets. Otherwise it will generate training targets to accelerate the training phase since we push some workload to CPU workers instead of GPUs. mean : array-like of size 3 Mean pixel values to be subtracted from image tensor. Default is [0.485, 0.456, 0.406]. std : array-like of size 3 Standard deviation to be divided from image. Default is [0.229, 0.224, 0.225]. box_norm : array-like of size 4, default is (1., 1., 1., 1.) Std value to be divided from encoded values. num_sample : int, default is 256 Number of samples for RPN targets. pos_iou_thresh : float, default is 0.7 Anchors larger than ``pos_iou_thresh`` is regarded as positive samples. neg_iou_thresh : float, default is 0.3 Anchors smaller than ``neg_iou_thresh`` is regarded as negative samples. Anchors with IOU in between ``pos_iou_thresh`` and ``neg_iou_thresh`` are ignored. pos_ratio : float, default is 0.5 ``pos_ratio`` defines how many positive samples (``pos_ratio * num_sample``) is to be sampled. """ def __init__(self, short=600, max_size=1000, net=None, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), box_norm=(1., 1., 1., 1.), num_sample=256, pos_iou_thresh=0.7, neg_iou_thresh=0.3, pos_ratio=0.5, **kwargs): self._short = short self._max_size = max_size self._mean = mean self._std = std self._anchors = None if net is None: return # use fake data to generate fixed anchors for target generation ashape = 128 # in case network has reset_ctx to gpu # 将rpn网络进行深拷贝，所以之后rpn网络有变化，这里的anchor_generator也会变 anchor_generator = copy.deepcopy(net.rpn.anchor_generator) anchor_generator.collect_params().reset_ctx(None) anchors = anchor_generator( mx.nd.zeros((1, 3, ashape, ashape))).reshape((1, 1, ashape, ashape, -1)) self._anchors = anchors # record feature extractor for infer_shape if not hasattr(net, 'features'): raise ValueError("Cannot find features in network, it is a Faster-RCNN network?") self._feat_sym = net.features(mx.sym.var(name='data')) from ....model_zoo.rpn.rpn_target import RPNTargetGenerator self._target_generator = RPNTargetGenerator( num_sample=num_sample, pos_iou_thresh=pos_iou_thresh, neg_iou_thresh=neg_iou_thresh, pos_ratio=pos_ratio, stds=box_norm, **kwargs) def __call__(self, src, label): """Apply transform to training image/label.""" # resize shorter side but keep in max_size # 假如src.shape=(w*h*c)=(375*500*3) label shape=(10*6) h, w, _ = src.shape # imag shape=(600*800*3) img = timage.resize_short_within(src, self._short, self._max_size) # bbox shape=(10*6)，做相同比例缩放或拉伸 bbox = tbbox.resize(label, (w, h), (img.shape[1], img.shape[0])) # random horizontal flip # h=600 w=800 h, w, _ = img.shape # 随机左右翻转，flips记录翻转类型，上下翻转，左右翻转 img, flips = timage.random_flip(img, px=0.5) # bbox也相应进行翻转 bbox = tbbox.flip(bbox, (w, h), flip_x=flips[0]) # to tensor # img shape变为3*600*800，并且在值在0-1之间，但并没有归一化 img = mx.nd.image.to_tensor(img) # 归一化操作 img = mx.nd.image.normalize(img, mean=self._mean, std=self._std) if self._anchors is None: return img, bbox.astype(img.dtype) # generate RPN target so cpu workers can help reduce the workload # feat_h, feat_w = (img.shape[1] // self._stride, img.shape[2] // self._stride) # 推断特征图尺寸 600//16=38, 800//16=50 oshape=(1, 1024, 38, 50) oshape = self._feat_sym.infer_shape(data=(1, 3, img.shape[1], img.shape[2]))[1][0] # 从预生成的anchors取出对应尺寸(38*50)的anchor，然后resize，ahchor shape=(28500*4), 28500=38*50*15 anchor = self._anchors[:, :, :oshape[2], :oshape[3], :].reshape((-1, 4)) # 得到bbox的坐标，gt_bboxes shape=(1*10*4) gt_bboxes = mx.nd.array(bbox[np.newaxis, :, :4]) # box_target得到正样本box跟gt_box之间要学习的平移和缩放距离 # box_mask得到box_target对应的mask # cls_target得到哪些是正样本，哪些是负样本，哪些需要忽略 cls_target, box_target, box_mask = self._target_generator( gt_bboxes, anchor, img.shape[2], img.shape[1]) # img：reshape、normalzie之后的图像 # bbox：reshape之后的gt_box # cls_target[0]：哪些是正样本，哪些是负样本，哪些需要忽略，跟之前一样，只是降了一个维度 # box_target[0]：正样本box跟gt_box之间要学习的平移和缩放距离，跟之前一样，只是降了一个维度 # box_mask[0]：box_target对应的mask，跟之前一样，只是降了一个维度 return img, bbox.astype(img.dtype), cls_target[0], box_target[0], box_mask[0] iou大于等于0.7的正样本，大于0，小于等于0.3之间的负样本，iou为0的忽略，正负样本一种256个，尽量均匀123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class RPNTargetGenerator(gluon.Block): """RPN target generator network. Parameters ---------- num_sample : int, default is 256 Number of samples for RPN targets. pos_iou_thresh : float, default is 0.7 Anchor with IOU larger than ``pos_iou_thresh`` is regarded as positive samples. neg_iou_thresh : float, default is 0.3 Anchor with IOU smaller than ``neg_iou_thresh`` is regarded as negative samples. Anchors with IOU in between ``pos_iou_thresh`` and ``neg_iou_thresh`` are ignored. pos_ratio : float, default is 0.5 ``pos_ratio`` defines how many positive samples (``pos_ratio * num_sample``) is to be sampled. stds : array-like of size 4, default is (1., 1., 1., 1.) Std value to be divided from encoded regression targets. allowed_border : int or float, default is 0 The allowed distance of anchors which are off the image border. This is used to clip out of border anchors. You can set it to very large value to keep all anchors. """ def __init__(self, num_sample=256, pos_iou_thresh=0.7, neg_iou_thresh=0.3, pos_ratio=0.5, stds=(1., 1., 1., 1.), allowed_border=0): super(RPNTargetGenerator, self).__init__() self._num_sample = num_sample self._pos_iou_thresh = pos_iou_thresh self._neg_iou_thresh = neg_iou_thresh self._pos_ratio = pos_ratio self._allowed_border = allowed_border self._bbox_split = BBoxSplit(axis=-1) self._matcher = CompositeMatcher([BipartiteMatcher(), MaximumMatcher(pos_iou_thresh)]) self._sampler = QuotaSampler(num_sample, pos_iou_thresh, neg_iou_thresh, 0., pos_ratio) self._cls_encoder = SigmoidClassEncoder() self._box_encoder = NormalizedBoxCenterEncoder(stds=stds) # pylint: disable=arguments-differ def forward(self, bbox, anchor, width, height): """ Only support batch_size=1 now. Be careful there's numpy operations inside """ F = mx.nd with autograd.pause(): # anchor with shape (N, 4) # 假如anchor的shape为(N, 4)，这里提取第二个维度的值，所以得到的shape为(N, 1) a_xmin, a_ymin, a_xmax, a_ymax = self._bbox_split(anchor) # invalid anchor mask with shape (N, 1) # imask中保存无效的anchor的信息，即那些超出像素的。对应位置为true imask = ( (a_xmin &gt;= -self._allowed_border) * (a_ymin &gt;= -self._allowed_border) * (a_xmax &lt;= (width + self._allowed_border)) * (a_ymax &lt;= (height + self._allowed_border))) &lt;= 0 # imask中保存无效的anchor的位置信息，例如第六个和二十个anchor无效，则imask为[6, 20] imask = mx.nd.array(np.where(imask.asnumpy() &gt; 0)[0], ctx=anchor.context) # calculate ious between (N, 4) anchors and (M, 4) bbox ground-truths # ious is (N, M) # 计算anchor与每个bbox的iou，加入anchor为N*4，bbox为1*M*4，则iou为1*N*M ious = F.contrib.box_iou(anchor, bbox, format='corner').transpose((1, 0, 2)) # 将无效的anchor对应的ious填充为0 ious[:, imask, :] = -1 # 得到每个bbox与哪个gt_box匹配，背景为-1，阈值0.7，例如[[-1, -1, 0, -1, 2, 1 ]] matches = self._matcher(ious) # samples得到哪些是正样本，哪些是负样本，以及哪些是需要忽略的样本，正负样本加起来一共256个 samples = self._sampler(matches, ious) # training targets for RPN # samples中-1：负样本，0：忽略，1正样本，而cls_target中，-1：忽略，0：负样本，1：正样本 cls_target, _ = self._cls_encoder(samples) # box_target得到正样本box跟gt_box之间要学习的平移和缩放距离 # box_mask得到box_target对应的mask # cls_target得到哪些是正样本，哪些是负样本，哪些需要忽略 box_target, box_mask = self._box_encoder( samples, matches, anchor.expand_dims(axis=0), bbox) return cls_target, box_target, box_mask 假设x shape=(1285007)match[0] shape=(1*28500)，表示哪个box与哪个gt_box的的iou最高，里面只有7个值&gt;=0，其他都是-1 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class BipartiteMatcher(gluon.HybridBlock): """A Matcher implementing bipartite matching strategy. Parameters ---------- threshold : float Threshold used to ignore invalid paddings is_ascend : bool Whether sort matching order in ascending order. Default is False. eps : float Epsilon for floating number comparison """ def __init__(self, threshold=1e-12, is_ascend=False, eps=1e-12): super(BipartiteMatcher, self).__init__() self._threshold = threshold self._is_ascend = is_ascend self._eps = eps def hybrid_forward(self, F, x): """BipartiteMatching Parameters: ---------- x : NDArray or Symbol IOU overlaps with shape (N, M), batching is supported. """ # x是每个anchor与gt_box的iou，N个样本，M个gt_box，则x shape=(1*N*M) # match[0]表示哪个anchor与label最匹配 # 例如N为4时，M为2，如果返回[[1, -1, 0, -1]]，表示第1个box与gt_box[1]最匹配，第3个anchor与gt_box[0]最匹配，第二个和第三个box没有匹配的gt_box match = F.contrib.bipartite_matching(x, threshold=self._threshold, is_ascend=self._is_ascend) # make sure if iou(a, y) == iou(b, y), then b should also be a good match # otherwise positive/negative samples are confusing # potential argmax and max # pargmax得到每个archor中哪个gt_box的分数最高，保存的是gt_box的索引 pargmax = x.argmax(axis=-1, keepdims=True) # (B, num_anchor, 1) # maxs得到各个gt_box最高的得分，shape (B, 1, num_gt) maxs = x.max(axis=-2, keepdims=True) # (B, 1, num_gt) # 按照pargmax中保存的索引得到每个anchor中最高的分数 pmax = F.pick(x, pargmax, axis=-1, keepdims=True) # (B, num_anchor, 1) # 特定的broadcast操作，例如pmax shape=(1*28500*1)，变为(1*28500*2)，同时第三个维度中的数组值跟maxs数组相应的位置进行比较，小于maxs则为0，大于等于则为1 # 然后pick操作相当于得到每个gt_box最高分对应的box，相应box的位置赋值为1，其他的赋值为0 # 例如得到[[[0], [0], [0], [1], [0], [1]]] mask = F.broadcast_greater_equal(pmax + self._eps, maxs) # (B, num_anchor, num_gt) mask = F.pick(mask, pargmax, axis=-1, keepdims=True) # (B, num_anchor, 1) # 得到mask的中值为1的box对应哪个gt_box，对应mask中值为0的box位置都赋为-1 # 例如得到[[[-1], [-1], [-1], [0], [-1], [1]]] new_match = F.where(mask &gt; 0, pargmax, F.ones_like(pargmax) * -1) # 按照match[0]，综合match[0]和new_match，将match[0]中为负的值，用new_match中相应位置的值代替 result = F.where(match[0] &lt; 0, new_match.squeeze(axis=-1), match[0]) return result 1234567891011121314151617181920212223242526272829303132333435363738class RCNNTargetGenerator(gluon.Block): """RCNN target encoder to generate matching target and regression target values. Parameters ---------- num_class : int Number of total number of positive classes. means : iterable of float, default is (0., 0., 0., 0.) Mean values to be subtracted from regression targets. stds : iterable of float, default is (.1, .1, .2, .2) Standard deviations to be divided from regression targets. """ def __init__(self, num_class, means=(0., 0., 0., 0.), stds=(.1, .1, .2, .2)): super(RCNNTargetGenerator, self).__init__() self._cls_encoder = MultiClassEncoder() self._box_encoder = NormalizedPerClassBoxCenterEncoder( num_class=num_class, means=means, stds=stds) #pylint: disable=arguments-differ def forward(self, roi, samples, matches, gt_label, gt_box): """ Only support batch_size=1 now. """ with autograd.pause(): # 得到每个box属于哪个类，负样本为0，例如shape=1*128 cls_target = self._cls_encoder(samples, matches, gt_label) # box_target得到每个box的偏移和缩放，shape=20*1*128*4 # box_mask得到每各box的是否属于这个类，shape=20*1*128*4 box_target, box_mask = self._box_encoder( samples, matches, roi, gt_label, gt_box) # modify shapes to match predictions # 去掉一个维度，shape=(128,) cls_target = cls_target[0] # shape变为128*20*4 box_target = box_target.transpose((1, 2, 0, 3))[0] box_mask = box_mask.transpose((1, 2, 0, 3))[0] return cls_target, box_target, box_mask 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class NormalizedPerClassBoxCenterEncoder(gluon.Block): """Encode bounding boxes training target with normalized center offsets. Input bounding boxes are using corner type: `x_&#123;min&#125;, y_&#123;min&#125;, x_&#123;max&#125;, y_&#123;max&#125;`. Parameters ---------- stds : array-like of size 4 Std value to be divided from encoded values, default is (0.1, 0.1, 0.2, 0.2). means : array-like of size 4 Mean value to be subtracted from encoded values, default is (0., 0., 0., 0.). """ def __init__(self, num_class, stds=(0.1, 0.1, 0.2, 0.2), means=(0., 0., 0., 0.)): super(NormalizedPerClassBoxCenterEncoder, self).__init__() assert len(stds) == 4, "Box Encoder requires 4 std values." assert num_class &gt; 0, "Number of classes must be positive" self._num_class = num_class self._stds = stds self._means = means with self.name_scope(): self.corner_to_center = BBoxCornerToCenter(split=True) def forward(self, samples, matches, anchors, labels, refs): """Encode BBox One entry per category""" F = nd # 根据matches得到每个box对应的gt_box坐标，例如shape=(1*128*4) ref_boxes = F.repeat(refs.reshape((0, 1, -1, 4)), axis=1, repeats=matches.shape[1]) ref_boxes = F.split(ref_boxes, axis=-1, num_outputs=4, squeeze_axis=True) ref_boxes = F.concat(*[F.pick(ref_boxes[i], matches, axis=2).reshape((0, -1, 1)) \ for i in range(4)], dim=2) # 得到每个box对应的类label ref_labels = F.repeat(labels.reshape((0, 1, -1)), axis=1, repeats=matches.shape[1]) ref_labels = F.pick(ref_labels, matches, axis=2).reshape((0, -1, 1)) # 对角坐标转为中心坐标 g = self.corner_to_center(ref_boxes) a = self.corner_to_center(anchors) # 计算偏移与缩放 t0 = ((g[0] - a[0]) / a[2] - self._means[0]) / self._stds[0] t1 = ((g[1] - a[1]) / a[3] - self._means[1]) / self._stds[1] t2 = (F.log(g[2] / a[2]) - self._means[2]) / self._stds[2] t3 = (F.log(g[3] / a[3]) - self._means[3]) / self._stds[3] codecs = F.concat(t0, t1, t2, t3, dim=2) # 假设samples shape=(1*128*1)，赋值四次后得到shape=(1*128*4)，然后找出大于0.5的值，赋值为true，其他false temp = F.tile(samples.reshape((0, -1, 1)), reps=(1, 1, 4)) &gt; 0.5 # 将正样本的位置填入t0 t1 t2 t3，其他的位置都是0 targets = F.where(temp, codecs, F.zeros_like(codecs)) # 将正样本的位置填入1，其他的都是0 masks = F.where(temp, F.ones_like(temp), F.zeros_like(temp)) out_targets = [] out_masks = [] # out_targets是一个list，有num_class个元素，每个元素是之前targets的一份赋值 # out_masks是一个list，有num_class个元素，第i个元素是一个1*128*4的ndarray，4个维度相同，非0表示这个box是第i类的box for cid in range(self._num_class): same_cid = ref_labels == cid # keep orig targets out_targets.append(targets) # but mask out the one not belong to this class out_masks.append(masks * same_cid.repeat(axis=-1, repeats=4)) # 假如有20个类，128个box，则shape=20*1*128*4，其中每一维都相同，都保存了所有正样本的平移和缩放，负样本为0 all_targets = F.stack(*out_targets, axis=0) # 假如有20个类，128个box，则shape=20*1*128*4，其中mask为1的位置表示相应的box属于第i类 all_masks = F.stack(*out_masks, axis=0) return all_targets, all_masks 123456789101112131415161718192021222324252627282930class MultiClassEncoder(gluon.HybridBlock): """Encode classification training target given matching results. This encoder will assign training target of matched bounding boxes to ground-truth label + 1 and negative samples with label 0. Ignored samples will be assigned with `ignore_label`, whose default is -1. Parameters ---------- ignore_label : float Assigned to un-matched samples, they are neither positive or negative during training, and should be excluded in loss function. Default is -1. """ def __init__(self, ignore_label=-1): super(MultiClassEncoder, self).__init__() self._ignore_label = ignore_label def hybrid_forward(self, F, samples, matches, refs): # 假设有两个gt_box，refs shape=1*2*1，有128个预测box，shape=1*128 # 这一步赋值refs，使得shape=1*128*2 refs = F.repeat(refs.reshape((0, 1, -1)), axis=1, repeats=matches.shape[1]) # 按照matches得出对应的box属于哪一类，加1是因为背景为0，shape=1*128 target_ids = F.pick(refs, matches, axis=2) + 1 # targets得到每个正例box属于哪一类，负样本为-1 targets = F.where(samples &gt; 0.5, target_ids, nd.ones_like(target_ids) * self._ignore_label) # 负样本变为0，shape=1*128 targets = F.where(samples &lt; -0.5, nd.zeros_like(targets), targets) # targets得出每个box属于哪一类 return targets 采样正样本和负样本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113class QuotaSampler(gluon.Block): """Sampler that handles limited quota for positive and negative samples. Parameters ---------- num_sample : int, default is 128 Number of samples for RCNN targets. pos_iou_thresh : float, default is 0.5 Proposal whose IOU larger than ``pos_iou_thresh`` is regarded as positive samples. neg_iou_thresh_high : float, default is 0.5 Proposal whose IOU smaller than ``neg_iou_thresh_high`` and larger than ``neg_iou_thresh_low`` is regarded as negative samples. Proposals with IOU in between ``pos_iou_thresh`` and ``neg_iou_thresh`` are ignored. neg_iou_thresh_low : float, default is 0.0 See ``neg_iou_thresh_high``. pos_ratio : float, default is 0.25 ``pos_ratio`` defines how many positive samples (``pos_ratio * num_sample``) is to be sampled. neg_ratio : float or None ``neg_ratio`` defines how many negative samples (``pos_ratio * num_sample``) is to be sampled. If ``None`` is provided, it equals to ``1 - pos_ratio``. fill_negative : bool If ``True``, negative samples will fill the gap caused by insufficient positive samples. For example, if ``num_sample`` is 100, ``pos_ratio`` and ``neg_ratio`` are both ``0.5``. Available positive sample and negative samples are 10 and 10000, which are typical values. Now, the output positive samples is 10(intact), since it's smaller than ``50(100 * 0.5)``, the negative samples will fill the rest ``40`` slots. If ``fill_negative == False``, the ``40`` slots is filled with ``-1(ignore)``. """ def __init__(self, num_sample, pos_thresh, neg_thresh_high, neg_thresh_low=-np.inf, pos_ratio=0.5, neg_ratio=None, fill_negative=True): super(QuotaSampler, self).__init__() self._fill_negative = fill_negative self._num_sample = num_sample if neg_ratio is None: self._neg_ratio = 1. - pos_ratio self._pos_ratio = pos_ratio assert (self._neg_ratio + self._pos_ratio) &lt;= 1.0, ( "Positive and negative ratio &#123;&#125; exceed 1".format(self._neg_ratio + self._pos_ratio)) self._pos_thresh = min(1., max(0., pos_thresh)) self._neg_thresh_high = min(1., max(0., neg_thresh_high)) self._neg_thresh_low = neg_thresh_low def forward(self, matches, ious): """Quota Sampler Parameters: ---------- matches : NDArray or Symbol Matching results, postive number for postive matching, -1 for not matched. ious : NDArray or Symbol IOU overlaps with shape (N, M), batching is supported. Returns: -------- NDArray or Symbol Sampling results with same shape as ``matches``. 1 for positive, -1 for negative, 0 for ignore. """ # 假设matches shape=(1*28500)， ious shape=(1) F = mx.nd # 最大正样本数量 max_pos = int(round(self._pos_ratio * self._num_sample)) # 最大负样本数量 max_neg = int(self._neg_ratio * self._num_sample) results = [] for i in range(matches.shape[0]): # init with 0s, which are ignored # 初始化为0 result shape=(28500, ) result = F.zeros_like(matches[0]) # negative samples with label -1 # 得到每个box最大的分数 # 例如 x=[[[1, 2, 3], [2, 8, 1], [1, 2, 5]]]，则返回的是[3, 8, 5] ious_max = ious.max(axis=-1)[i] # 得到ious_max中介于self._neg_thresh_low和self._neg_thresh_high之间的位置，默认0和0.3 neg_mask = ious_max &lt; self._neg_thresh_high neg_mask = neg_mask * (ious_max &gt;= self._neg_thresh_low) # 按照neg_mask，将result中对应位置值为-1，这些位置表示负样本 result = F.where(neg_mask, F.ones_like(result) * -1, result) # positive samples # 按照matches和ious_max，将正样本的位置值为1 result = F.where(matches[i] &gt;= 0, F.ones_like(result), result) result = F.where(ious_max &gt;= self._pos_thresh, F.ones_like(result), result) # re-balance if number of postive or negative exceed limits result = result.asnumpy() # 计算正样本的数量 num_pos = int((result &gt; 0).sum()) # 如果正样本的数量大于最大正样本数，随机将num_pos-max_pos个box的值置为0，即ignore了 if num_pos &gt; max_pos: disable_indices = np.random.choice( np.where(result &gt; 0)[0], size=(num_pos - max_pos), replace=False) result[disable_indices] = 0 # use 0 to ignore # 获得负样本数量 num_neg = int((result &lt; 0).sum()) if self._fill_negative: # if pos_sample is less than quota, we can have negative samples filling the gap # 如果正样本小于额给定值，那就用负样本来填补 # 例如，256个样本，128正128负，如果只有50个正的，那负样本就为201个 max_neg = max(self._num_sample - min(num_pos, max_pos), max_neg) if num_neg &gt; max_neg: # 随机选取num_neg-max_neg个负样本忽略，就剩下了max_neg个负样本 disable_indices = np.random.choice( np.where(result &lt; 0)[0], size=(num_neg - max_neg), replace=False) result[disable_indices] = 0 results.append(mx.nd.array(result)) # results中包含了 return mx.nd.stack(*results, axis=0) #123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class NormalizedBoxCenterEncoder(gluon.Block): """Encode bounding boxes training target with normalized center offsets. Input bounding boxes are using corner type: `x_&#123;min&#125;, y_&#123;min&#125;, x_&#123;max&#125;, y_&#123;max&#125;`. Parameters ---------- stds : array-like of size 4 Std value to be divided from encoded values, default is (0.1, 0.1, 0.2, 0.2). means : array-like of size 4 Mean value to be subtracted from encoded values, default is (0., 0., 0., 0.). """ def __init__(self, stds=(0.1, 0.1, 0.2, 0.2), means=(0., 0., 0., 0.)): super(NormalizedBoxCenterEncoder, self).__init__() assert len(stds) == 4, "Box Encoder requires 4 std values." self._stds = stds self._means = means with self.name_scope(): self.corner_to_center = BBoxCornerToCenter(split=True) def forward(self, samples, matches, anchors, refs): """Forward""" F = nd # TODO(zhreshold): batch_pick, take multiple elements? # 假如refs shape=(1*5*4)，是gt_box # 将gt_box重复matches的数量次，假如matches shape=(1*32490)，则ref_boxes shape=(1*32490*5*4) ref_boxes = nd.repeat(refs.reshape((0, 1, -1, 4)), axis=1, repeats=matches.shape[1]) # ref_boxes分为4分，每份shape=(1*32490*5) ref_boxes = nd.split(ref_boxes, axis=-1, num_outputs=4, squeeze_axis=True) # 根据matches得到每个box应对应的gt_box坐标，shape=(1*32490*4) ref_boxes = nd.concat(*[F.pick(ref_boxes[i], matches, axis=2).reshape((0, -1, 1)) \ for i in range(4)], dim=2) # 将对角坐标转为中心坐标 g = self.corner_to_center(ref_boxes) a = self.corner_to_center(anchors) # 得出每个box对应的平移和缩放大小 t0 = ((g[0] - a[0]) / a[2] - self._means[0]) / self._stds[0] t1 = ((g[1] - a[1]) / a[3] - self._means[1]) / self._stds[1] t2 = (F.log(g[2] / a[2]) - self._means[2]) / self._stds[2] t3 = (F.log(g[3] / a[3]) - self._means[3]) / self._stds[3] codecs = F.concat(t0, t1, t2, t3, dim=2) # 假设samples shape=(1*32490*1)，赋值四次后得到shape=(1*32490*4)，然后找出大于0.5的值，赋值为true，其他false temp = F.tile(samples.reshape((0, -1, 1)), reps=(1, 1, 4)) &gt; 0.5 # 将正样本的位置填入t0 t1 t2 t3，其他的位置都是0 targets = F.where(temp, codecs, F.zeros_like(codecs)) # 将正样本的位置填入1，其他的都是0 masks = F.where(temp, F.ones_like(temp), F.zeros_like(temp)) # 例如，最后的targets=[[[0, 0, 0, 0], [5, 8, 90, 100]]] # 最后的mask=[[[0, 0, 0, 0], [1, 1, 1, 1]]] return targets, masks 训练123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135def train(net, train_data, val_data, eval_metric, args): """Training pipeline""" net.collect_params().reset_ctx(ctx) trainer = gluon.Trainer( net.collect_train_params(), # fix batchnorm, fix first stage, etc... 'sgd', &#123;'learning_rate': args.lr, 'wd': args.wd, 'momentum': args.momentum, 'clip_gradient': 5&#125;) # lr decay policy lr_decay = float(args.lr_decay) lr_steps = sorted([float(ls) for ls in args.lr_decay_epoch.split(',') if ls.strip()]) lr_warmup = int(args.lr_warmup) # TODO(zhreshold) losses? rpn_cls_loss = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False) rpn_box_loss = mx.gluon.loss.HuberLoss(rho=1/9.) # == smoothl1 rcnn_cls_loss = mx.gluon.loss.SoftmaxCrossEntropyLoss() rcnn_box_loss = mx.gluon.loss.HuberLoss() # == smoothl1 metrics = [mx.metric.Loss('RPN_Conf'), mx.metric.Loss('RPN_SmoothL1'), mx.metric.Loss('RCNN_CrossEntropy'), mx.metric.Loss('RCNN_SmoothL1'),] rpn_acc_metric = RPNAccMetric() rpn_bbox_metric = RPNL1LossMetric() rcnn_acc_metric = RCNNAccMetric() rcnn_bbox_metric = RCNNL1LossMetric() metrics2 = [rpn_acc_metric, rpn_bbox_metric, rcnn_acc_metric, rcnn_bbox_metric] # set up logger logging.basicConfig() logger = logging.getLogger() logger.setLevel(logging.INFO) log_file_path = args.save_prefix + '_train.log' log_dir = os.path.dirname(log_file_path) if log_dir and not os.path.exists(log_dir): os.makedirs(log_dir) fh = logging.FileHandler(log_file_path) logger.addHandler(fh) logger.info(args) if args.verbose: logger.info('Trainable parameters:') logger.info(net.collect_train_params().keys()) logger.info('Start training from [Epoch &#123;&#125;]'.format(args.start_epoch)) best_map = [0] for epoch in range(args.start_epoch, args.epochs): while lr_steps and epoch &gt;= lr_steps[0]: new_lr = trainer.learning_rate * lr_decay lr_steps.pop(0) trainer.set_learning_rate(new_lr) logger.info("[Epoch &#123;&#125;] Set learning rate to &#123;&#125;".format(epoch, new_lr)) for metric in metrics: metric.reset() tic = time.time() btic = time.time() net.hybridize(static_alloc=True) base_lr = trainer.learning_rate for i, batch in enumerate(train_data): if epoch == 0 and i &lt;= lr_warmup: new_lr = base_lr * get_lr_at_iter((i // 500) / (lr_warmup / 500.)) if new_lr != trainer.learning_rate: logger.info('[Epoch 0 Iteration &#123;&#125;] Set learning rate to &#123;&#125;'.format(i, new_lr)) trainer.set_learning_rate(new_lr) batch = split_and_load(batch, ctx_list=ctx) batch_size = len(batch[0]) losses = [] metric_losses = [[] for _ in metrics] add_losses = [[] for _ in metrics2] with autograd.record(): for data, label, rpn_cls_targets, rpn_box_targets, rpn_box_masks in zip(*batch): gt_label = label[:, :, 4:5] gt_box = label[:, :, :4] cls_pred, box_pred, roi, samples, matches, rpn_score, rpn_box, anchors = net(data, gt_box) # losses of rpn rpn_score = rpn_score.squeeze(axis=-1) # 得出target中正负样本的数量 num_rpn_pos = (rpn_cls_targets &gt;= 0).sum() # rpn score loss rpn_loss1 = rpn_cls_loss(rpn_score, rpn_cls_targets, rpn_cls_targets &gt;= 0) * rpn_cls_targets.size / num_rpn_pos # rpn bounding loss rpn_loss2 = rpn_box_loss(rpn_box, rpn_box_targets, rpn_box_masks) * rpn_box.size / num_rpn_pos # rpn overall loss, use sum rather than average rpn_loss = rpn_loss1 + rpn_loss2 # generate targets for rcnn # 产生RCNN部分的target # cls_targets shape=(128,) box_targets shape=(128*20*4) box_masks=(128*20*4) cls_targets, box_targets, box_masks = net.target_generator(roi, samples, matches, gt_label, gt_box) # losses of rcnn # 得到正负样本总数 num_rcnn_pos = (cls_targets &gt;= 0).sum() # cls的交叉熵损失 rcnn_loss1 = rcnn_cls_loss(cls_pred, cls_targets, cls_targets &gt;= 0) * cls_targets.size / cls_targets.shape[0] / num_rcnn_pos # bounding的损失 rcnn_loss2 = rcnn_box_loss(box_pred, box_targets, box_masks) * box_pred.size / box_pred.shape[0] / num_rcnn_pos rcnn_loss = rcnn_loss1 + rcnn_loss2 # overall losses losses.append(rpn_loss.sum() + rcnn_loss.sum()) metric_losses[0].append(rpn_loss1.sum()) metric_losses[1].append(rpn_loss2.sum()) metric_losses[2].append(rcnn_loss1.sum()) metric_losses[3].append(rcnn_loss2.sum()) add_losses[0].append([[rpn_cls_targets, rpn_cls_targets&gt;=0], [rpn_score]]) add_losses[1].append([[rpn_box_targets, rpn_box_masks], [rpn_box]]) add_losses[2].append([[cls_targets], [cls_pred]]) add_losses[3].append([[box_targets, box_masks], [box_pred]]) autograd.backward(losses) for metric, record in zip(metrics, metric_losses): metric.update(0, record) for metric, records in zip(metrics2, add_losses): for pred in records: metric.update(pred[0], pred[1]) trainer.step(batch_size) # update metrics if args.log_interval and not (i + 1) % args.log_interval: # msg = ','.join(['&#123;&#125;=&#123;:.3f&#125;'.format(*metric.get()) for metric in metrics]) msg = ','.join(['&#123;&#125;=&#123;:.3f&#125;'.format(*metric.get()) for metric in metrics + metrics2]) logger.info('[Epoch &#123;&#125;][Batch &#123;&#125;], Speed: &#123;:.3f&#125; samples/sec, &#123;&#125;'.format( epoch, i, batch_size/(time.time()-btic), msg)) btic = time.time() msg = ','.join(['&#123;&#125;=&#123;:.3f&#125;'.format(*metric.get()) for metric in metrics]) logger.info('[Epoch &#123;&#125;] Training cost: &#123;:.3f&#125;, &#123;&#125;'.format( epoch, (time.time()-tic), msg)) if not (epoch + 1) % args.val_interval: # consider reduce the frequency of validation to save time map_name, mean_ap = validate(net, val_data, ctx, eval_metric) val_msg = '\n'.join(['&#123;&#125;=&#123;&#125;'.format(k, v) for k, v in zip(map_name, mean_ap)]) logger.info('[Epoch &#123;&#125;] Validation: \n&#123;&#125;'.format(epoch, val_msg)) current_map = float(mean_ap[-1]) else: current_map = 0. save_params(net, logger, best_map, current_map, epoch, args.save_interval, args.save_prefix) 损失函数 prob = \frac{1}{1 + \exp(-{pred})} \\ L = - \sum_i {label}_i * \log({prob}_i) + (1 - {label}_i) * \log(1 - {prob}_i)img：reshape、normalzie之后的图像bbox：reshape之后的gt_boxcls_target[0]：哪些是正样本，哪些是负样本，哪些需要忽略，跟之前一样，只是降了一个维度box_target[0]：正样本box跟gt_box之间要学习的平移和缩放距离，跟之前一样，只是降了一个维度box_mask[0]：box_target对应的mask，跟之前一样，只是降了一个维度return img, bbox.astype(img.dtype), cls_target[0], box_target[0], box_mask[0] 12345678910111213141516171819202122232425262728# no need to convert bounding boxes in training, just return# 训练过程中，返回如下# cls_pred：全连接网络最后的预测类型# box_pred: 全连接网络最后的box偏移# rpn_box：加上lebal的筛选出送入roi poolinglayer的box偏移# samples：rpn_box是正样本1，还是负样本-1，还是忽略0# matches：rpn_box匹配哪个样本，-1代表忽略的，其他值是真实类别# raw_rpn_score：原始产生的box score# raw_rpn_box：原始产生的box坐标偏移# anchors：原始产生的anchorsreturn (cls_pred, box_pred, rpn_box, samples, matches, raw_rpn_score, raw_rpn_box, anchors)cls_pred, box_pred, roi, samples, matches, rpn_score, rpn_box, anchors = net(data, gt_box)# cls_target[0]：哪些是正样本，哪些是负样本，哪些需要忽略，跟之前一样，只是降了一个维度 -1：忽略，0：负样本，1：正样本# box_target[0]：正样本box跟gt_box之间要学习的平移和缩放距离，跟之前一样，只是降了一个维度# box_mask[0]：box_target对应的mask，跟之前一样，只是降了一个维度return img, bbox.astype(img.dtype), cls_target[0], box_target[0], box_mask[0]for data, label, rpn_cls_targets, rpn_box_targets, rpn_box_masks in zip(*batch):# rpn网络预测的正负样本，target正负样本，lossrpn_loss1 = rpn_cls_loss(rpn_score, rpn_cls_targets, rpn_cls_targets &gt;= 0) * rpn_cls_targets.size / num_rpn_pos# rpn预测到的box偏移与缩放，target box的偏移与缩放，lossrpn_loss2 = rpn_box_loss(rpn_box, rpn_box_targets, rpn_box_masks) * rpn_box.size / num_rpn_pos# roicls_targets, box_targets, box_masks = net.target_generator(roi, samples, matches, gt_label, gt_box) 损失函数损失计算 rpn网络产生的box是前景还是背景。 12345# rpn_score：原始的box得分，没有经过sigmoid，例如shape=1*18500# rpn_cls_targets：这些box哪些应该是前景1，哪些是背景0，哪些应该忽略-1# rpn_cls_targets&gt;=0：表示只给前景和背景计算loss，-1的不计算lossrpn_cls_loss = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)rpn_loss1 = rpn_cls_loss(rpn_score, rpn_cls_targets, rpn_cls_targets &gt;= 0) * rpn_cls_targets.size / num_rpn_pos rpn网络产生的box的平移缩放损失。 1234# rpn_box：原始的平移和缩放# rpn_box_targets：期望的前景的平移与缩放# rpn_box_masks：rpn_box_targets的mask，就是说只计算前景的loss，因为背景不能计算rpn_loss2 = rpn_box_loss(rpn_box, rpn_box_targets, rpn_box_masks) * rpn_box.size / num_rpn_pos 评价标准这里构造评价器12345678910metrics = [mx.metric.Loss('RPN_Conf'), mx.metric.Loss('RPN_SmoothL1'), mx.metric.Loss('RCNN_CrossEntropy'), mx.metric.Loss('RCNN_SmoothL1'),]rpn_acc_metric = RPNAccMetric()rpn_bbox_metric = RPNL1LossMetric()rcnn_acc_metric = RCNNAccMetric()rcnn_bbox_metric = RCNNL1LossMetric()metrics2 = [rpn_acc_metric, rpn_bbox_metric, rcnn_acc_metric, rcnn_bbox_metric] 每一个batch，都会有这么两个局部变量，即list12metric_losses = [[] for _ in metrics]add_losses = [[] for _ in metrics2] 得到一个batch的loss之后，将各个loss加入到对应的list，然后在update各自的metric1234567891011121314metric_losses[0].append(rpn_loss1.sum())metric_losses[1].append(rpn_loss2.sum())metric_losses[2].append(rcnn_loss1.sum())metric_losses[3].append(rcnn_loss2.sum())add_losses[0].append([[rpn_cls_targets, rpn_cls_targets&gt;=0], [rpn_score]])add_losses[1].append([[rpn_box_targets, rpn_box_masks], [rpn_box]])add_losses[2].append([[cls_targets], [cls_pred]])add_losses[3].append([[box_targets, box_masks], [box_pred]])for metric, record in zip(metrics, metric_losses): metric.update(0, record)for metric, records in zip(metrics2, add_losses): for pred in records: metric.update(pred[0], pred[1])]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>RCNN</tag>
        <tag>Faster RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux知识积累]]></title>
    <url>%2F2018%2F06%2F16%2FLinux_Knowledge%2F</url>
    <content type="text"><![CDATA[dpkg 安装软件安装一个Debian软件包1dpkg -i &lt;package.deb&gt; mv 移动把多个文件移动到一个目录中，-t后面紧跟将要移动的目录。如123mv a b c -t dmv -t d a b cmv Document/* Download/ *表示文件夹下所有文件 rm 删除 123rm a.txtrm -r dirrm -rf dir -r 就是向下递归，不管有多少级目录，一并删除-f 就是直接强行删除，不作任何提示的意思 mkdir 新建文件夹 tar 解压-c:建立压缩档案-x:解压-r:向压缩归档文件末尾追加文件-z:有gzip属性的-j:有bz2属性的-v:显示所有过程-f:使用档案名字，这个参数是最后一个参数，后面只能接档案名 touch 新建文件 1touch test.java ~/.的意思代表/home/用户明目录，假设用户名是x，那么~/就是/home/x/，.是代表此目录本身，但是一般可以不写, 所以cd ~/. 和cd ~ 和cd ~/效果是一样的。但是.后面有东西又是另外一个问题，点在文件名头部，代表一个隐藏文件。~/.local是主目录下一个.local的文件夹的路径，并且从.可以看出，这是一个饮藏文件，如果不用ls -a的话，一般ls是无法看到的 文件权限 Linux 权限基于 UGO 模型进行控制：U 代表 User，是文件或文件夹所属用户的权限；G 代表 Group，是文件或文件夹所属组的权限；O 代表 Other，是其他用户对文件或文件夹的权限。每一个文件的权限基于 UGO 模型进行设置。权限三个一组（rwx），对应 UGO 分别设置（总共有 3 个组 9 个权限）。每一个文件拥有一个所属用户和所属组，对应 U、G模型；不属于该文件所属用户和所属组的使用 O（Other）模型对应的权限。使用ls -l查看权限。 chmod 权限修改u、g、o分别代表用户、组、其他;a代表ugo;+、-代表加入或删除对应权限;r、w、x代表三种权限;-R递归地修改。r = 4 (2 ^ 2); w = 2 (2 ^ 1);x = 1 (2 ^ 0);rw = 4 + 2 = 6;rwx = 4 + 2 + 1 = 7;r-x = 4 + 1 = 5。 1234chmod u +rw test.md "给文件的所属用户添加rw权限chmod g -x test.md "给文件的所属组移除x权限chmod go +r test.md "给文件的所属组和其他用户添加r权限chmod a -x test.md "给文件的所属UGO三个模型均移除x权限 df 磁盘空间情况 mount umount 挂载与取消挂载 ln 创建软连接 1ln -s a b "a是源文件，b是链接文件名]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ResNet解析]]></title>
    <url>%2F2018%2F06%2F15%2FResNet%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[ResNet在2015年被提出，在ImageNet比赛classification任务上获得第一名，因为它“简单与实用”并存，之后很多方法都建立在ResNet50或者ResNet101的基础上完成的，检测，分割，识别等领域都纷纷使用ResNet，Alpha zero也使用了ResNet。 残差块ResNet的基础块叫做残差块。一般有两种形式BasicBlock和Bottleneck。 BasicBlock结构中，主要使用了两个$3\times3$的卷积核。 残差residual分支为：OUT = CONV3*3 + BN + RELU + CONV3*3 + BN，然后与恒等映射identity相加再做relu OUT + X + RELU。 Bottleneck结构中。分别使用$1\times1，3\times3，1\times1$的卷积核，残差residual分支为：OUT = CONV1*1 + BN + RELU + CONV3*3 + BN + RELU + CONV1*1 + BN，然后与恒等映射identity相加再做relu OUT + X + RELU。使用Bottleneck结构可以减少网络参数数量。 如下图所示(直接粘贴论文中的原图，原图没有画BN结构）： 这两种结构分别针对ResNet34（左图）和ResNet50/101/152（右图），目的一目了然，就是为了降低参数的数目，第一个1x1的卷积把256维channel降到64维，然后在最后通过1x1卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632，而不使用bottleneck的话就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。对于BasicBlock，可以用于34层或者更少的网络中，对于Bottleneck Design的ResNet通常用于更深的如101这样的网络中，目的是减少计算和参数。 在之后的《Identity Mappings in Deep Residual Networks》中，又提出了一种更优的残差块结构，并且作者分析了什么样的残差结构才work。主要改变是将BN和RELU放在了CONV之前，对比如下： 实验结果表明，新的残差结构要优于之前的残差结构。 什么样的残差块work？下面(a)到(e)的结构哪个是我们常用的ResNet结构？ 答案是(a)和(e）。 对于每个图右侧部分我们称作“residual”分支，左侧部分我们称作“identity”分支。 ResNet原文中使用的结构是(a），（a)的特点有两个： BN和ReLU在weight的后面； 最后的ReLU在addition的后面。 对于特点(1)，属于常规范畴，我们平时也都这个顺序：Conv-&gt;BN-&gt;ReLU；对于特点(2），为什么ReLU放在addition后面呢？ 图(c)的结构work么？答案是不work。如果ReLU作为“residual”分支的结尾，不难发现“residual”分支的结果永远非负，这样前向的时候输入会单调递增，从而会影响特征的表达能力，所以我们希望“residual”分支的结果应该在 $(-\infty, +\infty)$。这点也是我们以后设计网络时所要注意的。 图(b)的结构work么？答案是不work。这里BN改变了“identity”分支的分布，影响了信息的传递，在训练的时候会阻碍loss的下降。 为什么“identity”分支发生变化，会影响信息传递，从而影响训练？回顾ResNet的公式： y_l=h(x_l)+F(x_l, W_l) and x_{l+1} = f(y_l)简化以上公式，令所有“identity”分支都是 $h(x_l) = x_l$ 以及 $x_{l+1} = y_l$，那么得到: x_{l+1} = x_l + F(x_l, W_l)我们递归的计算: x_{l+2} = x_{l+1} + F(x_{l+1}, W_{l+1}) = x_l + F(x_l, W_l) + F(x_{l+1}, W_{l+1}) x_L = x_l + \sum_{i=l}^{L-1}F(x_i, W_i) \tag{1}公式(1)，表达了任何第 $L$ 层(深层)与第 $l$ 层(浅层)之间关系；假设损失函数为loss，那么反向传播公式为： \frac{\partial loss}{\partial x_l} = \frac{\partial loss}{\partial x_L} \frac{\partial x_L}{\partial x_l} = \frac{\partial loss}{\partial x_L} (1 + \frac{\partial}{\partial x_l}\sum_{i=l}^{L-1}F(x_i, W_i))这个反向传播公式有几个特点： 关于 $x_l$ 的梯度信息与两部分值有关：$x_L$ 的梯度值，也就是说两层之间梯度信息无障碍传递了，以及 $\frac{\partial loss}{\partial x_L} \frac{\partial}{\partial x_l}\sum_{i=l}^{L-1}F(x_i, W_i))$; $\frac{\partial loss}{\partial x_l}$的值不会轻易抵消，因为在一个mini_batch中，$\frac{\partial}{\partial x_l}\sum_{i=l}^{L-1}F(x_i, W_i))$不会一直为-1； 有效的防止了当权重很小时，梯度消失的问题。 以上优秀的特点只有在假设 $h(x_l) = x_l$ 以及 $x_{l+1} = y_l$ 成立时才有效，所以ResNet要尽量保证两点： 不轻易改变”identity“分支的值，也就是输入与输出一致； addition之后不再接改变信息分布的层； 到此也就彻底回答了图(b)的结构为何会阻碍反向传播时的信息。 为什么选图(e)而不是图(d)在分析图(d)和图(e)之前，我们引出一个概念：Post-activation和Pre-activation，其中Post和Pre的概念是相对于weight(conv)层来说的，那么我们不难发现，图(a)、(b)、(c)都是Post-activation，图(d), (c)都是Pre-activation，而论文作者用实验结果得出了图(e)的结构比ResNet原结构要好。原因在于两点： 反向传播基本符合假设，信息传递无阻碍； BN层作为pre-activation，起到了正则化的作用。 残差块的代码实现残差块mxnet代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183# Blocksclass BasicBlockV1(HybridBlock): r"""BasicBlock V1 from `"Deep Residual Learning for Image Recognition" &lt;http://arxiv.org/abs/1512.03385&gt;`_ paper. This is used for ResNet V1 for 18, 34 layers. Parameters ---------- channels : int Number of output channels. stride : int Stride size. downsample : bool, default False Whether to downsample the input. in_channels : int, default 0 Number of input channels. Default is 0, to infer from the graph. """ def __init__(self, channels, stride, downsample=False, in_channels=0, **kwargs): super(BasicBlockV1, self).__init__(**kwargs) self.body = nn.HybridSequential(prefix='') self.body.add(_conv3x3(channels, stride, in_channels)) self.body.add(nn.BatchNorm()) self.body.add(nn.Activation('relu')) self.body.add(_conv3x3(channels, 1, channels)) self.body.add(nn.BatchNorm()) if downsample: self.downsample = nn.HybridSequential(prefix='') self.downsample.add(nn.Conv2D(channels, kernel_size=1, strides=stride, use_bias=False, in_channels=in_channels)) self.downsample.add(nn.BatchNorm()) else: self.downsample = None def hybrid_forward(self, F, x): residual = x x = self.body(x) if self.downsample: residual = self.downsample(residual) x = F.Activation(residual+x, act_type='relu') return xclass BottleneckV1(HybridBlock): r"""Bottleneck V1 from `"Deep Residual Learning for Image Recognition" &lt;http://arxiv.org/abs/1512.03385&gt;`_ paper. This is used for ResNet V1 for 50, 101, 152 layers. Parameters ---------- channels : int Number of output channels. stride : int Stride size. downsample : bool, default False Whether to downsample the input. in_channels : int, default 0 Number of input channels. Default is 0, to infer from the graph. """ def __init__(self, channels, stride, downsample=False, in_channels=0, **kwargs): super(BottleneckV1, self).__init__(**kwargs) self.body = nn.HybridSequential(prefix='') self.body.add(nn.Conv2D(channels//4, kernel_size=1, strides=stride)) self.body.add(nn.BatchNorm()) self.body.add(nn.Activation('relu')) self.body.add(_conv3x3(channels//4, 1, channels//4)) self.body.add(nn.BatchNorm()) self.body.add(nn.Activation('relu')) self.body.add(nn.Conv2D(channels, kernel_size=1, strides=1)) self.body.add(nn.BatchNorm()) if downsample: self.downsample = nn.HybridSequential(prefix='') self.downsample.add(nn.Conv2D(channels, kernel_size=1, strides=stride, use_bias=False, in_channels=in_channels)) self.downsample.add(nn.BatchNorm()) else: self.downsample = None def hybrid_forward(self, F, x): residual = x x = self.body(x) if self.downsample: residual = self.downsample(residual) x = F.Activation(x + residual, act_type='relu') return xclass BasicBlockV2(HybridBlock): r"""BasicBlock V2 from `"Identity Mappings in Deep Residual Networks" &lt;https://arxiv.org/abs/1603.05027&gt;`_ paper. This is used for ResNet V2 for 18, 34 layers. Parameters ---------- channels : int Number of output channels. stride : int Stride size. downsample : bool, default False Whether to downsample the input. in_channels : int, default 0 Number of input channels. Default is 0, to infer from the graph. """ def __init__(self, channels, stride, downsample=False, in_channels=0, **kwargs): super(BasicBlockV2, self).__init__(**kwargs) self.bn1 = nn.BatchNorm() self.conv1 = _conv3x3(channels, stride, in_channels) self.bn2 = nn.BatchNorm() self.conv2 = _conv3x3(channels, 1, channels) if downsample: self.downsample = nn.Conv2D(channels, 1, stride, use_bias=False, in_channels=in_channels) else: self.downsample = None def hybrid_forward(self, F, x): residual = x x = self.bn1(x) x = F.Activation(x, act_type='relu') if self.downsample: residual = self.downsample(x) x = self.conv1(x) x = self.bn2(x) x = F.Activation(x, act_type='relu') x = self.conv2(x) return x + residualclass BottleneckV2(HybridBlock): r"""Bottleneck V2 from `"Identity Mappings in Deep Residual Networks" &lt;https://arxiv.org/abs/1603.05027&gt;`_ paper. This is used for ResNet V2 for 50, 101, 152 layers. Parameters ---------- channels : int Number of output channels. stride : int Stride size. downsample : bool, default False Whether to downsample the input. in_channels : int, default 0 Number of input channels. Default is 0, to infer from the graph. """ def __init__(self, channels, stride, downsample=False, in_channels=0, **kwargs): super(BottleneckV2, self).__init__(**kwargs) self.bn1 = nn.BatchNorm() self.conv1 = nn.Conv2D(channels//4, kernel_size=1, strides=1, use_bias=False) self.bn2 = nn.BatchNorm() self.conv2 = _conv3x3(channels//4, stride, channels//4) self.bn3 = nn.BatchNorm() self.conv3 = nn.Conv2D(channels, kernel_size=1, strides=1, use_bias=False) if downsample: self.downsample = nn.Conv2D(channels, 1, stride, use_bias=False, in_channels=in_channels) else: self.downsample = None def hybrid_forward(self, F, x): residual = x x = self.bn1(x) x = F.Activation(x, act_type='relu') if self.downsample: residual = self.downsample(x) x = self.conv1(x) x = self.bn2(x) x = F.Activation(x, act_type='relu') x = self.conv2(x) x = self.bn3(x) x = F.Activation(x, act_type='relu') x = self.conv3(x) return x + residual 网络结构VGG-19、简单网络与ResNet的对比： 特点如下： 对于相同的输出特征图尺寸，层具有相同数量的滤波器。 如果特征图尺寸减半，则滤波器数量加倍。通过步长为2的卷积层直接执行下采样。 网络以全局平均池化层和具有softmax的1000维全连接层结束。 不使用dropout。 如下是作者讨论的各种ResNet的架构 ResNet解决了什么问题ResNet解决什么问题？下面是一些观点： 梯度消失的角度。在进行梯度反传计算时，从误差函数(顶部)开始，朝着输入数据方向(底部)逐层计算梯度。当层串联在一起的时候，根据链式法则每层的梯度乘在一起，这样经常导致梯度大小指数衰减。从而在靠近底部的层只得到很小的梯度，随之权重的更新量也变小，使得他们的收敛缓慢，train不动了。越深的网络这个问题越明显。ResNet因为有一个恒等映射，相当于一个高速通道，梯度反传的时候可以直接通过这个高速通道反向传播，前面的层也有梯度，这样整个网络就可以更好的训练，也能将网络做的更深，得到更多的特征。 梯度相关性的角度。The Shattered Gradients Problem: If resnets are the answer, then what is the question?这篇论文专门讨论ResNet为什么有效，论文大意是说，神经网络越来越深的时候，反传回来的梯度之间的相关性会越来越差，最后接近白噪声。因为我们知道图像是具备局部相关性的，那其实可以认为梯度也应该具备类似的相关性，这样更新的梯度才有意义，如果梯度接近白噪声，那梯度更新可能根本就是在做随机扰动。有了梯度相关性这个指标之后，作者分析了一系列的结构和激活函数，发现resnet在保持梯度相关性方面很优秀(相关性衰减从 $\frac{1}{2^L}$ 到了 $\frac{1}{2^L}$ ）。这一点其实也很好理解，从梯度流来看，有一路梯度是保持原样不动地往回传，这部分的相关性是非常强的。 参考文献[1]. Deep Residual Learning for Image Recognition[2]. Identity Mappings in Deep Residual Networks[3]. Very Deep Convolutional Networks for Large-Scale Image Recognition[4]. ImageNet Classification with Deep ConvolutionalNeural Networks[5]. Gradient-Based Learning Applied to Document Recognition[6]. ResNetV2：ResNet深度解析]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>ResNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim使用积累]]></title>
    <url>%2F2018%2F06%2F15%2FVim_Usage_Summary%2F</url>
    <content type="text"><![CDATA[.vimrc文件.vimrc 是控制 vim 行为的配置文件，位于 ~/.vimrc，不论 vim 窗口外观、显示字体，还是操作方式、快捷键、插件属性均可通过编辑该配置文件将 vim 调教成最适合你的编辑器。 命令模式 a 光标后插入，i 光标前插入，A shift+a 行尾插入，I shift+i 行首插入，o 在光标下面插入一行，O shift+o 子啊光标上面茶一行， j 向下移动一行,k 向上移动一行，h 向左移动一个字符，l 向右移动一个字符，gg为跳到第一行行首，G shift+g 最后一行行首；^ 0 移动到行首； $ 移动到行尾； Ctrl+u 视窗往上翻半页；ctrl+d 往下翻半页； Ctrl+f 视窗下翻一页；Ctrl+b 视窗往上翻一页；Ctrl+e 视窗往下卷一行；Ctrl+y 视窗往上卷一行 替换:s（substitute）命令用来查找和替换字符串。语法如下：:{作用范围}s/{目标}/{替换}/{替换标志}例如:%s/foo/bar/g会在全局范围(%)查找foo并替换为bar，所有出现都会被替换(g):s/foo/bar/g当前行]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeNet、AlexNet、VGGNet总结]]></title>
    <url>%2F2018%2F06%2F13%2FLeNet%E3%80%81AlexNet%E3%80%81VGGNet%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文简要回顾LeNet、AlexNet、VGGNet、ResNet这几个经典的CNN网络。 LeNet1998年的LeNet5标志着CNN的真正面世，但是这个模型在后来的一段时间并未能火起来，主要原因是计算能力跟不上，而且其他的算法(如SVM)也能达到类似的效果甚至超过。LeNet结构如下： C1层： 卷积层，6个$5\times5$卷积核S2层： 池化层，区域$2\times2$C3层： 卷积层，16个$5\times5$卷积核S4层： 池化层，区域$2\times2$C5层： 卷积层，120个$5\times5$卷积核F6层： 全连接层 AlexNetAlexNet结构如下： 第一层中的卷积窗口是$11\times11$。因为ImageNet图片高宽均比MNIST大十倍以上，对应图片的物体占用更多的像素，所以使用更大的窗口来捕获物体。同时使用步幅 4 来较大减小输出高宽。 第二层减少到$5\times5$，之后全采用$3\times3$。此外，第一，第二和第五个卷积层之后都跟了窗口为$3\times3$步幅为2的最大池化层。另外，AlexNet使用的通道数也数十倍大于LeNet。 AlexNet做了如下重要改动： 水平翻转 随机剪裁、平移变换 颜色、光照变换 dropout 用relu代替sigmoid Local Response Normalization，即用临近的数据做归一化 多gpu并行 VGGNetVGG是由牛津大学Visual Geometry Group(网络即以课题组的名字命名)提出的卷积神经网络模型。AlexNet指明了加深卷积神经网络的深度可以取得很高的结果，但却没有阐明如何设计更深的网络。VGG提出了可以通过重复使用简单的基础块来构建深层模型。 VGG块VGG模型的基础组成单位是连续数个相同的使用填充1的$3\times3$卷积层后接上一个步幅为2的$2\times2$最大池化层。卷积层保持输入高宽，而池化层则对其减半。 AlexNet在第一层用了$11\times11$（stride 4)的卷基层，但是VGG从第一层开始就使用$3\times3$的小卷积核，并且只使用了stride 1。 改用小卷积核的原因是两个堆叠的$3\times3$卷积的感受野等于一个$5\times5$的感受野，三个相当于$7\times7$，但中间可以加入更多的非线性变换，而且多个小卷积核比一个大卷积核的参数少。一个大小为7的感受野，其参数总量为 $3\times(9\times C^2)$ ，如果直接使用$7\times7$卷积核，其参数总量为$49\times C^2$，这里 $C$ 指的是输入和输出的通道数。很明显，$27\times C^2$小于$49\times C^2$，即减少了参数；而且$3\times 3$卷积核有利于更好地保持图像性质。因为$3\times 3$是最小的能够捕获像素八邻域信息的尺寸。 VGG网络论文中推荐的VGG结构如下图所示，目前主要用VGG-16和VGG-19。 VGG的优点 VGG的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（$3\times 3$）和最大池化尺寸（$2\times 2$）。 几个小滤波器（$3\times 3$）卷积层的组合比一个大滤波器（$5\times 5$或$7\times 7$）卷积层好。 验证了通过不断加深网络结构可以提升性能。 VGG的缺点VGG耗费更多计算资源，并且使用了更多的参数（这里不是$3\times 3$卷积的锅），导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。VGG可是有3个全连接层啊！ PS：有的文章称：发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。 参考文献[1]. Very Deep Convolutional Networks for Large-Scale Image Recognition[2]. ImageNet Classification with Deep ConvolutionalNeural Networks[3]. Gradient-Based Learning Applied to Document Recognition[4]. 一文读懂VGG网络[5]. 为什么倾向于使用3*3 小卷积核堆叠代替大卷积核]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LeNet</tag>
        <tag>AlexNet</tag>
        <tag>VGG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mAP for Object Detection]]></title>
    <url>%2F2018%2F06%2F12%2FmAP%20for%20Object%20Detection%2F</url>
    <content type="text"><![CDATA[mAP是目标检测任务中衡量检测精度的指标。 mAP定义及相关概念 mAP: mean Average Precision, 即各类别AP的平均值 AP: PR曲线下面积 PR曲线: Precision-Recall曲线 Precision: TP / (TP + FP) Recall: TP / (TP + FN) TP: IoU&gt;0.5的检测框数量（同一Ground Truth只计算一次） FP: IoU&lt;=0.5的检测框，或者是检测到同一个GT的多余检测框的数量 FN: 没有检测到的GT的数量 mAP的具体计算要计算mAP必须先绘出各类别PR曲线，计算出AP。而如何采样PR曲线，VOC采用过两种不同方法。参见：The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit 在VOC2010以前，只需要选取当Recall &gt;= 0, 0.1, 0.2, …, 1共11个点时的Precision最大值，然后AP就是这11个Precision的平均值。 在VOC2010及以后，需要针对每一个不同的Recall值（包括0和1），选取其大于等于这些Recall值时的Precision最大值，然后计算PR曲线下面积作为AP值。 假设，对于Aeroplane类别，我们网络有以下输出(BB表示BoundingBox序号，IoU&gt;0.5时GT=1)： BB confidence GT BB1 0.9 1 BB2 0.9 1 BB1 0.8 1 BB3 0.7 0 BB4 0.7 0 BB5 0.7 1 BB6 0.7 0 BB7 0.7 0 BB8 0.7 1 BB9 0.7 1 因此，我们有 TP=5 (BB1, BB2, BB5, BB8, BB9), FP=5 (重复检测到的BB1也算FP)。除了表里检测到的5个GT以外，我们还有2个GT没被检测到，因此: FN = 2. 这时我们就可以按照Confidence的顺序给出各处的PR值，如下： rank Precision Recall 1 1.00 0.14 2 1.00 0.29 3 0.66 0.29 4 0.50 0.29 5 0.40 0.29 6 0.50 0.43 7 0.43 0.43 8 0.38 0.43 9 0.44 0.57 10 0.50 0.71 对于上述PR值，如果我们采用： VOC2010之前的方法，我们选取Recall &gt;= 0, 0.1, …, 1的11处Percision的最大值：1, 1, 1, 0.5, 0.5, 0.5, 0.5, 0.5, 0, 0, 0。此时Aeroplane类别的 AP = 5.5 / 11 = 0.5 VOC2010及以后的方法，对于Recall &gt;= 0, 0.14, 0.29, 0.43, 0.57, 0.71, 1，我们选取此时Percision的最大值：1, 1, 1, 0.5, 0.5, 0.5, 0。此时Aeroplane类别的 AP = (0.14-0)*1 + (0.29-0.14)*1 + (0.43-0.29)*0.5 + (0.57-0.43)*0.5 + (0.71-0.57)*0.5 + (1-0.71)*0 = 0.5 mAP就是对每一个类别都计算出AP然后再计算AP平均值就好了 参考文献[1]. mAP (mean Average Precision) for Object Detection[2]. 目标检测中的mAP是什么含义？]]></content>
      <categories>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>mAP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中常见的评价指标]]></title>
    <url>%2F2018%2F06%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E8%A7%81%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[本文介绍机器学习中常见的评价指标，如：AUC、ROC曲线、PR曲线等。 AUC与ROC曲线AUC是一个模型评价指标，只能用于二分类模型的评价。定义为ROC曲线下的面积，在0到1之间。 ROC曲线ROC全称是Receive Operating Characteristic。根据预测结果以及真实标记可以得到一个混淆矩阵。 预测为P 预测为N 实际为P TP FN 实际为N FP TN 定义如下两个量: TPR=\frac{TP}{TP+FN} \\ FPR=\frac{FP}{FP+TN}称TPR为真正例率，表示实际为正例的样本中预测为正例的比例；称FPR为假正例率，表示实际为反例的样本中预测为正例的比例。ROC空间就是以FPR作为横轴，以TPR作为纵轴的二维空间。这样对于上面的混淆矩阵，我们计算出的(FPR,TPR)对应ROC空间上一个点，这个点表示的就是对应分类器的性能。在ROC空间中越靠近左上角的分类器，性能越好。 对于输出为正例概率或者得分的分类器，可以按照概率或者得分的大小对实例进行排序，这样排在前面的实例是正例的可能性大，排在后面的实例是正例的可能性小。然后按顺序，每次以当前实例的概率/得分做阈值。概率/得分超过或等于此阈值的实例预测为正例，否则预测为反例（其实就是把所有排在当前实例前面的实例以及当前实例预测为正例；其余预测为反例）。这样当前实例有m种，每一种都对应一次预测，而每一次预测都有对应的混淆矩阵。于是在ROC空间中我们可以得到m个点，这m个点的连线称为分类器的ROC曲线。m越大，曲线越光滑。 ROC曲线选用的两个指标，都不依赖于具体的类别分布。注意TPR用到的TP和FN同属P列，FPR用到的FP和TN同属N列，所以即使P或N的整体数量发生了改变，也不会影响到另一列。也就是说，即使正例与负例的比例发生了很大变化，ROC曲线也不会产生大的变化。 AUC值的意义AUC值为ROC曲线所覆盖的区域面积，显然，AUC越大，分类器分类效果越好。 AUC=\sum_{i=2}^{m}\frac{(x_{i}-x_{i-1})*(y_i+y_{i-1})}{2} AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。 0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器妥善设定阈值的话，能有预测价值。 AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。 AUC &lt; 0.5，比随机猜测还差。 假设分类器的输出是样本属于正类的socre（置信度），则AUC的物理意义为，任取一对（正、负）样本，正样本的score大于负样本的score的概率。AUC值越大，正确率越高。 使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。 PR曲线与查准率、查全率、F1-MeasurePR曲线是以R为横坐标，P为纵坐标的曲线。 Precision Recall F1-Measure查准率(Precision)P和查全率(Recall)R的定义为： P=\frac{TP}{TP+FP} \\ R = \frac{TP}{TP+FN}Precision就是当前划分到正样本类别中，被正确分类的比例（即正式正样本所占比例），就是我们一般理解意义上所关心的正样本的分类准确率；Recall = TPR，即当前被分到正样本类别中，真实的正样本占所有正样本的比例，即召回率（召回了多少正样本比例）。 F-Measure是Precision和Recall加权调和平均，当参数a=1时，就是最常见的F1了：： F = \frac{(a^2+1)P*R} {a^2(P+R)} \\ F1 = \frac{2PR} {P+R}我们当然希望检索的结果P越高越好，R也越高越好，但事实上这两者在某些情况下是矛盾的。F1综合了P和R的结果，当F1较高时则比较说明实验方法比较理想。 PR曲线这里举例来说明怎么绘制P-R曲线。假设有测试实例$\{x_1,x_2,x_3,x_4,x_5,x_6\}$，其中真实标记为正例的是$x_1,x_2,x_5$，真实标记为反例的是$x_3,x_4,x_6$。 现在根据学习算法A得到了模型，按其预测的为正例的可能性大小对测试样例进行排序，结果如下：$x_5,x_2,x_1,x_3,x_4,x_6$。 把$x_5$预测为正例，其余预测为反例，则$P=1, R=\frac{1}{3}$，其对应的混淆矩阵如下： 预测为P 预测为N 真实为P 1 2 真实为N 0 3 把$x_5,x_2$预测为正例，其余预测为反例，则$P=1, R=\frac{2}{3}$; 把$x_5,x_2,x_1$预测为正例，其余预测为反例，则$P=1, R=1$; 把$x_5,x_2,x_1,x_3$预测为正例，其余预测为反例，则$P=\frac{3}{4}, R=1$; 把$x_5,x_2,x_1,x_3,x_4$预测为正例，其余预测为反例，则$P=\frac{3}{5}, R=1$; 把$x_5,x_2,x_1,x_3,x_4,x_6$预测为正例，则$P=\frac{1}{2}, R=1$。 综上，可画出P—R曲线如下： 若一个学习器的P-R曲线被另一个学习器完全”包住”，则后者的性能优于前者。当存在交叉时，可以计算曲线围住面积，但比较麻烦。这时候就可以计算F1-Measure。 参考文献[1]. 机器学习之类别不平衡问题 (2) —— ROC和PR曲线[2]. 机器学习之分类性能度量指标 : ROC曲线、AUC值、正确率、召回率[3]. 西瓜书《机器学习》阅读笔记3——Chapter2_ROC曲线[4]. 西瓜书《机器学习》阅读笔记2——Chapter2_查准率、查全率与F1]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AUC</tag>
        <tag>Precision</tag>
        <tag>Recall</tag>
        <tag>ROC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[权重衰减]]></title>
    <url>%2F2018%2F06%2F11%2FWeight_Decay%2F</url>
    <content type="text"><![CDATA[权重衰减(weight decay)等价于 $L2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使得学出的模型参数值较小，是应对过拟合的常用手段。我们先描述 $L2$ 范数正则化，再解释它为何与权重衰减等价。 $L2$ 正则化就是在代价函数后面再加上一个正则化项： \begin{eqnarray} C = C_0 + \frac{\lambda}{2n} \sum_w w^2 \end{eqnarray}$C_0$ 代表原始的代价函数，后面那一项就是 $L2$ 正则化项，$\lambda$ 就是正则项系数。 我们对权重和bias求导如下： \begin{eqnarray} \frac{\partial C}{\partial w} &=& \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} w \\ \frac{\partial C}{\partial b} &=& \frac{\partial C_0}{\partial b} \end{eqnarray}权重和偏差更新如下： \begin{eqnarray} b & \rightarrow & b -\eta \frac{\partial C_0}{\partial b} \\ w & \rightarrow & w-\eta \frac{\partial C_0}{\partial w}-\frac{\eta \lambda}{n} w = \left(1-\frac{\eta \lambda}{n}\right) w -\eta \frac{\partial C_0}{\partial w} \end{eqnarray}可以发现 $L2$ 正则化对b的更新没有影响，但是对 $w$ 的更新有影响。现在 $w$ 前面系数为 $1-\frac{\eta \lambda}{n}$，因为 $\eta\; \lambda \; n$ 都是正的，它的效果是减小 $w$，这也就是权重衰减（weight decay）的由来。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>过拟合</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dropout]]></title>
    <url>%2F2018%2F06%2F11%2FDropout%2F</url>
    <content type="text"><![CDATA[深度学习模型常常使用丢弃法（dropout）来应对过拟合问题。丢弃法有一些不同的变体，这里特指倒置丢弃法（inverted dropout），它被广泛使用于深度学习。简单地所，dropout就是随机地删除网络中的部分单元，然后进行正向和反向传播。 为了确保测试模型的确定性，dropout的使用只发生在训练模型时，并非测试模型时。当神经网络中的某一层使用丢弃法时，该层的神经元将有一定概率被丢弃掉。设丢弃概率为 $p$。 当模型使用了dropout layer，训练的时候只有占比为 $1-p$ 的隐藏层单元参与训练，那么在预测的时候，如果所有的隐藏层单元都需要参与进来，则得到的结果相比训练时平均要大 $\frac{1}{1-p}$，为了避免这种情况，就需要测试的时候将输出结果乘以 $1-p$ 使下一层的输入规模保持不变。而一般使用的inverted dropout，我们可以在训练的时候直接将dropout后留下的权重扩大 $\frac{1}{1-p}$ 倍，这样就可以使结果的scale保持不变，而在预测的时候也不用做额外的操作了，更方便一些。 每次训练迭代时，隐藏层中每个神经元都有可能被丢弃，即我们只激活了一部分神经元在跑，这样整体网络的复杂度就降下来了，可以防止过拟合。dropout一般放在relu之后。 我们可以把dropout类比成将许多大的神经网络进行集成的一种bagging方法。一般莱索每一个神经网络的训练是非常耗时和占用很多内存的，训练很多的神经网络进行集合分类就显得太不实际了。但是，dropout可以训练所有子网络的集合，这些子网络通过去除整个网络中的一些神经元来获得。 bagging与dropout的对比 在bagging中，所有的分类器都是独立的，而在dropout中，所有的模型都是共享参数的； 在bagging中，所有的分类器都是在特定的数据集下训练至收敛，而在dropout中没有明确的模型训练过程。网络都是在一步中训练一次（输入一个样本，随机训练一个子网络）；]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>过拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习中的优化算法]]></title>
    <url>%2F2018%2F06%2F11%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文介绍深度学习中一些常用的优化算法。 梯度下降我们先以简单的一维梯度下降为例，解释梯度下降算法可以降低目标函数值的原因。 一维梯度下降一维梯度是一个标量，也称导数。 假设函数 $f: \mathbb{R} \rightarrow \mathbb{R}$ 的输入和输出都是标量。给定足够小的数 $\epsilon$，根据泰勒展开公式，我们得到以下的近似 f(x + \epsilon) \approx f(x) + f'(x) \epsilon假设 $\eta$ 是一个常数，将 $\epsilon$ 替换为 $-\eta f’(x)$ 后，我们有 f(x - \eta f'(x)) \approx f(x) - \eta f'(x)^2如果 $\eta$ 是一个很小的正数，那么 f(x - \eta f'(x)) \lesssim f(x)也就是说，如果目标函数 $f(x)$ 当前的导数 $f’(x) \neq 0$，按照 x \leftarrow x - \eta f'(x)迭代自变量 $x$ 可能会降低 $f(x)$ 的值。由于导数 $f’(x)$ 是梯度 $\nabla_x f$。 梯度下降算法中的 $\eta$ 叫做学习率。这是一个超参数，需要人工设定。学习率 $\eta$ 要取正数。需要注意的是，学习率过大可能会造成自变量 $x$ 越过（overshoot）目标函数 $f(x)$ 的最优解，甚至发散。从泰勒公式的角度讲，泰勒公式成立的前提条件是 $\epsilon$ 很小，如果 $\epsilon$ 很大，则泰勒公式不成立，梯度下降就难以收敛。 然而，如果学习率过小，目标函数中自变量的收敛速度会过慢。实际中，一个合适的学习率通常是需要通过多次实验找到的。 多维梯度下降现在考虑一个更广义的情况：目标函数的输入为向量，输出为标量。 假设目标函数 $f: \mathbb{R}^d \rightarrow \mathbb{R}$ 的输入是一个 $d$ 维向量 $\boldsymbol{x} = [x_1, x_2, \ldots, x_d]^\top$。目标函数 $f(\boldsymbol{x})$ 有关 $\boldsymbol{x}$ 的梯度是由 $d$ 个偏导数组成的向量： \nabla_{\boldsymbol{x}} f(\boldsymbol{x}) = \bigg[\frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_d}\bigg]^\top为表示简洁，用 $\nabla f(\boldsymbol{x})$ 代替 $\nabla_{\boldsymbol{x}} f(\boldsymbol{x})$。梯度中每个偏导数元素 $\partial f(\boldsymbol{x})/\partial x_i$ 代表着 $f$ 在 $x_i$ 方向的变化率。为了测量 $f$ 沿着单位向量 $\boldsymbol{u}$ 方向上的变化率，在多元微积分中，我们定义 $f$ 在 $\boldsymbol{x}$ 上沿着 $\boldsymbol{u}$ 方向的方向导数为 D_{\boldsymbol{u}} f(\boldsymbol{x}) = \lim_{h \rightarrow 0} \frac{f(\boldsymbol{x} + h \boldsymbol{u}) - f(\boldsymbol{x})}{h}该方向导数可以改写为 D_{\boldsymbol{u}} f(\boldsymbol{x}) = \nabla f(\boldsymbol{x}) \cdot \boldsymbol{u}方向导数 $D_{\boldsymbol{u}} f(\boldsymbol{x})$ 给出了 $f$ 在 $\boldsymbol{x}$上沿着所有可能方向的变化率。为了最小化 $f$，我们希望找到 $f$ 能被降低最快的方向。因此，我们可以通过单位向量 $\boldsymbol{u}$ 来最小化方向导数 $D_{\boldsymbol{u}} f(\boldsymbol{x})$。 由于 D_{\boldsymbol{u}} f(\boldsymbol{x}) = |\nabla f(\boldsymbol{x})| \cdot |\boldsymbol{u}| \cdot \text{cos} (\theta) = |\nabla f(\boldsymbol{x})| \cdot \text{cos} (\theta)其中 $\theta$ 为梯度 $\nabla f(\boldsymbol{x})$ 和单位向量 $\boldsymbol{u}$ 之间的夹角，当 $\theta = \pi$，$\text{cos}(\theta)$ 取得最小值-1。因此，当 $\boldsymbol{u}$ 在梯度方向 $\nabla f(\boldsymbol{x})$ 的相反方向时，方向导数 $D_{\boldsymbol{u}} f(\boldsymbol{x})$ 被最小化。所以，我们可能通过下面的梯度下降算法来不断降低目标函数 $f$ 的值： \boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f(\boldsymbol{x})随机梯度下降当训练数据集很大时，梯度下降每次迭代的计算开销随着样本数量线性增长。因此，当训练数据样本数很大时，梯度下降每次迭代的计算开销很高。这时我们可以使用随机梯度下降。 给定学习率 $\eta$（取正数），在每次迭代时，随机梯度下降算法随机均匀采样 $i$ 并计算 $\nabla f_i(\boldsymbol{x})$来迭代 $\boldsymbol{x}$： \boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f_i(\boldsymbol{x})事实上，随机梯度 $\nabla f_i(\boldsymbol{x})$ 是对梯度 $\nabla f(\boldsymbol{x})$ 的无偏估计： \mathbb{E}i \nabla f_i(\boldsymbol{x}) = \frac{1}{n} \sum{i = 1}^n \nabla f_i(\boldsymbol{x}) = \nabla f(\boldsymbol{x})小批量随机梯度下降广义上，每一次迭代可以随机均匀采样一个由训练数据样本索引所组成的小批量（mini-batch）$\mathcal{B}$。一般来说， 我们可以通过重复采样（sampling with replacement）或者不重复采样（sampling without replacement）得到同一个小批量中的各个样本。前者允许同一个小批量中出现重复的样本，后者则不允许如此。对于这两者间的任一种方式，我们可以使用 \nabla f_\mathcal{B}(\boldsymbol{x}) = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}\nabla f_i(\boldsymbol{x})来迭代 $\boldsymbol{x}$： \boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f_\mathcal{B}(\boldsymbol{x})其中，$|\mathcal{B}|$ 代表样本批量大小，$\eta$（取正数）称作学习率。同样，小批量随机梯度 $\nabla f_\mathcal{B}(\boldsymbol{x})$ 也是对梯度 $\nabla f(\boldsymbol{x})$ 的无偏估计: \mathbb{E}_\mathcal{B} \nabla f_\mathcal{B}(\boldsymbol{x}) = \nabla f(\boldsymbol{x})这个算法叫做小批量随机梯度下降。该算法每次迭代的计算开销为 $\mathcal{O}(|\mathcal{B}|)$。当批量大小为1时，该算法即随机梯度下降；当批量大小等于训练数据样本数，该算法即梯度下降。 Momentum梯度下降算法每次迭代时沿着目标函数下降最快的方向更新参数。在梯度下降中，每次更新参数的方向仅仅取决当前位置，这可能会带来一些问题。 考虑一个输入为二维向量$\mathbf{x} = [x_1, x_2]^\top$，输出为标量的目标函数$f: \mathbb{R}^2 \rightarrow \mathbb{R}$。下面为该函数的等高线示意图（每条等高线表示相同函数值的点：越靠近中间函数值越小）。 由于目标函数在竖直方向（ $x_2$ 轴方向）比在水平方向（ $x_1$ 轴方向）更弯曲，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。因此，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这造成了上图中自变量向最优解移动较慢。 以小批量随机梯度下降为例（当批量大小等于训练集大小时，该算法即为梯度下降；批量大小为1即为随机梯度下降），对小批量随机梯度算法做如下修改，就得到了动量法： \begin{split}\begin{aligned} \boldsymbol{v} &\leftarrow \gamma \boldsymbol{v} + \eta \nabla f_\mathcal{B}(\boldsymbol{x}),\\ \boldsymbol{x} &\leftarrow \boldsymbol{x} - \boldsymbol{v}. \end{aligned}\end{split}其中 $\boldsymbol{v}$ 是当前速度，动量参数 $\gamma$ 满足 $0 \leq \gamma \leq 1$。 为了更清晰地理解动量法，先解释指数加权移动平均（exponentially weighted moving average）。给定超参数 $\gamma$ 且 $0 \leq \gamma &lt; 1$，当前时刻 $t$ 的变量 $y^{(t)}$ 是上一时刻 $t-1$ 的变量 $y^{(t-1)}$ 和当前时刻另一变量 $x^{(t)}$ 的线性组合： y^{(t)} = \gamma y^{(t-1)} + (1-\gamma) x^{(t)}对 $y^{(t)}$ 展开： \begin{aligned} y^{(t)} &= (1-\gamma) x^{(t)} + \gamma y^{(t-1)} \ \\ &= (1-\gamma)x^{(t)} + (1-\gamma) \cdot \gamma x^{(t-1)} + \gamma^2y^{(t-2)} \ \\ &= (1-\gamma)x^{(t)} + (1-\gamma) \cdot \gamma x^{(t-1)} + (1-\gamma) \cdot \gamma^2x^{(t-2)} + \gamma^3y^{(t-3)}\ \\ &\ldots \end{aligned}由于 \lim_{n \rightarrow \infty} (1-\frac{1}{n})^n = \exp(-1) \approx 0.3679可以将 $\gamma^{1/(1-\gamma)}$ 近似为 $\exp(-1)$。例如 $0.95^{20} \approx \exp(-1)$。如果把 $\exp(-1)$ 当做一个比较小的数，可以在近似中忽略所有含 $\gamma^{1/(1-\gamma)}$ 和比 $\gamma^{1/(1-\gamma)}$ 更高阶的系数的项。例如，当 $\gamma=0.95$ 时， y^{(t)} \approx 0.05 \sum_{i=0}^{19} 0.95^i x^{(t-i)}因此，在实际中，常常将 $y$ 看作是对最近 $1/(1-\gamma)$ 个时刻的 $x$ 值的加权平均。例如，当 $\gamma = 0.95$ 时，$y$ 可以被看作是对最近20个时刻的 $x$ 值的加权平均；当 $\gamma = 0.9$ 时，$y$ 可以看作是对最近10个时刻的 $x$ 值的加权平均：离当前时刻越近的 $x$ 值获得的权重越大。 现在，我们对动量法的速度变量做变形： \boldsymbol{v} \leftarrow \gamma \boldsymbol{v} + (1 - \gamma) \frac{\eta \nabla f_\mathcal{B}(\boldsymbol{x})}{1 - \gamma}由指数加权移动平均的形式可得，速度变量 $\boldsymbol{v}$ 实际上对 $(\eta\nabla f_\mathcal{B}(\boldsymbol{x})) /(1-\gamma)$ 做了指数加权移动平均。给定动量超参数 $\gamma$ 和学习率 $\eta$，含动量法的小批量随机梯度下降可被看作使用了特殊梯度来迭代目标函数的自变量。这个特殊梯度是最近 $1/(1-\gamma)$ 个时刻的 $\nabla f_\mathcal{B}(\boldsymbol{x})/(1-\gamma)$ 的加权平均。 给定目标函数，在动量法的每次迭代中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决过去各个梯度在各个方向上是否一致。图示展示了使用动量法的梯度下降迭代目标函数自变量的情景。我们将每个梯度代表的箭头方向在水平方向和竖直方向做分解。由于所有梯度的水平方向为正（向右）、在竖直上时正（向上）时负（向下），自变量在水平方向移动幅度逐渐增大，而在竖直方向移动幅度逐渐减小。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。 目标函数 $f$ 的等高线图和自变量 $[x_1, x_2]$ 在使用动量法的梯度下降中的迭代。每条等高线（椭圆实线）代表所有函数值相同的自变量的坐标。实心圆代表自变量初始坐标。每个箭头头部代表自变量在每次迭代后的坐标。 Adagrad优化算法中，无论是梯度下降、随机梯度下降、小批量随机梯度下降还是使用动量法，目标函数自变量的每一个元素在相同时刻都使用同一个学习率来自我迭代。 举个例子，假设目标函数为 $f$，自变量为一个多维向量 $[x_1, x_2]^\top$，该向量中每一个元素在更新时都使用相同的学习率。例如在学习率为 $\eta$ 的梯度下降中，元素 $x_1$ 和 $x_2$ 都使用相同的学习率$\eta$来自我迭代： x_1 \leftarrow x_1 - \eta \frac{\partial{f}}{\partial{x_1}}, \ x_2 \leftarrow x_2 - \eta \frac{\partial{f}}{\partial{x_2}}如何让 $x_1$ 和 $x_2$ 使用不同的学习率自我迭代？Adagrad就是一个在迭代过程中不断自我调整学习率，并让模型参数中每个元素都使用不同学习率的优化算法。 Adagrad的算法会使用一个小批量随机梯度按元素平方的累加变量 $\boldsymbol{s}$，并将其中每个元素初始化为0。在每次迭代中，首先计算小批量随机梯度 $\boldsymbol{g}$，然后将该梯度按元素平方后累加到变量 $\boldsymbol{s}$，算法如下： \boldsymbol{s} \leftarrow \boldsymbol{s} + \boldsymbol{g} \odot \boldsymbol{g} \\ \boldsymbol{g}^\prime \leftarrow \frac{\eta}{\sqrt{\boldsymbol{s} + \epsilon}} \odot \boldsymbol{g} \\ \boldsymbol{x} \leftarrow \boldsymbol{x} - \boldsymbol{g}^\prime其中 $\eta$ 是初始学习率且 $\eta &gt; 0$，$\epsilon$ 是为了维持数值稳定性而添加的常数，例如 $10^{-7}$。需要注意其中按元素开方、除法和乘法的运算。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。 需要强调的是，小批量随机梯度按元素平方的累加变量 $\boldsymbol{s}$ 出现在含调整后学习率的梯度 $\boldsymbol{g}^\prime$ 的分母项。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么就让该元素的学习率下降快一点；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么就让该元素的学习率下降慢一点。然而，由于 $\boldsymbol{s}$ 一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，Adagrad在迭代后期由于学习率过小，可能较难找到一个有用的解。 RMSProp在Adagrad中，由于调整学习率时分母上的变量 $\boldsymbol{s}$ 一直在累加按元素平方的小批量随机梯度，目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，Adagrad在迭代后期由于学习率过小，可能较难找到一个有用的解。为了应对这一问题，RMSProp算法对Adagrad做了一点小小的修改。 RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均变量 $\boldsymbol{s}$，并将其中每个元素初始化为0。 给定超参数 $\gamma$ 且 $0 \leq \gamma &lt; 1$ ，在每次迭代中，RMSProp首先计算小批量随机梯度 $\boldsymbol{g}$，然后对该梯度按元素平方项 $\boldsymbol{g} \odot \boldsymbol{g}$ 做指数加权移动平均，记为 $\boldsymbol{s}$，算法如下： \boldsymbol{s} \leftarrow \gamma \boldsymbol{s} + (1 - \gamma) \boldsymbol{g} \odot \boldsymbol{g} \\ \boldsymbol{g}^\prime \leftarrow \frac{\eta}{\sqrt{\boldsymbol{s} + \epsilon}} \odot \boldsymbol{g} \\ \boldsymbol{x} \leftarrow \boldsymbol{x} - \boldsymbol{g}^\prime其中 $\eta$ 是初始学习率且 $\eta &gt; 0$，$\epsilon$ 是为了维持数值稳定性而添加的常数，例如 $10^{-8}$ 。 需要强调的是，RMSProp只在Adagrad的基础上修改了变量 $\boldsymbol{s}$ 的更新方法：对平方项 $\boldsymbol{g} \odot \boldsymbol{g}$ 从累加变成了指数加权移动平均。由于变量 $\boldsymbol{s}$ 可看作是最近 $1/(1-\gamma)$ 个时刻的平方项 $\boldsymbol{g} \odot \boldsymbol{g}$ 的加权平均，自变量每个元素的学习率在迭代过程中避免了“只降不升”的问题。 AdadeltaAdadelta也是针对Adagrad在迭代后期可能较难找到有用解的问题，对小批量随机梯度按元素平方项做指数加权移动平均而不是累加。但Adadelta中没有学习率超参数。 Adadelta算法也像RMSProp一样，使用了小批量随机梯度按元素平方的指数加权移动平均变量 $\boldsymbol{s}$，并将其中每个元素初始化为0。 给定超参数 $\rho$ 且 $0 \leq \rho &lt; 1$， 在每次迭代中，RMSProp首先计算小批量随机梯度 $\boldsymbol{g}$，然后对该梯度按元素平方项 $\boldsymbol{g} \odot \boldsymbol{g}$ 做指数加权移动平均，记为$\boldsymbol{s}$： \boldsymbol{s} \leftarrow \rho \boldsymbol{s} + (1 - \rho) \boldsymbol{g} \odot \boldsymbol{g}然后，计算当前需要迭代的目标函数自变量的变化量 $\boldsymbol{g}^\prime$： \boldsymbol{g}^\prime \leftarrow \frac{\sqrt{\Delta\boldsymbol{x} + \epsilon}}{\sqrt{\boldsymbol{s} + \epsilon}} \odot \boldsymbol{g}其中 $\epsilon$ 是为了维持数值稳定性而添加的常数，例如 $10^{-5}$。和Adagrad与RMSProp一样，目标函数自变量中每个元素都分别拥有自己的学习率。上式中 $\Delta\boldsymbol{x}$ 初始化为零张量，并记录 $\boldsymbol{g}^\prime$ 按元素平方的指数加权移动平均： \Delta\boldsymbol{x} \leftarrow \rho \Delta\boldsymbol{x} + (1 - \rho) \boldsymbol{g}^\prime \odot \boldsymbol{g}^\prime同样地，最后的自变量迭代步骤与小批量随机梯度下降类似： \boldsymbol{x} \leftarrow \boldsymbol{x} - \boldsymbol{g}^\primeAdamAdam是一个组合了动量法和RMSProp的优化算法，使用了动量变量 $\boldsymbol{v}$ 和RMSProp中变量 $\boldsymbol{s}$，并将它们中每个元素初始化为0。在每次迭代中，时刻 $t$ 的小批量随机梯度记作 $\boldsymbol{g}_t$。 和动量法类似，给定超参数 $\beta_1$ 且满足 $0 \leq \beta_1 &lt; 1$（算法作者建议设为0.9），将小批量随机梯度的指数加权移动平均记作动量变量 $\boldsymbol{v}$，并将它在时刻 $t$ 的值记作 $\boldsymbol{v}_t$： \boldsymbol{v}_t \leftarrow \beta_1 \boldsymbol{v}_{t-1} + (1 - \beta_1) \boldsymbol{g}_t和RMSProp中一样，给定超参数 $\beta_2$ 且满足 $0 \leq \beta_2 &lt; 1$（算法作者建议设为0.999）， 将小批量随机梯度按元素平方后做指数加权移动平均得到 $\boldsymbol{s}$ ，并将它在时刻 $t$ 的值记作 $\boldsymbol{s}_t$： \boldsymbol{s}_t \leftarrow \beta_2 \boldsymbol{s}_{t-1} + (1 - \beta_2) \boldsymbol{g}_t \odot \boldsymbol{g}_t需要注意的是，当 $t$ 较小时，过去各时刻小批量随机梯度权值之和会较小。例如当 $\beta_1 = 0.9$ 时，$\boldsymbol{v}_1 = 0.1\boldsymbol{g}_1$。为了消除这样的影响，对于任意时刻 $t$ ，我们可以将 $\boldsymbol{v}_t$ 再除以 $1 - \beta_1^t$ ，从而使得过去各时刻小批量随机梯度权值之和为1。这也叫做偏差修正。在Adam算法中，我们对变量 $\boldsymbol{v}$ 和 $\boldsymbol{s}$ 均作偏差修正： \hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_1^t} \\ \hat{\boldsymbol{s}}_t \leftarrow \frac{\boldsymbol{s}_t}{1 - \beta_2^t}接下来，Adam算法使用以上偏差修正后的变量 $\hat{\boldsymbol{v}}_t$ 和 $\hat{\boldsymbol{s}}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整： \boldsymbol{g}_t^\prime \leftarrow \frac{\eta \hat{\boldsymbol{v}}_t}{\sqrt{\hat{\boldsymbol{s}}_t + \epsilon}}其中 $\eta$ 是初始学习率且 $\eta &gt; 0$，$\epsilon$ 是为了维持数值稳定性而添加的常数，例如 $10^{-8}$。和Adagrad、RMSProp以及Adadelta一样，目标函数自变量中每个元素都分别拥有自己的学习率。 最后，时刻 $t$ 的自变量 $\boldsymbol{x}_t$ 的迭代步骤与小批量随机梯度下降类似： \boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t^\prime参考文献[1]. 《深度学习》 [美] 伊恩·古德费洛 / [加] 约书亚·本吉奥 / [加] 亚伦·库维尔[2]. 动手深度学习]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>sgd</tag>
        <tag>Adagrad</tag>
        <tag>动量法</tag>
        <tag>RMSProp</tag>
        <tag>Adadelta</tag>
        <tag>Adam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-means聚类]]></title>
    <url>%2F2018%2F06%2F10%2Fk-means%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>k-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的类别不均衡问题]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[类别不均衡是指在分类学习算法中，不同类别样本的比例相差悬殊，它会对算法的学习过程造成重大的干扰。比如在一个二分类的问题上，有1000个样本，其中5个正样本，995个负样本，在这种情况下，算法只需将所有的样本预测为负样本，那么它的精度也可以达到99.5%，虽然结果的精度很高，但它依然没有价值，因为这样的学习算法不能预测出正样本。 解决方法: 欠采样，减少数量较多那一类样本的数量，使得正负样本比例均衡。 过采样，增加数量较少那一类样本的数量，使得正负样本比例均衡。 不处理样本，样本分类阈值移动。 欠采样如果随机从多数类样本中抽取一部分数据进行删除，可能会误删样本中一些重要的信息。不建议直接随机删除。目前比较流行的方法是EasyEnsemble和BalanceCascade。 EasyEnsemble 首先从多数类中有放回随机抽取出若干子集。 将每个子集与少数类数据联合起来训练生成多个基分类器。 最终将这些基分类器组合形成一个集成学习系统。 BalanceCascade通过一次随机欠采样产生训练集，训练一个分类器，对于那些分类正确的多数类样本不放回，然后对这个剩下的多数类样本再次进行欠采样产生第二个训练集，训练第二个分类器，同样把分类正确的样本不放回，以此类推，直到满足某个停止条件，最终的模型也是集成分类器。 过采样过采样法不能简单地对初始正例样本进行重复采样，否则会造成严重的过拟合。目前的代表性算法是SMOTE，此算法基于“插值”来为少数类合成新的样本。 SMOTE设训练集的一个少数类的样本数为 $T$ ，SMOTE算法将为这个少数类合成 $NT$ 个新样本。考虑该少数类的一个样本 $i$ ，其特征向量为 $\boldsymbol x_i,i\in\{1,…,T\}$ ： 首先从该少数类的全部 $T$ 个样本中找到样本 $\boldsymbol x_i$ 的 $k$ 个近邻（例如用欧氏距离），记为 $\boldsymbol x_{i(near)},near\in\{1,…,k\}$ ； 然后从这 $k$ 个近邻中随机选择一个样本 $\boldsymbol x_{i(nn)}$ ，再生成一个 0 到 1 之间的随机数 $\zeta_1$ ，从而合成一个新样本 $\boldsymbol x_{i1}$ ： \boldsymbol x_{i1}=\boldsymbol x_i+\zeta_1\cdot (\boldsymbol x_{i(nn)}−\boldsymbol x_i) 将步骤2重复进行 $N$ 次，从而可以合成 $N$ 个新样本：$\boldsymbol x_{inew},new\in{1,…,N}$。 那么，对全部的 $T$ 个少数类样本进行上述操作，便可为该少数类合成 $NT$ 个新样本。 如果样本的特征维数是 2 维，那么每个样本都可以用二维平面上的一个点来表示。SMOTE算法所合成出的一个新样本 $\boldsymbol x_{i1}$ 相当于是表示样本 $\boldsymbol x_i$ 的点和表示样本 $\boldsymbol x_{i(nn)}$ 的点之间所连线段上的一个点。所以说该算法是基于“插值”来合成新样本。 调整分类阈值从线性分类器的角度来进行理解。使用 $\boldsymbol {y=\omega^Tx}+b$ 对新样本 $\boldsymbol x$ 进行分类的时候，实际上是在用预测出的 $\boldsymbol y$ 与一个阈值进行比较，比如通常情况下我们将这个阈值设置为 0.5。当大于 0.5 则判定为正例，否则判定为反例。$\boldsymbol y$ 实际上表达了正例的可能性，几率 $\frac{y}{1-y}$ 反映的是正例的可能性与反例的可能性的比值。当输出为 1 的时候表示两种可能性相同。所以有如下决策： 若\frac{y}{1-y}>1\ \ 则预测为正例然而，当训练样本中的正负样本数目不同的时候，这时我们令 $m^+$ 表示正例样本的数量，$m^-$ 表示负样本的数量，则可以得到观测几率 $\frac{m^+}{m^-}$。因为通常假设训练集是真实样本集的无偏采样，所以观测几率也就代表了真实几率。所以应该存在下面的判定决策： 若\frac{y}{1-y}>\frac{m^+}{m^-}\ \ 则预测为正例但是我们的分类器是基于 $\frac{y}{1-y}&gt;1$ 进行决策，所以，需要对上面这个预测值进行调整，使得在执行 $\frac{y}{1-y}&gt;1$ 时，实际是在执行 $\frac{y}{1-y}&gt;\frac{m^+}{m^-}$。只需要做出如下调整即可： \frac{y'}{1-y'}=\frac{y}{1-y}\times \frac{m^-}{m^+}而这也就是类别不平衡学习的基本策略——再缩放(rescaling)。 从上面的公式中我们就可以看出，再缩放的思想十分简单，但是在实际应用中却并不是很容易，主要是 “训练集是真实样本总体的无偏采样” 这个假设往往不成立，也就是说，不一定能有效基于训练样本的观测几率来推断出真是几率。 参考文献[1]. 类不平衡问题与SMOTE过采样算法[2]. 《机器学习》 周志华[3]. 不均衡学习的抽样方法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>类别不均衡</tag>
        <tag>欠采样</tag>
        <tag>过采样</tag>
        <tag>再缩放</tag>
        <tag>SMOTE</tag>
        <tag>EasyEnsemble</tag>
        <tag>BalanceCascade</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM和LR的对比分析]]></title>
    <url>%2F2018%2F06%2F08%2FSVM%E5%92%8CLogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[本文对svm和lr进行对比分析。 SVM和LR的不同 样本点对模型的作用不同。SVM中，只有关键的样本点（支持向量）对模型结果有影响，而LR中，每一个样本点都对模型有影响。 损失函数不同。SVM是hinge损失函数，LR是log损失函数 输出不同。LR可以对每个样本点给出类别判断的概率值，SVM无法做到。 可处理的特征空间维度不同。LR在特征空间维度很高时，表现较差。SVM则可以通过对偶求解高效应对这一挑战。 防过拟合能力不同。SVM模型中内含了L2正则，可有效防止过拟合。LR要自己添加正则项。 处理非线性分类问题能力不同。SVM可通过核函数灵活地将非线性问题转化为线性分类问题。LR如果要做到这一点，需要自己手动地进行特征转换。 处理分类问题能力不同。SVM只能处理二类分类问题，如果要处理多类别分类，需要进行 one VS one 或one VS all建模。LR可以直接进行多类别分类。 计算复杂度不同。对于海量数据，SVM的效率较低，LR效率比较高。 对数据要求不同。SVM依赖于数据表达出的距离测度，所以需要对数据进行标准化处理，而LR不需要。 能力范围不同。 SVM拓展后，可解决回归问题，LR不能。 抗噪声数据能力不同。SVM的损失函数基于距离测度，抗噪声能力要强于LR。 LR和SVM哪个更能对付异常点Outlier？LR的loss function是对数loss L(y_n f(x_n)) = ln(1 + exp(-y_n f(x_n)))SVM的loss function是hinge损失 L(y_n f(x_n)) = max(0, 1-y_n f(x_n))函数图像如下： 当$f$对样本预测正确并且大于一定程度时，确切说就是当$yf(x)$大于1时，二者的行为明显不同，lr的loss还会鼓励$f$继续增大$yf(x)$，而svm则会告诉$f$，对这个样本预测已经非常完美了。 所谓outlier，是怎么产生的，无非有两种情况，一种就是这个样本的标签$y$搞错了，一种就是没搞错，但这个样本是一个个例，不具备统计特性。 不论对于哪一种情况，svm会在$f$将这个out lier预测的比较正确时，就停止，不会一直优化对outlier的预测，因为没有什么太大意义了。而lr则不同，它会继续要求$f$对这个outlier的预测进行优化，并且永不停止，显然，这样的优化很可能会削弱$f$的泛化性能，因为没有必要死磕outlier 。 并且从思想上讲， SVM只使用了支撑向量，而LR使用的全部样本，所以SVM更不容易被outliers影响。 参考文献[1]. 数据挖掘面试题之SVM和LR的不同[2]. 机器学习面试之LR和SVM哪个更能对付异常点out lier？]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>logistic</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic回归]]></title>
    <url>%2F2018%2F06%2F08%2FLogistic%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[Logistics回归也叫对数几率回归，是统计学习中的经典分类方法，属于线性模型。虽然被叫做回归，实际是一个分类方法。 logistics回归模型假设有一个二分类问题，输出为$y \in \{0, 1\}$，而线性回归模型产生的预测值为$z = w^Tx + b$是实数值，我们希望实现 $z$ 值到 $0/1$ 值的转化，最理想的阶跃函数（unit-step function）。 \begin{equation*} y = \begin{cases} 0 & \mbox{z < 0}\\ 0.5 & \mbox{z = 0} \\ 1 & \mbox{z > 0} \end{cases} \end{equation*}若 $z$ 大于0就判为正类，小于0就判为反类，临界值任意。然而阶跃函数不可导，不连续，我们希望有一个单调可微的函数来供我们使用。我们使用Sigmoid function： \phi (z) = \frac{1}{1+e^{-z}}其图像如下： 将 $z = w^Tx + b$ 代入，得： \phi (z) = \frac{1}{1+e^{-(w^Tx+b)}}化简可得： \ln(\frac{\phi (z) }{1-\phi (z) }) = w^Tx+b事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是 $p$，那么该事件的几率是 \frac{P}{1-P}该事件的对数几率（log odds）： logit(P)=\log\frac{p}{1-p}由此可以看出，sigmoid函数的作用实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。 我们将$\phi(z)$视为后验概率估计$P(y=1|x)$，则 ln(\frac{P(y=1|x)}{P(y=0|x)}) = w^Tx+b显然有： P(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}} \\ P(y=0|x)=\frac{1}{1+e^{w^Tx+b}}这样的模型就是logistics回归模型。 参数估计最大化似然函数即为最小化负的似然函数，从损失函数的视角来看，它就成了log损失函数了： L(Y,P(Y|X)) = -\log P(Y|X)令$z = w^Tx + b$，设： P(y=1|x;w) = \phi(w^Tx + b) = \phi (z) \\ P(y=0|x;w) = 1 - \phi (z) \\将它带入到上式，通过推导可以得到logistic的损失函数表达式，如下： \begin{aligned} L(y,P(Y=y|x)) = \left\{\begin{matrix} \log (1+exp\ (-z\ )) & ,y=1\\ \log (1+exp\ ( z\ )) & ,y=0\\ \end{matrix}\right. \end{aligned}似然函数： L(w)=\prod_{i=1}^{n}p(y_i|x_i;w)=\prod_{i=1}^{n}(\phi(z_i))^{y_i}(1-\phi(z_i))^{1-y_i}对数似然函数： \begin{eqnarray*} l(w)=lnL(w)=\sum_{i = 1}^n [y_iln(\phi(z_i)) + (1 - y_i)ln(1-\phi(z_i))] \end{eqnarray*}损失函数如下： J(w)=-\frac{1}{n}l(w)=-\frac{1}{n}\sum_{i = 1}^n [y_iln(\phi(z_i)) + (1 - y_i)ln(1-\phi(z_i))]其中，$n$为样本个数。 这样，就可以采用梯度下降或拟牛顿法来最小化损失函数。 梯度下降使用梯度下降法来对参数更新。 参数更新如下： w_j := w_j + \Delta w_j,\ \Delta w_j = -\eta \dfrac{\partial J(w)}{\partial w_j}其中，$w_j$表示第$j$个特征的权重；$\eta$为学习率，用来控制步长。 $sigmoid function$有一个很好的性质就是$\phi’(z) = \phi(z)(1 - \phi(z))$ ，这个导数在接下来的 求偏导如下： \begin{align*} \dfrac{\partial J(w)}{w_j} &= -\frac{1}{n}\sum_{i=1}^n (y_i\dfrac{1}{\phi(z_i)}-(1 - y_i)\dfrac{1}{1-\phi(z_i)})\dfrac{\partial \phi(z_i)}{\partial w_j} \\ &=-\frac{1}{n}\sum_{i=1}^n (y_i\dfrac{1}{\phi(z_i)}-(1 - y_i)\dfrac{1}{1-\phi(z_i)})\phi(z_i)(1-\phi(z_i))\dfrac{\partial z_i}{\partial w_j} \\ &=-\frac{1}{n}\sum_{i=1}^n (y_i(1-\phi(z_i))-(1-y_i)\phi(z_i))x_i^{(j)} \\ &=-\frac{1}{n}\sum_{i=1}^n (y_i-\phi(z_i))x_i^{(j)} \end{align*}所以，在使用梯度下降法更新权重时，只要根据下式即可 w_j :=w_j+\eta \frac{1}{n} \sum_{i=1}^n (y_i-\phi(z_i))x_i^{(j)}$x_i^{(j)}$表示第$i$个样本的第$j$个分量。 随机梯度下降在样本量极大的时候，每次更新权重会非常耗费时间，这时可以采用随机梯度下降法，这时每次迭代时需要将样本重新打乱，然后用下式不断更新权重。 for \ i \ in \ range(n) \\ w_j := w_j + \eta (y_i-\phi(z_i))x_i^{(j)},也就是去掉了求和，而是对随机样本点都进行更新。 为什么使用sigmoid函数事实上，我们对logistic回归做了如下假设：假设给定特征$x$和参数，$y$是服从伯努利分布的二值随机变量。我们的任务用数学语言描述就是，寻找一个模型，输入$x$后，可以告诉我们$y$所服从的随机分布的参数，知道参数后，就可以计算$y$的期望作为预测。分布的参数就是$\phi=P(y=1)$，同时也是该分布的期望。 我们的思路如下： 对每一个确定的$x$，$y$仍然是一个随机变量 该随机变量服从某个随机分布 努力求出这个随机分布的参数 求出该随机分布的期望 将期望作为预测值 从更高的层次看伯努利分布如何根据$x$求出$y$所属的伯努利分布的参数$\phi$呢。直接看，似乎没什么思路，我们需要换个角度。 伯努利分布实际上属于某一大类分布中的一种情况。这一大类分布就是指数分布族。这就好比， $x + 1=0$是一个方程，但从更广泛的角度来看，它只是 $ax + b = 0$一次方程的一种具体情况而已。 从指数分布族的角度来分析，我们很容易构建起$x$与伯努利分布参数的联系。 指数分布族指数分布家族是定义的一组分布式，伯努利分布及高斯分布等分布都可以看做指数分布的特殊形式。指数分布的公式如下： p(y;\eta) = b(y)exp(\eta^TT(y)-a(\eta))$p(y;\eta)$表示$y$在$\eta$下概率，但是它不是条件概率，因为$\eta$不是随机变量。 它告诉我们：对于一个随机变量$y$，只要确定三个函数，就可以确定一类分布。这三个函数就是：$b(y)\quad T(y) \quad A(\eta)$，$\eta$用来确定该类分布的具体参数。 对于伯努利分布，首先，我们假设$p(y=1;\phi) = \phi;p(y=0;\phi) = 1-\phi$，这个就是定义。 \begin{aligned} p(y;\phi) &= \phi^y (1-\phi)^{(1-y)} \\ &= exp[yln\phi+(1-y)ln(1-\phi)]\\ &= exp[yln{\phi \over1-\phi}+ln(1-\phi)]\\ \end{aligned}其中从定义来讲 $y=0 || y=1$（因为伯努利分布） 那么我们做下列转化，令$\eta = ln{\phi \over(1-\phi)},\phi = {1\over1+e^{-\eta}}$,自然参数和均值参数之间的转化正好满足sigmoid函数 b(y)=1\\ T(y)=y\\ a(\eta)=-ln(1-\phi)=-ln{1\over1+e^\eta}最后，可以得到： p(y;\eta) = exp[\eta y-ln{1 \over 1+e^\eta}]可以看到，伯努利分布确实可以改写成指数分布族的形式。并且，伯努利分布的参数$\phi$与$\eta$之间，还有一个sigmoid的函数关系。 参考文献[1]. 《机器学习》 周志华[2]. 《统计学习方法》 李航[3]. 逻辑斯蒂回归[4]. 对数几率回归推导[5]. 逻辑回归(logistic regression)的本质——极大似然估计[6]. 机器学习面试之逻辑回归输出的值是真实的概率吗？[7]. 指数分布族和广义线性模型[8]. 数据挖掘面试题之SVM和LR的不同[9]. logistic回归详解(二）：损失函数（cost function）详解[10]. 逻辑回归的常见面试点总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>logistic</tag>
        <tag>极大似然估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机（SVM）]]></title>
    <url>%2F2018%2F06%2F08%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%2F</url>
    <content type="text"><![CDATA[支持向量机(support vector machines，SVM)是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。 间隔与支持向量分类学习的最基本想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。能将训练样本划分开的平面可能有很多个，应该选择位于两类训练样本正中间的划分超平面，原因是这个超平面的分类结果最鲁棒，泛化能力最强。在样本空间中，划分超平面可通过以下线性方程来描述 \omega^T x + b = 0样本空间中任意点 $x$ 到超平面 $(\omega,b)$ 的距离可以写成 r = \frac{\vert \omega^T x + b \vert}{\vert \vert \omega \vert \vert}假设超平面能够正确分类样本，则可以通过对 $\omega$ 缩放可以使得下式成立 \left\{ \begin{aligned} \omega^T x_i + b \geq +1, & y_i = +1 \\ \omega^T x_i + b \leq -1, & y_i = -1\\ \end{aligned} \right.距离超平面最近的几个样本点使得上式等号成立，称作“支持向量”。两个异类支持向量到超平面的距离之和称为“间隔”，$\gamma = \frac{2}{\vert \vert \omega \vert \vert}$ 欲最大化间隔，等价于最小化 $\frac{1}{2}\vert \vert \omega \vert \vert^2$, 这就是支持向量机的基本型。 \begin{split} & \min_{\omega,b} \ \frac{1}{2}\vert \vert \omega \vert \vert^2\\ & s.t. \quad y_i (\omega^Tx_i + b) \geq 1,i=1,\cdots,m \end{split}对偶问题上述问题是一个凸二次规划问题，能直接用现成的优化计算包求解。但是通过拉格朗日乘子法变换到对偶变量的优化问题之后，可以找到一种更加有效的方法来进行求解。 原问题的拉格朗日函数为 L(\omega,b,\alpha) = \frac{1}{2} \vert \vert \omega \vert \vert^2 + \sum\limits_{i=1}^{m} \alpha_i (1 - y_i (\omega^T x_i + b)) \tag1 \\ \alpha_i \geq 0, \ \alpha=(\alpha_1,\alpha_2,...,\alpha_m)^T, \ i=1,2,...,m原始问题的对偶问题是： \max_\alpha \;\min_{\omega,b}\; L(\omega,b,\alpha)先求 $\min\limits_{\omega,b} L(\omega,b,\alpha)$，对 $\omega, b$ 求偏导为零可以得到 \begin{aligned} & \omega = \sum\limits_{i=1}^{m} \alpha_i y_i x_i \\ & 0 = \sum\limits_{i=1}^{m} \alpha_i y_i \\ \end{aligned}将 $\omega$ 代入 $(1)$，得到对偶问题为： \begin{aligned} & \max_{\alpha} \ \sum\limits_{i=1}^{m} \alpha_i - \frac{1}{2} \sum\limits_{i=1}^{m} \sum\limits_{j=1}^{m} \alpha_i \alpha_j y_i y_j x_i^T x_j\\ &s.t. \quad \sum\limits_{i=1}^{m} \alpha_i y_i = 0 \\ & \quad\quad\ \ \ \alpha_i \geq 0, i=1,\cdots,m \end{aligned}解出 $\alpha$ 后，求出 $\omega, b$，于是可以得到模型为 \begin{aligned} f(x) &= \omega^T x + b \\ &= \sum\limits_{i=1}^{m} \alpha_i y_i x_i^T x + b \end{aligned}上述过程需要满足KKT(Karush-Kuhn-Tucker)条件，即 \left\{ \begin{aligned} & \alpha_i \geq 0 \\ & y_if(x_i) - 1 \geq 0 \\ & \alpha_i (y_if(x_i) - 1) = 0 \end{aligned} \right.对任意训练样本 $(x_i,y_i)$，总有 $\alpha_i=0$ 或 $y_i f(x_i) = 1$ 。若 $\alpha_i=0$，则该样本不会在 $f(x)$ 的求和中出现，也就不会对 $f(x)$ 有任何影响；若 $\alpha_i &gt; 0$，则必有 $y_if(x_i)=1$ ，所对应样本点位于最大间隔边界上，是一个支持向量。因此训练完成后，大部分的样本都不需要保留，最终模型仅与支持向量有关。 SMO算法如果用二次规划算法求解对偶问题，则问题的规模正比于训练样本数，这会在实际任务中造成很大开销，为此提出SMO(Sequential Minimal Optimization)算法。 步骤：不断执行以下两个步骤直到收敛 选取一对需要更新的变量 $\alpha_i$ 和 $\alpha_j$ 固定 $\alpha_i$ 和 $\alpha_j$ 以外的参数，求解对偶问题更新后的 $\alpha_i$ 和 $\alpha_j$ 只要选取的 $α_i$ 和 $α_j$ 中有一个不满足KKT条件， 目标函数就会在迭代后减小。KKT条件违背的程度越大，变量更新后可能导致的目标函数值减幅越大。 使选取的两变量所对应样本之间的间隔最大（两个变量有很大的差别，对它们进行更新会带给目标函数值更大的变化）。 核函数对于原始样本空间线性不可分的情况，我们做如下处理：将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。已经证明：如果原始空间是有限维，那么一定存在一个高维特征空间使样本线性可分。 令 $\phi(x)$ 表示将 $x$ 映射后的特征向量，于是模型可表示为 $f(x)=\omega^T \phi(x) + b$，我们有： \begin{split} \min_{\omega,b} \ &\frac{1}{2}\vert \vert \omega \vert \vert^2\\ s.t. \ & y_i (\omega^T \phi(x_i) + b) \geq 1, \quad i=1,\cdots,m \end{split}对偶问题为 \begin{aligned} \max_{\alpha} &\sum\limits_{i=1}^{m} \alpha_i - \frac{1}{2} \sum\limits_{i=1}^{m} \sum\limits_{j=1}^{m} \alpha_i \alpha_j y_i y_j \phi(x_i)^T \phi(x_j)\\ s.t. & \sum\limits_{i=1}^{m} \alpha_i y_i = 0 \\ & \alpha_i \geq 0, i=1,\cdots,m \end{aligned}由于特征空间维数可能很高，直接计算 $\phi(x_i )^T \phi(x_j)$ 通常是困难的。设想函数 k(x_i,x_j) = = \phi(x_i)^T \phi(x_j)即 $x_i$ 与 $x_j$ 在特征空间的内积等于它们在原始样本空间中通过核函数计算的结果。有了这样的核函数，则可将上述优化问题重写为 \begin{aligned} \max_{\mathbf{\alpha}} & \sum_{i=1}^{m}\alpha_{i} - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i} \alpha_{j} y_{i} y_{j} k(\mathbf{x}_{i},\mathbf{x}_{j}) \\ s.t. & \sum\limits_{i=1}^{m} \alpha_i y_i = 0 \\ & \alpha_i \geq 0, i=1,\cdots,m \end{aligned}求解后，可得 \begin{aligned} f(\mathbf{x}) & =\omega^{T} \phi(x) + b \\ & = \sum_{i=1}^{m}\alpha_{i} y_{i} \phi({x}_{i})^{T} \phi({x}_{j}) + b \\ & = \sum_{i=1}^{m}\alpha_{i} y_{i} k({x}_{i},{x}_{j}) + b \end{aligned}其中，$k(.,.)$ 就是核函数。 核函数选择成为支持向量机的最大变数，若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。 常用核函数： 线性核 $k(x_i,x_j) = x_i^T x_j$ 多项式核 $k(x_i,x_j) = (x_i^T x_j)^d$ 高斯核(RBF核) $k(x_i,x_j) = exp(-\frac{\vert \vert x_i - x_j \vert \vert ^2}{2\delta^2})$ 拉普拉斯核 $k(x_i,x_j) = exp(-\frac{\vert \vert x_i - x_j \vert \vert}{\delta})$ Sigmod核 $k(x_i,x_j) = tanh(\beta x_i^T x_j + \theta)$ 核函数选择一般用线性核和高斯核，也就是Linear核与RBF核。需要注意的是需要对数据归一化处理。RBF kernel可以处理非线性的情况，linear kernel可以认为是RBF kernel的特殊情况。然后一般情况下RBF效果是不会差于Linear，但是时间上RBF会耗费更多。 下面是吴恩达的见解： 如果Feature的数量很大，往往线性可分，这时候选用LR或者是Linear Kernel的SVM； 如果Feature的数量比较小，而样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM； 如果不满足上述两点，即Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel。 软间隔与正则化现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分，即便线性可分，也很难判定这个结果不是由于过拟合造成的。缓解这个问题的一个方法是允许支持向量机在一些样本上出错，引入“软间隔”概念。允许某些样本不满足约束$y_i (\omega^T x_i + b) \geq 1$，SVM对训练集里面的每个样本$(x_i,y_i)$引入了一个松弛变量$\xi_i \geq 0$,使函数间隔加上松弛变量大于等于1，也就是说： y_i (\omega^T x_i + b) \geq 1 - \xi_i当然，松弛变量不能白加，这是有成本的，每一个松弛变量$\xi_i$, 对应了一个代价$\xi_i$，软间隔支持向量机： \begin{aligned} \min_{\omega,b} \ &\frac{1}{2} \vert \vert \omega \vert \vert^2 + C\sum\limits_{i=1}^{m} \xi_i\\ s.t. \ & y_i (\omega^T x_i + b) \geq 1 - \xi_i \\ & \xi_i \geq 0,i=1,\cdots,m \end{aligned}与硬间隔支持向量机相似，软间隔支持向量机也是一个二次规划问题，可以通过拉格朗日乘子法得到拉格朗日函数： L(\omega,b,\alpha) = \frac{1}{2} \vert \vert \omega \vert \vert^2 + C \sum\limits_{i=1}^{m} \xi_i + \sum\limits_{i=1}^{m} \alpha_i (1 - \xi_i - y_i (\omega^T x_i + b)) - \sum\limits_{i=1}^{m} \mu_i \xi_i令偏导为零可以得到 \begin{aligned} & \omega = \sum\limits_{i=1}^{m} \alpha_i y_i x_i \\ & 0 = \sum\limits_{i=1}^{m} \alpha_i y_i \\ & C = \alpha_i + \mu_i \\ \end{aligned}对偶问题为 \begin{aligned} \max_{\alpha} &\sum\limits_{i=1}^{m} \alpha_i - \frac{1}{2} \sum\limits_{i=1}^{m} \sum\limits_{j=1}^{m} \alpha_i \alpha_j y_i y_j x_i^T x_j\\ s.t. & \sum\limits_{i=1}^{m} \alpha_i y_i = 0 \\ & 0 \leq \alpha_i \leq C, i=1,\cdots,m \end{aligned}上述过程需要满足KKT(Karush-Kuhn-Tucker)条件，即 \left\{ \begin{aligned} & \alpha_i \geq 0 \\ & \mu_i \geq 0 \\ & y_if(x_i) - 1 + \xi_i \geq 0 \\ & \alpha_i (y_if(x_i) - 1 + \xi_i) = 0 \\ & \xi_i \geq 0 \\ & \mu_i \xi_i = 0 \end{aligned} \right.实际上支持向量机和对率回归的优化目标相近，通常情况下他们的性能相当。对率回归的优势主要对于其输出具有自然的概率意义，即在给出预测标记的同时也给了概率，而支持向量机的输出不具有概率意义，欲得到概率需要进行特殊处理；此外，对率回归能够直接用于多分类任务，支持向量机为此需要进行推广。另一方面，可以看出hinge损失函数有一块平摊的零区域，这使得支持向量机的解具有稀疏性，而对率损失是光滑的而单调递减函数，不能导出类似支持向量的概念。因此对率回归的解依赖于更多的训练样本，其预测开销大。 参考文献[1]. 《机器学习》 周志华[2]. 《统计学习方法》 李航[3]. 支持向量机[4]. 机器学习面试之有必要手推SVM吗]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络(CNN)]]></title>
    <url>%2F2018%2F06%2F06%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[卷积神经网络的各层中的神经元是3维排列的：宽度、高度和深度（这里的深度指的是激活数据体的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数）。层中的神经元将只与前一层中的一小块区域连接，而不是采取全连接方式。 构建卷积网络的各种层一个简单的卷积神经网络是由各种层按照顺序排列组成。卷积神经网络主要由三种类型的层构成：卷积层，汇聚（Pooling）层和全连接层（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。 卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数（神经元的突触权值和偏差）。而ReLU层和汇聚层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。 卷积层卷积层是构建卷积神经网络的核心层，它产生了网络中大部分的计算量。 概述 卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。 在每个卷积层上，我们会有一整个集合的滤波器（比如12个），每个都会生成一个不同的二维激活图。将这些激活映射在深度方向上层叠起来就生成了输出数据。 以大脑做比喻 如果你喜欢用大脑和生物神经元来做比喻，那么输出的3D数据中的每个数据项可以被看做是神经元的一个输出，而该神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）。 局部连接 我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。 空间维度（宽和高）：连接在空间（宽高）上是局部的。 在深度方向上：连接的大小总是和输入量的深度一致。 例：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。 超参数 3个超参数控制着输出数据体的尺寸：深度（depth），步长（stride）和零填充（zero-padding）。下面是对它们的讨论： 输出数据体的深度：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为深度列（depth column），也有人使用纤维（fibre）来称呼它们。 滤波器步长：在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。 零填充：有时候将输入数据体用0在边缘处进行填充是很方便的。这个零填充（zero-padding）的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。 输出数据体在空间上的尺寸可以通过输入数据体尺寸（W），卷积层中神经元的感受野尺寸（F），步长（S）和零填充的数量（P）的函数来计算。（这里假设输入数组的空间形状是正方形，即高度和宽度相等）输出数据体的空间尺寸为(W-F +2P)/S+1。比如输入是7x7，滤波器是3x3，步长为1，填充为0，那么就能得到一个5x5的输出。如果步长为2，输出就是3x3。 一般说来，当步长$S=1$时，零填充的值是$P=(F-1)/2$，这样就能保证输入和输出数据体有相同的空间尺寸。 参数共享 在卷积层中使用参数共享是用来控制参数的数量。 作一个合理的假设：如果一个特征在计算某个空间位置(x,y)的时候有用，那么它在计算另一个不同位置(x2,y2)的时候也有用。基于这个假设，可以显著地减少参数数量。换言之，就是将深度维度上一个单独的2维切片看做深度切片（depth slice），比如一个数据体尺寸为[55x55x96]的就有96个深度切片，每个尺寸为[55x55]。在每个深度切片上的神经元都使用同样的权重和偏差。在这样的参数共享下，例子中的第一个卷积层就只有96个不同的权重集了，一个权重集对应一个深度切片，如果F=11，则共有96x11x11x3=34848个不同的权重，或34944个参数（+96个偏差）。在每个深度切片中的55x55个权重使用的都是同样的参数。在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集。 注意，如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的卷积（这就是“卷积层”名字由来）。这也是为什么总是将这些权重集合称为滤波器（filter）（或卷积核（kernel）），因为它们和输入进行了卷积。 注意有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征。一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心。你可能期望不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为局部连接层（Locally-Connected Layer）。 1x1卷积 一些论文中使用了1x1的卷积，这个方法最早是在论文Network in Network中出现。人们刚开始看见这个1x1卷积的时候比较困惑，尤其是那些具有信号处理专业背景的人。因为信号是2维的，所以1x1卷积就没有意义。但是，在卷积神经网络中不是这样，因为这里是对3个维度进行操作，滤波器和输入数据体的深度是一样的。比如，如果输入是[32x32x3]，那么1x1卷积就是在高效地进行3维点积（因为输入深度是3个通道）。 小结 输入数据体的尺寸为 $W_1\times H_1\times D_1$。 四个超参数：滤波器的数量$K$，滤波器的空间尺寸$F$，步长$S$，零填充数量$P$。 输出数据体的尺寸为$W_2\times H_2\times D_2$ ，其中：$W_2=(W_1-F+2P)/S+1$，宽度和高度的计算方法相同，$D_2=K$ 由于参数共享，每个滤波器包含$F\cdot F\cdot D_1$个权重，卷积层一共有$F\cdot F\cdot D_1\cdot K$个权重和$K$个偏置。 对这些超参数，常见的设置是$F=3，S=1，P=1$。同时设置这些超参数也有一些约定俗成的惯例和经验。 汇聚层通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。汇聚层的一些公式： 输入数据体尺寸$W_1\cdot H_1\cdot D_1$ 有两个超参数：空间大小$F$，步长$S$ 输出数据体尺寸$W_2\cdot H_2\cdot D_2$，其中 W_2=(W_1-F)/S+1 \\ H_2=(H_1-F)/S+1 \\ D_2=D_1 \\ 因为对输入进行的是固定函数计算，所以没有引入参数 在汇聚层中很少使用零填充 在实践中，最大汇聚层通常只有两种形式：一种是$F=3,S=2$，也叫重叠汇聚（overlapping pooling），另一个更常用的是$F=2,S=2$。对更大感受野进行汇聚需要的汇聚尺寸也更大，而且往往对网络有破坏性。 普通汇聚（General Pooling）：除了最大汇聚，汇聚单元还可以使用其他的函数，比如平均汇聚（average pooling）或L-2范式汇聚（L2-norm pooling）。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。 反向传播 回顾反向传播的内容，其中$max(x,y)$函数的反向传播可以简单理解为将梯度只沿最大的数回传。因此，在向前传播经过汇聚层的时候，通常会把池中最大元素的索引记录下来（有时这个也叫作道岔（switches）），这样在反向传播的时候梯度的路由就很高效。 不使用汇聚层 很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层。 归一化层在卷积神经网络的结构中，提出了很多不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制。但是这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极其有限的。对于不同类型的归一化层，可以看看Alex Krizhevsky的关于cuda-convnet library API的讨论。 全连接层在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。 卷积神经网络的结构卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。 层的排列规律卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下： INPUT -&gt; [[CONV -&gt; RELU]N -&gt; POOL?]M -&gt; [FC -&gt; RELU]*K -&gt; FC 其中*指的是重复次数，POOL?指的是一个可选的汇聚层。其中N &gt;=0,通常N&lt;=3,M&gt;=0,K&gt;=0,通常K&lt;3。例如，下面是一些常见的网络结构规律： INPUT -&gt; FC,实现一个线性分类器，此处N = M = K = 0。 INPUT -&gt; CONV -&gt; RELU -&gt; FC INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; FC。此处在每个汇聚层之间有一个卷积层。 INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]*3 -&gt; [FC -&gt; RELU]*2 -&gt; FC。此处每个汇聚层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的汇聚操作前，多重的卷积层可以从输入数据中学习到更多的复杂特征。 几个小滤波器卷积层的组合比一个大滤波器卷积层好：假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。第二个卷积层上的神经元对第一个卷积层有一个3x3的视野，也就是对输入数据体有5x5的视野。同样，在第三个卷积层上的神经元对第二个卷积层有3x3的视野，也就是对输入数据体有7x7的视野。假设不采用这3个3x3的卷积层，二是使用一个单独的有7x7的感受野的卷积层，那么所有神经元的感受野也是7x7，但是就有一些缺点。首先，多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征。其次，假设所有的数据有C个通道，那么单独的7x7卷积层将会包含C\times (7\times 7\times C)=49C^2个参数，而3个3x3的卷积层的组合仅有3\times (C\times (3\times 3\times C))=27C^2个参数。直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层。前者可以表达出输入数据中更多个强力特征，使用的参数也更少。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存。 最新进展：传统的将层按照线性进行排列的方法已经受到了挑战，挑战来自谷歌的Inception结构和微软亚洲研究院的残差网络（Residual Net）结构。这两个网络（下文案例学习小节中有细节）的特征更加复杂，连接结构也不同。 参考文献[1]. Convolutional Neural Networks (CNNs/ConvNets)[2]. CS231n课程笔记翻译：卷积神经网络笔记]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主成分分析（PCA）]]></title>
    <url>%2F2018%2F06%2F06%2FPrincipal_Component_Analysize(PCA)%2F</url>
    <content type="text"><![CDATA[主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。一般我们提到降维最容易想到的算法就是PCA，下面我们就对PCA的原理做一个总结。 PCA的思想PCA就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是 $n$ 维的，共有 $m$ 个数据$(x^{(1)}，x^{(2)}，…，x^{(m)})$。我们希望将这 $m$ 个数据的维度从 $n$ 维降到 $k$ 维，希望这 $m$ 个 $k$ 维的数据集尽可能的代表原始数据集。我们知道数据从 $n$ 维降到 $k$ 维肯定会有损失，但是我们希望损失尽可能的小。那么如何让这 $k$ 维的数据尽可能表示原来的数据呢？ 先看看最简单的情况，也就是 $n=2，k=1$，也就是将数据从二维降维到一维。数据如下图。我们希望找到某一个维度方向，它可以代表这两个维度的数据。图中列了两个向量方向，红色表示的向量和绿色表示的向量。那么哪个向量可以更好的代表原始数据集呢？ 可以很直观地看到，数据点和直线的距离就在降维的过程中丢失掉了。显然，绿线丢失的数据要比红线多。所以，可以判断，使用红线相比绿线会更好。我们也注意到，投影到红线上的蓝点，离散的程度大于投影到绿线上的蓝点，这也从另一个角度说明投影到红线丢失的信息相对更少。 PCA的优点：不需要设定参数，结果仅与数据有关。可以方便的应用到各个场合。 应用：用于降维，可以减少过拟合。从神经科学到计算机图形学都有它的用武之地。被誉为应用线形代数最有价值的结果之一。事实上，第一主分量集中了最大的信息量，常常占80％以上。第二、三主分量的信息量依次很快递减，到了第N分量，信息几乎为零。选择较少的主成分来表示数据不但可以用作特征降维，还可以用来消除数据中的噪声。在信号处理中认为信号具有较大的方差，噪声有较小的方差。在很多情况下，在特征值谱中排列在后面的主成分(次成分)往往反映了数据中的随机噪声。因此，如果把特征值很小的成分置为0，再反变换回原空间，则实现了对原数据的降噪。 有两种方式解释PCA，最大方差理论和最小平方误差理论。 最大方差理论我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。最好的 $k$ 维特征是将 $n$ 维样本点转换为 $k$ 维后，每一维上的样本方差都很大。降维的过程即高维向低维映射的过程(向一些轴投影)。 投影的概念向量$\mathbb x^{(i)}=(x_1^{(i)}，x_2^{(i)}，\dots，x_n^{(i)})^T$在单位向量$u=(u_1，u_2，\dots，u_n)^T$(代表某条直线的方向向量)上的投影的长度为:$\langle x^{(i)}，u \rangle ={x^{(i)}}^Tu=u^Tx^{(i)}$。 由于数据预处理后每一维度均值为0，投影后仍为0，因此方差为: {1\over m}\sum_{i=1}^m({x^{(i)}}^Tu)^2 ={1\over m}\sum_{i=1}^m u^Tx^{(i)}{x^{(i)}}^Tu =u^T({1\over m}\sum_{i=1}^mx^{(i)}{x^{(i)}}^T)u最后一个等号后边的二次项的中间部分即为样本特征的协方差矩阵. 用$A$来表示该协方差矩阵，用 $\lambda$ 表示方差(目标函数)，则上式写作: \lambda=u^TAu由于 $u$ 是单位向量，即 $u^Tu=1$，上式两边都左乘 $u$ 得， $u\lambda=\lambda u=uu^TAu=Au$，即 Au=\lambda u可以看出，$\lambda$ 就是 $A$ 的特征值，$u$ 是特征向量。最佳的投影直线是特征值 $λ$ 最大时对应的特征向量，其次是 $λ$ 第二大对应的特征向量，依次类推。因此，我们只需要对协方差矩阵进行特征值分解，得到的前 $k$ 大特征值对应的特征向量就是最佳的 $k$ 维新特征，而且这 $k$ 维新特征是正交的。得到前 $k$ 个 $u$ 以后，样例 $x$ 通过以下变换可以得到新的样本: y^{(i)}= \begin{bmatrix} u_1^Tx^{(i)} \\ u_2^Tx^{(i)} \\ \vdots \\ u_k^Tx^{(i)} \end{bmatrix} \in \mathbb R^kPCA 将 $n$ 个特征降维到 $k$ 个，可以用来进行数据压缩，如果 100 维的向量最后可以用10 维来表示，那么压缩率为 90%。同样图像处理领域的 KL 变换使用 PCA 做图像压缩。但PCA 要保证降维后，还要保证数据的特性损失最小。 PCA步骤 特征中心化。即每一维的数据都减去该维的均值，得到新的矩阵B。这里的“维”指的就是一个特征（或属性），变换之后每一维的均值都变成了0。 计算B的协方差矩阵C。 计算协方差矩阵C的特征值和特征向量。 选取大的特征值对应的特征向量，得到新的数据集。 参考文献[1]. 《机器学习》 周志华[2]. 特征降维之PCA[3]. 主成分分析PCA]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>主成分分析</tag>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协方差矩阵]]></title>
    <url>%2F2018%2F06%2F06%2F%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。简单来讲，协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）。而协方差矩阵，只是将所有变量的协方差关系用矩阵的形式表现出来而已。通过矩阵这一工具，可以更方便地进行数学运算。 协方差通常，在提到协方差的时候，需要对其进一步区分。 随机变量的协方差。跟数学期望、方差一样，是分布的一个总体参数。 样本的协方差。是样本集的一个统计量，可作为联合分布总体参数的一个估计。在实际中计算的通常是样本的协方差。 随机变量的协方差在概率论和统计中，协方差是对两个随机变量联合分布线性相关程度的一种度量。两个随机变量越线性相关，协方差越大，完全线性无关，协方差为零。定义如下: \operatorname {cov} (X,Y)=\operatorname {E} {\ {\big [}(X-\operatorname {E} [X])(Y-\operatorname {E} [Y]){\big ]}}当$X，Y$是同一个随机变量时，$X$与其自身的协方差就是$X$的方差，可以说方差是协方差的一个特例。 \operatorname{var}(X) = \operatorname{cov}(X,X) = \operatorname{E}\big[(X-\operatorname{E}[X])^2]由于随机变量的取值范围不同，两个协方差不具备可比性。如 $X，Y，Z$ 分别是三个随机变量，想要比较 $X$ 与 $Y$ 的线性相关程度强，还是 $X$ 与 $Z$ 的线性相关程度强，通过 $\operatorname{cov}(X,Y)$ 与 $\operatorname{cov}(X,Z)$ 无法直接比较。定义相关系数 $\eta$ 为 \eta ={\dfrac {\operatorname {cov} (X,Y)}{\sqrt {\operatorname {var} (X)\cdot \operatorname {var} (Y)}}}\通过 $X$ 的方差 $\operatorname{var}(X)$ 与 $Y$ 的方差 $\operatorname{var}(Y)$ 对协方差 $\operatorname{cov}(X,Y)$ 归一化，得到相关系数 $\eta，\eta$ 的取值范围是 $[-1,1]$ 。1表示完全线性相关，-1表示完全线性负相关，0表示线性无关。线性无关并不代表完全无关，更不代表相互独立。 样本的协方差在实际中，通常我们手头会有一些样本，样本有多个属性，每个样本可以看成一个多维随机变量的样本点，我们需要分析两个维度之间的线性关系。协方差及相关系数是度量随机变量间线性关系的参数，由于不知道具体的分布，只能通过样本来进行估计。 设样本对应的多维随机变量为 $\textbf X=[X_1, X_2, X_3, …, X_n]^T$，样本集合为 $\{\textbf x_{\cdot j}=[x_{1j},x_{2j},…,x_{nj}]^T|1\leqslant j\leqslant m\}$ ，$m$ 为样本数量。与样本方差的计算相似，$a$ 和 $b$ 两个维度样本的协方差公式为，其中 $1\leqslant a\leqslant n，1\leqslant b\leqslant n$，$n$ 为样本维度 \operatorname {cov}(a,b)=\dfrac {\sum_{j=1}^m{(x_{aj}-\bar x_a)(x_{bj}-\bar x_b)}}{m-1}这里分母为 $m-1$ 是因为随机变量的数学期望未知，以样本均值代替，自由度减一。 协方差矩阵这里主要讨论样本的协方差矩阵。 两个特征变量的协方差矩阵协方差本身就能够处理二维问题，两个变量的协方差矩阵并没有实际意义，不过为了方便后面多维的推广，我们还是从二维开始。 用一个例子来解释会更加形象。 假设我们有 4 个样本，每个样本都有两个变量，也就是两个特征，它们表示如下： x_1=(1,2)，x_2=(3,6)，x_3=(4,2)，x_4=(5,2)用一个矩阵表示为： Z=\begin{bmatrix} 1 & 2 \\ 3 & 6 \\ 4 & 2 \\ 5 & 2 \end{bmatrix}现在，我们用两个变量空间 $X，Y$ 来表示这两个特征： X=\begin{bmatrix} 1 \\ 3 \\ 4 \\ 5 \end{bmatrix}, \ \ \ Y=\begin{bmatrix} 2 \\ 6 \\ 2 \\ 2 \end{bmatrix}由于协方差反应的是两个变量之间的相关性，因此，协方差矩阵表示的是所有变量之间两两相关的关系，具体来讲，一个包含两个特征的矩阵，其协方差矩阵应该有 $2 \times 2$ 大小： Cov(Z)=\begin{bmatrix} Cov(X,X) & Cov(X,Y) \\ Cov(Y,X) & Cov(Y,Y) \end{bmatrix}接下来，就来逐一计算 $Cov(Z)$ 的值。首先，我们需要先计算出 $X，Y$ 两个特征空间的平均值：$\overline x=3.25，\overline y=3$。然后，根据协方差的数学定义，计算协方差矩阵的每个元素： \begin{align*} & Cov(X,X)=\frac{(1-3.25)^2+(3-3.25)^2+(4-3.25)^2+(5-3.25)^2}{4-1}=2.9167 \\ & Cov(X,Y)=\frac{(1-3.25)(2-3)+(3-3.25)(6-3)+(4-3.25)(2-3)+(5-3.25)(2-3)}{4-1}=-0.3333 \\ & Cov(Y,X)=\frac{(2-3)(1-3.25)+(6-3)(3-3.25)+(2-3)(4-3.25)+(2-3)(5-3.25)}{4-1}=-0.3333 \\ & Cov(Y,Y)=\frac{(2-3)^2+(6-3)^2+(2-3)^2+(2-3)^2}{4-1}=4 \end{align*}所以协方差矩阵 Cov(Z)=\begin{bmatrix} 2.9167 & -0.3333 \\ -0.3333 & 4.000 \end{bmatrix}好了，虽然这只是一个二维特征的例子，但我们已经可以从中总结出协方差矩阵 $\Sigma$ 的「计算套路」： \Sigma_{ij}=\frac{(样本矩阵第i列-第i列均值)^T(样本矩阵第j列-第j列均值)}{样本数-1}这里所说的样本矩阵可以参考上面例子中的 $Z$。 多个特征变量的协方差矩阵协方差矩阵是个对称矩阵，对角线上的元素是各维度上随机变量的方差。 假设我们有三个样本： x_1=(1,2,3,4)^T， x_2=(3,4,1,2)^T， x_3=(2,3,1,4)^T。同理我们将它们表示成样本矩阵： Z=\begin{bmatrix} 1 & 2 & 3 & 4 \\ 3 & 4 & 1 & 2 \\ 2 & 3 & 1 & 4 \end{bmatrix}按照上面给出的计算套路，我们需要先计算出矩阵每一列的均值，从左到右分别为：2、3、1.67、3.33。然后按照上面讲到的公式，计算矩阵每个元素的值，四个变量的协方差矩阵，大小为 $4 \times 4$ ： \Sigma_{11}=\frac{(第1列-第1列的均值)^T*(第1列-第1列的均值)}{样本数-1} \\=\frac{(-1,1,0)^T*(-1,1,0)}{2}=1（后面的依此类推……） 独立变量的协方差以上的讨论都是针对一般情况进行计算的，毕竟变量互相独立的情况较少。 如果两个变量 $X, Y$ 独立，那么它们的协方差 $Cov(X,Y) = 0$。简要证明如下（简单起见，假设变量是离散的）： 由于 $X, Y$ 独立，所以它们的概率密度函数满足：$p(x,y)=p_x(x)p_y(y)$。 求出期望： \begin{eqnarray} E(XY) & = &\sum_x \sum_y {x*y*p(x,y)} \notag \\ & = &\sum_x \sum_y x*y*p_x(x)*p_y(y) \notag \\ & = &\sum_x{x*p_x(x)}\sum_y{y*p_y(y)} \notag \\ & = &E(X)E(Y) \notag \end{eqnarray}利用协方差的另一个公式：$Cov(X,Y)=E(X,Y)-E(X)E(Y)$，可以推出，当 $X, Y$ 相互独立时，$Cov(X, Y)=0$。 这时，协方差矩阵就变成一个对角矩阵了： Cov(Z)=\begin{bmatrix} Cov(X,X) & 0\\ 0 & Cov(Y,Y) \end{bmatrix}协方差矩阵的作用作为一种数学工具，协方差矩阵经常被用来计算特征之间的某种联系。在机器学习的论文中，协方差矩阵的出现概率还是很高的，用于降维的主成分分析法（PCA）就用到了协方差矩阵。另外，由于协方差矩阵是一个对称矩阵，因此它包含了很多很有用的性质，这也导致它受青睐的程度较高。 参考文献[1]. 协方差与协方差矩阵[2]. 协方差矩阵]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>协方差矩阵</tag>
        <tag>协方差</tag>
        <tag>方差</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy基础]]></title>
    <url>%2F2018%2F06%2F05%2FNumpy%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[本文主要介绍Numpy的结构化数组、时间表示、文件IO。 结构化数组在C语言中我们可以通过struct关键字定义结构体，结构体中的字段占据连续的内存空间，每个结构体占用的内存大小都相同。和C语言一样，在NumPy中也很容易对这种结构数据进行操作。只要NumPy中的结构定义和C语言中的定义相同，NumPy就可以很方便地读取C语言的结构数组的二进制数据，转换为NumPy的结构数组。 假设我们需要定义一个结构数组，它的每个元素都有name, age和weight字段。在NumPy中可以如下定义： 123456789import numpy as np# 第一种定义方式person = np.dtype(&#123; 'names': ['name', 'age', 'weight'], 'formats': ['S32', 'i', 'f']&#125;)a = np.array([("zhangsan", 32, 75.5), ("wangwu", 24, 66)], dtype=person)print(a.dtype) [(&#39;name&#39;, &#39;S32&#39;), (&#39;age&#39;, &#39;&lt;i4&#39;), (&#39;weight&#39;, &#39;&lt;f4&#39;)] 我们先创建一个dtype对象person，通过其字典参数描述结构类型的各个字段。字典有两个关键字：names，formats。每个关键字对应的值都是一个列表。names定义结构中的每个字段名，而formats则定义每个字段的类型： S32 : 32个字节的字符串类型，由于结构中的每个元素的大小必须固定，因此需要指定字符串的长度 i : 32bit的整数类型，相当于np.int32 f : 32bit的单精度浮点数类型，相当于np.float32 类型描述前面为我们添加了 |, &lt; 等字符，这些字符用来描述字段值的字节顺序： | : 忽视字节顺序 &lt; : 低位字节在前 &gt; : 高位字节在前 然后我们调用array函数创建数组，通过关键字参数 dtype=person， 指定所创建的数组的元素类型为结构person。 另外一种描述结构类型的方法： 一个包含多个组元的列表，其中形如 (字段名, 类型描述) 的组元描述了结构中的每个字段，示例如下： 1234# 第二种定义方式person2 = np.dtype([('name', 'S32'), ('age', np.int32), ('weight', np.float32)])b = np.array([("zhangsan", 32, 75.5), ("wangwu", 24, 66)], dtype=person2)print(b.dtype) [(&#39;name&#39;, &#39;S32&#39;), (&#39;age&#39;, &#39;&lt;i4&#39;), (&#39;weight&#39;, &#39;&lt;f4&#39;)] 结构体数组的存取方式和一般数组相同，通过下标能够取得其中的元素： 123456print(a[0])print(a[0].dtype)c = a[1]c['name'] = 'LiMing'print(a[1]['name']) (b&#39;zhangsan&#39;, 32, 75.5) [(&#39;name&#39;, &#39;S32&#39;), (&#39;age&#39;, &#39;&lt;i4&#39;), (&#39;weight&#39;, &#39;&lt;f4&#39;)] b&#39;LiMing&#39; 我们不但可以获得结构元素的某个字段，还可以直接获得结构数组的字段，它返回的是原始数组的视图，因此可以通过修改b[0]改变a[0][&quot;age&quot;]： 12d = a[:]['age']d array([32, 24], dtype=int32) 12d[0] = 40a[0]['age'] 40 通过调用a.tostring或者a.tofile方法，可以直接输出数组a的二进制形式： 1a.tofile('test.bin') C语言的结构体为了内存寻址方便，会自动的添加一些填充用的字节，这叫做内存对齐。例如如果把下面的name[32]改为name[30]的话，由于内存对齐问题，在name和age中间会填补两个字节，最终的结构体大小不会改变。因此如果numpy中的所配置的内存大小不符合C语言的对齐规范的话，将会出现数据错位。为了解决这个问题，在创建dtype对象时，可以传递参数align=True，这样numpy的结构数组的内存对齐和C语言的结构体就一致了。 123456struct person&#123; char name[32]; int age; float weight;&#125;; 1person3 = np.dtype([('name', 'S32'), ('age', np.int32), ('weight', np.float32)], align=True) 参考文献：[1]. 用Python做科学计算 时间和日期基本时间表示123np.datetime64('2018-06-01')np.datetime64('2018-06')np.datetime64('2018-06-01 23:02:01') numpy.datetime64(&#39;2018-06-01T23:02:01&#39;) 12# 第二个参数可以是 Y M D h m s 年月日时分秒np.datetime64('2018-06', 'h') numpy.datetime64(&#39;2018-06-01T00&#39;,&#39;h&#39;) 1np.array(['2018-06-01', '2018-06-02'], dtype='datetime64') array([&#39;2018-06-01&#39;, &#39;2018-06-02&#39;], dtype=&#39;datetime64[D]&#39;) all the dates for month: 1np.arange('2016-06', '2016-07', dtype='datetime64[D]') array([&#39;2016-06-01&#39;, &#39;2016-06-02&#39;, &#39;2016-06-03&#39;, &#39;2016-06-04&#39;, &#39;2016-06-05&#39;, &#39;2016-06-06&#39;, &#39;2016-06-07&#39;, &#39;2016-06-08&#39;, &#39;2016-06-09&#39;, &#39;2016-06-10&#39;, &#39;2016-06-11&#39;, &#39;2016-06-12&#39;, &#39;2016-06-13&#39;, &#39;2016-06-14&#39;, &#39;2016-06-15&#39;, &#39;2016-06-16&#39;, &#39;2016-06-17&#39;, &#39;2016-06-18&#39;, &#39;2016-06-19&#39;, &#39;2016-06-20&#39;, &#39;2016-06-21&#39;, &#39;2016-06-22&#39;, &#39;2016-06-23&#39;, &#39;2016-06-24&#39;, &#39;2016-06-25&#39;, &#39;2016-06-26&#39;, &#39;2016-06-27&#39;, &#39;2016-06-28&#39;, &#39;2016-06-29&#39;, &#39;2016-06-30&#39;], dtype=&#39;datetime64[D]&#39;) 1np.arange('2016-06-20', '2016-07', dtype='datetime64[D]') array([&#39;2016-06-20&#39;, &#39;2016-06-21&#39;, &#39;2016-06-22&#39;, &#39;2016-06-23&#39;, &#39;2016-06-24&#39;, &#39;2016-06-25&#39;, &#39;2016-06-26&#39;, &#39;2016-06-27&#39;, &#39;2016-06-28&#39;, &#39;2016-06-29&#39;, &#39;2016-06-30&#39;], dtype=&#39;datetime64[D]&#39;) 时间间隔1np.datetime64('2018-06-01') - np.datetime64('2017-06-01') numpy.timedelta64(365,&#39;D&#39;) 1np.datetime64('2018') + np.timedelta64(20, 'D') numpy.datetime64(&#39;2018-01-21&#39;) 1np.datetime64('2018') + np.timedelta64(12, 'h') numpy.datetime64(&#39;2018-01-01T12&#39;,&#39;h&#39;) 1np.timedelta64(1, 'W') / np.timedelta64(1, 'D') 7.0 1np.timedelta64(2, 'W') / np.timedelta64(1, 'D') 14.0 12a = np.timedelta64(1, 'Y')np.timedelta64(a, 'M') numpy.timedelta64(12,&#39;M&#39;) 12# 一年366还是365天不确定，所以会报错np.timedelta64(a, 'D') --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-109-c0eb8eb6d063&gt; in &lt;module&gt;() 1 # 一年366还是365天不确定，所以会报错 ----&gt; 2 np.timedelta64(a, &#39;D&#39;) TypeError: Cannot cast NumPy timedelta64 scalar from metadata [Y] to [D] according to the rule &#39;same_kind&#39; 12a = np.timedelta64(1,'W')np.timedelta64(a, 'D') numpy.timedelta64(7,&#39;D&#39;) 文件IOndarray对象可以保存到磁盘文件并从磁盘文件加载。 可用的 IO 功能有： load()和save()函数处理numpy二进制文件(带npy扩展名) savez()将多个ndarray保存到一个文件中 loadtxt()和savetxt()函数处理正常的文本文件 Numpy为ndarray对象引入了一个简单的文件格式。 这个npy文件在磁盘文件中，存储重建ndarray所需的数据、图形、dtype和其他信息，以便正确获取数组，即使该文件在具有不同架构的另一台机器上。 1234567# 保存a = np.array([1, 2, 3, 4])np.save('outfile', a)# 读取b = np.load('outfile.npy')b array([1, 2, 3, 4]) 1234567a1 = np.random.random(10)a2 = np.random.random(8)a3 = np.random.random(3)np.savez('outfile.npz', a1, a2, a3_array=a3)b = np.load('outfile.npz')b['arr_0'] array([0.12914584, 0.59498359, 0.73354194, 0.34842121, 0.10339349, 0.03454324, 0.43330201, 0.55238437, 0.65248843, 0.82193438]) 1b['a3_array'] array([0.39751163, 0.40637335, 0.38091367]) 12345a = np.array([1, 2, 3, 4, 5])np.savetxt('out.txt', a)b = np.loadtxt('out.txt')b array([1., 2., 3., 4., 5.])]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记]]></title>
    <url>%2F2018%2F06%2F04%2FPython_Learning_Note%2F</url>
    <content type="text"><![CDATA[这是我平时写Python代码时总结的一些东西，内容比较基础。 数据结构list的一些用法append、extend和+ 的区别12345678910111213l1 = [1, 2, 3]l2 = [4, 5, 6]l1.append(l2)print('after append: ', l1)l1 = [1, 2, 3]l2 = [4, 5, 6]l1.extend(l2)print('after extend: ', l1)list1 = [1, 2, 3]list2 = ['a', 'b', 'c']list1 + list2 after append: [1, 2, 3, [4, 5, 6]] after extend: [1, 2, 3, 4, 5, 6] [1, 2, 3, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;] 用count获取list中某元素出现的次数123test = [1, 2, 3, 5, 5, 8, 1]print(test.count(1))print(test.count(9)) 2 0 初始化list，里面有10个812l1 = [8] * 10l1 [8, 8, 8, 8, 8, 8, 8, 8, 8, 8] 用index得到list中某元素首次出现的位置如果没有会抛出异常 12print(test.index(2))print(test.index(1)) 1 0 计算list中各元素出现次数1234test = [1, 2, 3, 2, 2, 3, 1, 4]for i in set(test): num = test.count(i) print("elem: %d, count: %d" % (i, num)) elem: 1, count: 2 elem: 2, count: 3 elem: 3, count: 2 elem: 4, count: 1 获得最大最小值12min(test)max(test) 8 使用defaultdict代替dict的好处使用defaultdict任何未定义的key都会默认返回一个默认的参数值, 而相同情况下dict()会返回KeyError. 1234from collections import defaultdictd1 = defaultdict(int)d1['a'] += 2d1['a'] 2 123d1 = defaultdict(lambda: 10)d1['b'] += 2d1['b'] 12 切片tuple和字符串也支持切片整个数组头尾颠倒12a = [1, 2, 3, 4]a[::-1] [4, 3, 2, 1] 限定最大长度切割列表时，即使start或end索引越界也不会有问题。利用这一特性，我们可以限定输入序列的最大长度 123a = list(range(8))first_ten_items = a[:10]first_ten_items [0, 1, 2, 3, 4, 5, 6, 7] 在切割后的新列表上修改，不影响原列表123b = a[:]b[0] = 100a [0, 1, 2, 3, 4, 5, 6, 7] 扩张与收缩在赋值时对左侧列表使用切割操作，会把该列表中指定范围的对象替换为新值，此切片的长度无需与新值个数相等，列表会根据新值的个数相应的扩张或收缩 12a[2:5] = 'a', 'b', 'c', 'd'a [0, 1, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, 5, 6, 7] isinstance()判断对象类型判断是否是可迭代对象 12from collections import Iterableisinstance("abc", Iterable) True 判断是否是一个字符串 1isinstance(123, str) False 生成器 生成器表达式可以创建生成器，只需把列表生成式中的[]改为() generator是可迭代对象 如果一个函数中包含 yield 关键字，那这个函数就成了一个generator 生成器只能遍历一次 12g = (x * x for x in range(10))g &lt;generator object &lt;genexpr&gt; at 0x00000265400FAD00&gt; 12for n in g: print(n) 0 1 4 9 16 25 36 49 64 81 用zip函数同时遍历两个迭代器 zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回一个可迭代的zip对象。 同时也可以有两个以上的参数。当传入参数的长度不同时，zip能自动以最短序列长度为准进行截取，获得元组。 如果要显示出来，用list( )函数 如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用*号操作符，可以将元组解压为列表。 12345678910a = [1, 2, 3]b = [4, 5, 6]c = [4, 5, 6, 7, 8]zipped = zip(a,b)print(type(zipped))for i in zipped: print(i)# list显示出来list(zip(a,b)) &lt;class &#39;zip&#39;&gt; (1, 4) (2, 5) (3, 6) [(1, 4), (2, 5), (3, 6)] 123str1 = 'abc'str2 = 'def123'list(zip(str1, str2)) [(&#39;a&#39;, &#39;d&#39;), (&#39;b&#39;, &#39;e&#39;), (&#39;c&#39;, &#39;f&#39;)] 123# 三个参数l1,l2,l3 = (1,2,3),(4,5,6),(7,8,9)list(zip(l1,l2,l3)) [(1, 4, 7), (2, 5, 8), (3, 6, 9)] 1234# 解包zipped = zip(l1, l2, l3)zip_un = zip(*zipped)list(zip_un) [(1, 2, 3), (4, 5, 6), (7, 8, 9)] 使用zfill()填充字符串zfill(n)方法返回指定长度的字符串，原字符串右对齐，前面填充0。 12a = '4'print(a.zfill(3)) 004 函数函数的参数Python的函数定义强大而灵活，函数的参数包括以下几个类型： 位置参数 默认参数 可变参数 关键字参数 命名关键字参数 位置参数（略）默认参数默认参数必须指向不变对象，否则可能引起难以追踪的问题。详细解释见廖雪峰Python教程 可变参数可变参数就是传入的参数个数是可变的，可以是1个、2个到任意个，还可以是0个。这些可变参数在函数调用时自动组装为一个tuple。 123456def calc(*numbers): sum = 0 for n in numbers: sum = sum + n*n return sumcalc(1, 2, 3, 4) 30 1calc() 0 如果已经有一个list或者tuple，要调用一个可变参数可以这样做：在list或tuple前面加一个 * 号，把list或tuple的元素变成可变参数传进去： 12nums = [1, 2, 3]calc(*nums) 14 关键字参数关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。 123456def person(name, age, **kw): print(name, age) for key, value in kw.items(): print(key, value)person('Tom', 17, city='beijing', company='alibaba') Tom 17 city beijing company alibaba 命名关键字参数如果要限制关键字参数的名字，就可以用命名关键字参数，例如，只接收 city 和 job 作为关键字参数。这种方式定义的函数如下： 12def person_one(name, age, *, city, job): print(name, age, city, job) 和关键字参数 **kw 不同，命名关键字参数需要一个特殊分隔符 *，* 后面的参数被视为命名关键字参数。 命名关键字参数必须传入参数名，这和位置参数不同。如果没有传入参数名，调用将报错 1person_one('Jack', 24, city='bejing', job='engineer') Jack 24 bejing engineer 如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符 * 了： 12def person_two(name, age, *args, city, job): print(name, age, args, city, job) 常用内建函数map()map()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。 12345def f(x): return x*xr = map(f, [1, 2, 3, 4, 5])list(r) [1, 4, 9, 16, 25] filter()filter()也接收一个函数和一个序列。filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 可见用 filter() 这个高阶函数，关键在于正确实现一个“筛选”函数。 12345def is_odd(n): return n % 2 == 1r = filter(is_odd, [1, 2, 3, 4, 5, 6])list(r) [1, 3, 5] sorted()1sorted([36, 6, -10, 8, -7]) [-10, -7, 6, 8, 36] 1sorted([36, 6, -10, 8, -7], key = abs) [6, -7, 8, -10, 36] 1sorted([36, 6, -10, 8, -7], key = abs, reverse=True) [36, -10, 8, -7, 6] Python使用特殊规则来比较两个列表或元组，先比较下表为0的元素，如果相等，再比较下标为1的元素，以此类推 123number_pair = [(1, 6), (1, 3), (0, 4), (0, 2)]number_pair.sort()number_pair [(0, 2), (0, 4), (1, 3), (1, 6)] 装饰器装饰器是可调用的对象，其参数是另一个函数（被装饰的函数），返回值也是一个函数。Gamma 等人写的《设计模式：可复用面向对象软件的基础》一书是这样概述“装饰器”模式的： “动态地给一个对象添加一些额外的职责”。 装饰器的一个关键特性是，它们在被装饰的函数定义之后立即运行。这通常是在导入时（即Python加载模块时）。 装饰器通常在一个模块中定义，然后应用到其他模块的函数上。 大多数装饰器会在内部定义一个函数，然后将其返回（而不是返回原函数）。 返回原函数这种技术并非没有用处，很多 Python Web 框架使用这样的装饰器把函数添加到某种中央注册处。 123456789def deco(func): print("run deco()") def inner(): print('run inner()') return inner@decodef target(): print('run target()') run deco() 1target() run inner() 1234567891011121314151617181920212223import timedef clock(func): def clocked(*args): t0 = time.perf_counter() result = func(*args) elapsed = time.perf_counter() - t0 name = func.__name__ arg_str = ','.join(repr(arg) for arg in args) print('[%0.8fs] %s(%s) -&gt; %r' % (elapsed, name, arg_str, result)) return result return clocked@clockdef snooze(seconds): time.sleep(seconds) @clockdef factorial(n): return 1 if n &lt; 2 else n * factorial(n-1)snooze(.123)factorial(6) [0.12261603s] snooze(0.123) -&gt; None [0.00000049s] factorial(1) -&gt; 1 [0.00003877s] factorial(2) -&gt; 2 [0.00005998s] factorial(3) -&gt; 6 [0.00007753s] factorial(4) -&gt; 24 [0.00009777s] factorial(5) -&gt; 120 [0.00011532s] factorial(6) -&gt; 720 720 变量作用域 Python 不要求声明变量，但是假定在函数定义体中赋值的变量是局部变量。 如果在函数中赋值时想让解释器把变量当成全局变量，要使用global声明： 首先看一段代码： 12345678b = 6def f1(a): print(a) print(b) b = 9f1(3) 3 --------------------------------------------------------------------------- UnboundLocalError Traceback (most recent call last) &lt;ipython-input-45-882882f7cac7&gt; in &lt;module&gt;() 6 b = 9 7 ----&gt; 8 f1(3) &lt;ipython-input-45-882882f7cac7&gt; in f1(a) 3 def f1(a): 4 print(a) ----&gt; 5 print(b) 6 b = 9 7 UnboundLocalError: local variable &#39;b&#39; referenced before assignment 出现错误，一开始我很吃惊，我觉得会打印 6，因为有个全局变量b，而且是在print(b)之后为局部变量b赋值。 事实是， Python编译函数的定义体时，它判断b是局部变量，因为在函数中给它赋值了。后面调用f1(3)时， f1的定义体尝试获取局部变量b的值时，发现b没有绑定值，抛出错误。 123456789b = 6def f1(a): global b print(a) print(b) b = 9f1(3) 3 6 闭包闭包指延伸了作用域的函数，其中包含函数定义体中引用、但是不在定义体中定义的非全局变量。函数是不是匿名的没有关系，关键是它能访问定义体之外定义的非全局变量。它会保留定义函数时存在的自由变量的绑定，这样调用函数时，虽然定义作用域不可用了，但是仍能使用那些绑定。 用途：当闭包执行完后，仍然能够保持住当前的状态。比如说，如果你希望函数的每次执行结果，都是基于这个函数上次的运行结果。 nonlocal 声明的作用是把变量标记为自由变量，即使在函数中为变量赋予新值了，也会变成自由变量。如果为 nonlocal 声明的变量赋予新值，闭包中保存的绑定会更新。nonlocal清楚的表明：如果在闭包内给该变量赋值，那么修改的其实是闭包外的那个作用域中的变量，nonlocal不能延伸到模块级别。 123456789101112131415def make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averageravg = make_averager()avg(10)avg(11) 10.5 面向对象__dict__与dir__dict__是用来存储对象属性的一个字典，其键为属性名，值为属性的值。Python的实例有自己的_dict__，它对应的类也有自己的__dict__，（但有些特殊的对象没有__dict__属性，这里不做讨论） 实例的__dict__仅存储与该实例相关的实例属性，正是因为实例的__dict__属性，每个实例的实例属性才会互不影响。 类的__dict__存储所有实例共享的变量和函数(类属性，方法等)，类的__dict__并不包含其父类的属性。 dir()是Python提供的一个API函数，dir()函数会自动寻找一个对象的所有属性(包括从父类中继承的属性)。 1234567891011121314151617181920212223class A(object): class_var = 1 def __init__(self): self.name = 'xy' self.age = 2 @property def num(self): return self.age + 10 def fun(self): pass def static_f(): pass def class_f(cls): pass a = A()a.sex = 'man'b = A()print('A类属性: ', A.__dict__)print('a实例属性: ', a.__dict__)dir(b) A类属性: {&#39;__module__&#39;: &#39;__main__&#39;, &#39;class_var&#39;: 1, &#39;__init__&#39;: &lt;function A.__init__ at 0x000002653F09A730&gt;, &#39;num&#39;: &lt;property object at 0x00000265401D3368&gt;, &#39;fun&#39;: &lt;function A.fun at 0x00000265401D5268&gt;, &#39;static_f&#39;: &lt;function A.static_f at 0x00000265401D52F0&gt;, &#39;class_f&#39;: &lt;function A.class_f at 0x00000265401D5378&gt;, &#39;__dict__&#39;: &lt;attribute &#39;__dict__&#39; of &#39;A&#39; objects&gt;, &#39;__weakref__&#39;: &lt;attribute &#39;__weakref__&#39; of &#39;A&#39; objects&gt;, &#39;__doc__&#39;: None} a实例属性: {&#39;name&#39;: &#39;xy&#39;, &#39;age&#39;: 2, &#39;sex&#39;: &#39;man&#39;} [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;age&#39;, &#39;class_f&#39;, &#39;class_var&#39;, &#39;fun&#39;, &#39;name&#39;, &#39;num&#39;, &#39;static_f&#39;] is和==的区别 is 比较的是两个实例对象是不是完全相同，它们是不是同一个对象，占用的内存地址是否相同。（即比较的id是否相同，这id类似于人的身份证标识）。 == 比较的是两个对象的内容是否相等，即内存地址可以不一样，内容一样就可以了。默认会调用对象的 eq()方法。 直接赋是引用。 1234a = [1, 2, 3]b = aprint(a is b)print(a == b) True True 1234a = [1, 2, 3]b = [1, 2, 3]print(a is b)print(a == b) False True 常见内建模块collectionsCounterCounter类的目的是用来跟踪值出现的次数。它是一个无序的容器类型，以字典的键值对形式存储，其中元素作为key，其计数作为value。计数值可以是任意的Interger（包括0和负数）。 123from collections import Counterc = Counter('grammer')c['r'] 2 12c = Counter(&#123;'a':4, 'b':2&#125;)c['a'] 4 当所访问的键不存在时，返回0，而不是KeyError；否则返回它的计数。 1c['d'] 0 可以使用一个iterable对象或者另一个Counter对象来更新键值。 12c.update('witch')c['h'] 1 most_common([n])返回一个TopN列表。如果n没有被指定，则返回所有元素。当多个元素计数值相同时，排列是无确定顺序的。 12c = Counter('abccdabdddfdae')c.most_common(2) [(&#39;d&#39;, 5), (&#39;a&#39;, 3)]]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2018%2F06%2F03%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树（decision tree）是一种基本的分类与回归方法，它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要的有点是模型可读性强，分类速度快，学习时利用训练数据，根据损失函数最小化的原则建立决策树模型。决策树的学习通常包括三个步骤： 特征选择 决策树的生成 决策树的修剪 决策树模型模型组成决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或者属性，叶结点表示一个类。 决策树学习假设给定训练数据集 D=\{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}其中，$x_i$ 为输入实例（特征向量），$y_i$ 为类标记，$N$ 为样本容量。 决策树学习本质上是从训练数据集中归纳出一组分类规则。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。另一个角度看，决策树学习是由训练数据集估计条件概率模型。我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。 决策树学习用损失函数表示这一目标。如下所述，决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。 当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题（NP的英文全称是Non-deterministic Polynomial的问题，即多项式复杂程度的非确定性问题），所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优(sub-optimal)的。 决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。 特征选择熵：熵 $H(X)$ 只依赖于 $X$ 分布，而与 $X$ 的取值无关，所以也可以将 $X$ 的熵记作 $H(p)$ ， H(p)=-\sum_{i=1}^n p_i \log p_i条件熵：条件熵 $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性，定义为 $X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望： H(Y|X)=\sum_{i=1}^n p_i H(Y|X=x_i)信息增益：特征$A$对训练数据集$D$的信息增益 $g(D,A)$ ，定义为集合$D$的经验熵 $H(D)$ 与特征A给定条件下$D$的经验条件熵 $H(D|A)$ 之差，即 g(D,A)=H(D)−H(D|A)信息增益比：特征$A$对训练数据集D的信息增益比 $g_R(D,A)$ 定义为其信息增益 $g(D,A)$ 与训练数据集$D$关于特征$A$的值的熵 $H_A(D)$ 之比，即 g_R(D,A)=\frac{g(D,A)}{H_A(D)}其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2 \frac{|D_i|}{|D|}$，$n$ 为特征A的取值个数。 ID3算法应用特征增益选择特征，C4.5应用特征增益比选择特征。 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。 决策树的生成ID3算法ID3算法（interative dichotomiser 3）的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。具体方法是：从根结点（root node）开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。 C4.5算法C4.5算法与ID3算法相似。C4.5在生成的过程中，用信息增益比来选择特征。 决策树的剪枝决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树容易出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，解决这一问题的方法是考虑决策树的复杂度，对已生成的决策树进行简化。 在决策树学习中将已生成的树进行简化的过程称为剪枝（pruning）。 决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现。设树 $T$ 的叶结点个数为 $|T|$，$t$ 是树 $T$ 的叶结点，该叶结点有 $N_t$ 个样本点，其中 $k$ 类的样本点有 $N_{tk}$ 个， $H_t(T)$ 为叶节点t上的经验熵，$\alpha \ge0$ 为参数，则决策树学习的损失函数可以定义为 C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|其中，经验熵为 H_t(T) = -\sum_k \frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}在损失函数中，将第一项记为 C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}log{N_{tk}\over N_t}这时有 C_{\alpha}(T)=C(T)+\alpha|T|其中$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂度，参数$\alpha\ge0$控制两者之间的影响。较大的$\alpha$促使选择较简单的模型（树），较小的$\alpha$促使选择较复杂的模型（树）。$\alpha=0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。 可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。 树的剪枝算法 输入：生成算法产生的整个树$T$，参数$\alpha$。 输出：修剪后的子树$T_{\alpha}$ 计算每个结点的经验熵 递归地从树的叶结点向上回缩设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数值分别是$C_\alpha(T_B)$与$C_\alpha(T_A)$，如果 C_\alpha(T_A)\le C_\alpha(T_B)则进行剪枝，即将父结点变为新的叶结点 返回2，直至不能继续为止，得到损失函数最小的子树$T_\alpha$ CART算法分类与回归树(classification and regression tree,CART)模型是应用广泛的决策树学习方法。CART同样由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归。 CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。 CART算法由以下两步组成： 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝标准。 CART生成决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼（Gini index）指数最小化准则，进行特征选择，生成二叉树。 回归树的生成 最小二乘回归树生成算法： 输入：训练数据集$D$。输出：回归树$f(x)$。 在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树： 选择最优切分变量$j$与切分点$s$，求解 \min_{j,s}\left[\min_{c_1}\sum_{s_i\in R_1(j,s)}(y_i-c_i)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值的对$(j,s)$ 对选定的对$(j,s)$划分区域并决定相应的输出值： R_1(j,s)=\{ x|x^{(j)}\le s\}, \quad R_2(j,s)=\{ x|x^{(j)}> s\} \\ \hat c_m=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i, \quad x\in R_m, \quad m=1,2 继续对两个子区域调用步骤1，2，直至满足停止条件。 将输入空间划分为$M$个区域$R_1,R_2,\cdots,R_M$，生成决策树： f(x)=\sum_{m=1}^M\hat c_mI(x\in R_m) 分类树的生成分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。 基尼指数基尼指数（定义）：分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$,则概率分布的基尼指数定义为： Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2对于二分类问题，若样本点属于第1个类的概率是$p$，则概率分布的基尼指数为 Gini(p)=2p(1-p)对于给定的样本集合$D$，其基尼指数为 Gini(D)=1-\sum_{k=1}^K\left( \frac{|C_k|}{|D|}\right )^2这里$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。 如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即 D_1=\{ (x,y)\in D | A(x)=a\},D_2=D-D_1在特征$A$的条件下，集合$D$的基尼指数定义为 Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D,A)$表示经$A=a$分割后集合$D$的不确定性。基尼指数越大，样本集合的不确定性就越大，这一点与熵类似。 CART分类树的生成 输入：训练数据集$D$输出：CART决策树 根据训练数据集，从根结点开始，递归地对每个结点进行一下操作，构建二叉决策树： 设结点的训练数据集为$D$，计算现有特征对该数据集的基尼指数。此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试是“是”或“否”将$D$分割成$D_1$和$D_2$两部分。 在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点，依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中。 对两个子结点递归地调用1，2，直至满足停止条件。 生成CART决策树。 算法停止的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。 CART剪枝损失函数定义为如下： C_{\alpha}(T)=C(T)+\alpha|T|$\alpha$权衡拟合程度与树的复杂度。 那么我们如何找到这个合适的$\alpha$来使拟合程度与复杂度之间达到最好的平衡呢，最好的办法就是，我们将$\alpha$从0取到正无穷，对于每一个固定的$\alpha$，我们都可以找到使得$C_{\alpha}(T)$最小的最优子树$T(\alpha)$ 。当$\alpha$ 很小的时候，$T_0$ 是这样的最优子树，当$\alpha$ 很大的时候，单独一个根节点是这样的最优的子树。 尽管 $\alpha$ 取值无限多，但是 $T_0$ 的子树是有限个，因此我们可以生成这样一个子树序列 $T_0, T_1, T_2, …, T_n$。 Breiman证明：将 $\alpha$ 从小增大，$0=\alpha_0&lt;\alpha_1&lt;…&lt;\alpha_n&lt;+\infty$ ，剪枝得到的子树对应着区间 $[\alpha_i,\alpha_{i+1})$ 中，$i=0,1,…,n$对应着子树序列$\{T_0,T_1,…,T_n\}$。序列中的子树是嵌套的。 因此，剪枝可分为两部分，第一部分生成子树序列，第二部分交叉验证。 形成子树序列我们每次剪枝剪的都是某个内部节点的子节点，也就是将某个内部节点的所有子节点回退到这个内部节点里，并将这个内部节点作为叶子节点。因此在计算整体的损失函数时，这个内部节点以外的值都没变，只有这个内部节点的局部损失函数改变了，因此我们本需要计算全局的损失函数，但现在只需要计算内部节点剪枝前和剪枝后的损失函数。 对任意内部节点$t$，剪枝前的状态：有 $|T_t|$ 个叶子节点，预测误差是$C(T_t)$；剪枝后的状态：只有本身一个叶子节点，预测误差是$C(t)$。 剪枝前以$t$为根节点的子树的损失函数是 C_{\alpha}(T_t)=C(T_t)+\alpha |T_t|剪枝后以$t$为单节点树的损失函数是 C_{\alpha}(t)=C(t)+\alpha当$\alpha =0$或者充分小时，有： C_{\alpha}(T_t) \lt C_{\alpha}(t)当增大$\alpha$，总有那么一个点，能够使得： C_{\alpha}(T_t) = C_{\alpha}(t)当继续增大$\alpha$时，不等式反向。只要$\alpha = \frac{C(t)-C_(T_t)}{|T_t|-1}$，$T_t$与$t$有相同的损失函数值，而$t$的节点比$T_t$少，对$T_t$剪枝。 因此，我们对 $T_i$ 中的每个内部节点$t$都计算 g(t)=\frac{C(t)-C_(T_t)}{|T_t|-1}它表示剪枝后整体损失函数减少的程度。在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\alpha_1$，$T_1$为区间$[\alpha_1, \alpha_2)$的最优子树。 如此剪下去，知道根节点，在这一个过程中，不断增加$\alpha$的值，产生新区间。 交叉验证利用独立数据集，测试子树序列中各棵子树的平方误差或基尼指数，从中选择最优子树。 CART剪枝算法：输入：CART算法生成的决策树$T_0$输出：最优决策树$T_{\alpha}$ 设$k =0,T =T_0$ 设$\alpha = + \infty$ 自下而上地对各个内部结点$t$计算$C(T_t),\vert T_t\vert$以及 g(t) =\frac{C(t)-C(T_t)}{\vert T_t\vert-1} \\ \alpha = min (\alpha, g(t))这里，$T_t$表示$t$为根结点的子树，$C(T_t)$是对训练数据的预测误差，$\vert T_t \vert$是$T_t$的叶结点个数。 对$g(t) = \alpha$的内部结点$t$进行剪枝，并对叶结点$t$以多数表决法决定其类，得到树$T$。 设$k = k+1,\alpha_k = \alpha,T_k =T$。 如果$T_k$不是由根结点及两个叶结点构成的树，则回到步骤3；否则令$T_k = T_n$。 采用交叉验证法在子树序列$T_0,T_1,…,T_n$中选取最优子树$T_{\alpha}$ 参考文献[1]. 《统计学习方法》 李航[2]. 决策树之剪枝原理与CART算法[3]. 决策树—统计学习方法总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>基尼指数</tag>
        <tag>信息增益</tag>
        <tag>CART</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Effective Python》 读书笔记]]></title>
    <url>%2F2018%2F06%2F02%2FEffectivePython%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[《Effective Python 59》 是非常值得一读的Python进阶书籍，它阐述了Python语言中一些鲜为人知的微妙特性，并给出了能够改善代码功能及运行效率的习惯用法。相比其他Python书籍，这本书有以下几个特点： 从工程实践出发、以场景为主导阐述如何编写高质量、可维护的代码，并配套的代码范例。 内容不拖沓，省去很多基础语法。 不厚，只有二百多页（这一点至关重要）。 这是我看完之后做的一份读书笔记，列出来以供以后不时查阅。（还没写完，持续更新…） 用Pythonic方式来思考bytes、str与unicode的区别很多时候项目从Python2.x迁移到Python3.x会遇到字符编码的问题，原因是Pyhton2.x和Python3.x的字符编码不统一，具体如下：Python3：有两种表示字符序列的类型：bytes和str： bytes：8个二进制位 str：unicode字符 Python2：有两种表示字符序列的类型：str和unicode： str：8个二进制位 unicode：unicode字符 unicode$\rightarrow$ 二进制: 常见的编码方式是utf-8，使用encode方法 二进制$\rightarrow$unicode: 使用decode方法 在编程的时候，编解码放在接口外面来做。程序核心使用unicode字符，且不要对字符编码做任何假设。 12345678910111213def to_str(bytes_or_str): if isinstance(bytes_or_str, bytes): value = bytes_or_str.decoce('utf-8') else: value = bytes_or_str return valuedef to_bytes(bytes_or_str): if isinstance(bytes_or_str, str): value = bytes_or_str.encode('utf-8') else: value = bytes_or_str return value 在Python3中，涉及到文件处理的操作（使用内置的open函数）会默认的以UTF-8进行编码。而在Python2中默认采用二进制形式来编码。这也是导致很多意外事故发生的根源，特别是对于那些更习惯使用Python2的程序员而言。 比方说，将几个随机的二进制数据写入到一个文件中。在Python2中，下面的这段代码可以正常的工作，但是在Python3中却会报错并退出。 123import oswith open('random.bin', 'w') as f: f.write(os.urandom(10)) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-4-71b20f96b6df&gt; in &lt;module&gt;() 1 import os 2 with open(&#39;random.bin&#39;, &#39;w&#39;) as f: ----&gt; 3 f.write(os.urandom(10)) TypeError: write() argument must be str, not bytes 导致这个异常发生的原因是在Python3中对于open函数又新增了一个名为encoding的参数。此参数默认为UTF-8。这样在文件句柄上进行read和write操作时，必须传入Unicode字符串的str实例，而不是包含了二进制数据的bytes实例。 用生成器表达式来改写数据量较大的列表推导 当输入的数据量较大时，列表推导可能因为占用太多内存而出问题。 使用圆括号构成生成器表达式，由生成器表达式所返回的迭代器，可以逐次产生输出值，从而避免内存占用问题。 把某个生成器表达式所返回的迭代器，放在另一个生成器表达式的for子表达式中，即可将二者组合起来。 串在一起的生成器表达式执行速度很快，果要把多种手法组合起来，以操作大批量数据，最好是用生成器表达式实现。 12345678910111213# 这种类表推导只适合处理少量的输入值，对于大量数据，最好考虑生成器表达式而不是列表生成式value = [len(x) for x in open('my_file.txt')]# 生成器表达value = (len(x) for x in open('my_file.txt'))print('value: ', value)print('next value: ',next(value))print('next value: ', next(value))# 使用生成器表达式的另一个好处是可以互相组合# 这种连锁生成器表达式，可以迅速在python中执行roots = ((x, x*10) for x in value)print('next roots: ', next(roots)) value: &lt;generator object &lt;genexpr&gt; at 0x000001A6AC0D1308&gt; next value: 7 next value: 1 next roots: (6, 60) 尽量用enumerate代替range enumerate提供了一种精简的写法，可以在遍历迭代器时获知每个元素的索引。 尽量用enumerate来改写那种将range与下标访问相结合的序列遍历代码。 可以个enumerate提供第二个参数，以指定开始计数时所用的值（默认值为0）。 123456789101112131415# 第一种写法。不推荐print('range写法，不推荐')flavor_list = ['vanilla', 'chocolate', 'pecan', 'strawberry']for i in range(len(flavor_list)): print('%d: %s' % (i+1, flavor_list[i]))# enumerateprint('\nenumerate方法')for i, flavor in enumerate(flavor_list): print('%d: %s' % (i+1, flavor)) # 还可以直接指定enumerate函数开始计数时所用的值print('\n指定enumerate函数开始计数时所用的值')for i, flavor in enumerate(flavor_list, 1): print('%d: %s' % (i, flavor)) range写法，不推荐 1: vanilla 2: chocolate 3: pecan 4: strawberry enumerate方法 1: vanilla 2: chocolate 3: pecan 4: strawberry 指定enumerate函数开始计数时所用的值 1: vanilla 2: chocolate 3: pecan 4: strawberry 用zip函数同时遍历两个迭代器 内置的zip可以平行地遍历多个迭代器。 Python3中的zip相当于生成器，会在遍历过程中逐次产生元组，而Python2中直接把这些元组完全生成好，并一次性返回整份列表。 如果提供的迭代器长度不等，zip会提前终止。 123456789101112names = ['LiMing', 'LiLi', 'ZhangMei']letters = [len(n) for n in names]longest_name = Nonemax_letters = 0for i in range(len(names)): count = letters[i] if count &gt; max_letters: max_letters = count longest_name = names[i]print(longest_name, max_letters) ZhangMei 8 12345678longest_name = Nonemax_letters = 0for name, count in zip(names, letters): if count &gt; max_letters: longest_name = name max_letters = count print(longest_name, max_letters) ZhangMei 8 不要在for和while循环后面写else块 Python有种特殊写法，可在for和while循环的内部语句块之后紧跟一个else块。 但这种写法既不直观，又容易让人误解，应该避免这种写法。 合理利用 try/except/else/finally 结构中的每个代码块 try...finally...这种结构简单的说是在try下的全部操作如果某项失败的话就终止并执行finally下定义的语句。如果全部操作都没有报错，那么最后也执行finally下定义的语句，经常用于既要向上传播异常，又要在异常发生时执行某些清理操作。 try...except...else... 可以混合使用。 12345678910111213141516171819202122232425262728293031323334353637# try...finally...handle = open('my_file.txt', 'w')try: handle.write('111') handle.write('222')finally: print('close handle') handle.close()# try...except...else...import jsondef load_json_key(data, key): try: result_dict = json.load_json_key(data) except ValueError as e: raise KeyError from e else: return result_dict[key]# 混合使用UNDEFINED = object()def divide_json(path): handle = open(path, 'r+') try: data = handle.read() op = json.loads(data) value = (op['numerator'], op['denominator']) except ZeroDivisionError as e: return UNDEFINED else: op['result'] = value result = json.dumps(op) handle.seek(0) handle.write(result) return value finally: handle.close() close handle 函数尽量用异常来表示特殊情况，而不要返回None 返回None的来作为特殊的含义很出错，因为None和其他的变量（例如 zero，空字符串）在条件表达式的判断下是等价的。 函数在遇到特殊情况时，应该抛出异常，而不是返回None。这样调用者就能够合理地按照函数中的说明文档来处理由此而引发的异常了。 用None表示特殊情况如下： 12345678def divide(a, b): try: return True, a/b except ZeroDivisionError: return False, None sucess, result = divide(3,3)result 1.0 用异常表示特殊情况如下： 123456789101112def divide(a, b): try: return a / b except ZeroDivisionError as e: raise ValueError('Invalid inputs') from etry: result = divide(3, 0)except ValueError: print('Invlid inputs')else: print('Result is: %f' % result) Invlid inputs 第一种方法问题在于，Python程序员习惯用-表示用不到的变量，那很有可能调用者会轻易的跳过元组的第一部。第二种方法让程序员不得不处理异常情况。 了解如何在闭包里使用外围作用域中的变量假如有一份数字列表，要对其排序，但在排序时，要把出现在某个组群中的数字，放在组群外的那些数字之前，简单的实现如下： 1234567891011def sort_priority(values, group): def helper(x): if x in group: return (0, x) return (1, x) values.sort(key=helper)numbers = [8, 3, 1, 2, 5, 4, 7, 6, -1]group = &#123;2, 3, 5, 7&#125;sort_priority(numbers, group)numbers [2, 3, 5, 7, -1, 1, 4, 6, 8] 上面程序能正常工作的原因是以下三个方面： Python支持闭包：闭包是一种定义在某个作用域中的函数，这种函数引用了那个作用域中的变量 Python的函数是一级对象，也就是说，我们可以直接引用函数、把函数赋给变量、把函数当成参数 Python使用特殊规则比较两个元祖，首先比较下标为0的对应元素，如果相等，再比较下标为1的对应元素，以此类推 若果增加一个功能，如果在数字出现在了组群中，返回一个标志，先试试下面这种写法： 12345678910111213def sort_priority2(numbers, group): found = False def helper(x): if x in group: found = True return (0, x) return (1, x) numbers.sort(key=helper) return foundfound = sort_priority2(numbers, group)print(found)print(numbers) False [2, 3, 5, 7, -1, 1, 4, 6, 8] 排序的结果是对的，但标志不对。解释如下： 当在表达式中引用变量的时候，Python解释器会按如下顺序遍历各作用域： 当前函数的作用域。 任何外围作用域（比如其他的包含着的函数）。 包含当前代码的模块域（也称之为全局作用域）。 内置域（包含了像len,str等函数的域）。 如果上述地方都没找到，就抛出异常。 当给变量赋值时，如果变量在当前作用域内已经被定义过，那么该变量会具备新值，如果当前作用域没有这个变量，Python会把这次的赋值行为视为对变量的定义 在Python3中有一种特殊的写法，能够获取闭包内的数据，我们可以用nonlocal语句表明这样的意图。 1234567891011121314def sort_priority2(numbers, group): found = False def helper(x): nonlocal found if x in group: found = True return (0, x) return (1, x) numbers.sort(key=helper) return foundfound = sort_priority2(numbers, group)print(found)print(numbers) True [2, 3, 5, 7, -1, 1, 4, 6, 8] nonlocal清楚的表明：如果在闭包内给该变量赋值，那么修改的其实是闭包外的那个作用域中的变量，nonlocal不能延伸到模块级别 Python2中不支持nonlocal nonlocal可能也会像全局变量一样遭到滥用，建议只在及其简单的函数中使用这种机制 如果使用nonlocal的那些代码，已经写的越来越复杂了，那就应该将相关的状态封装成辅助类 下面定义的类与nonlocal的功能相同 1234567891011121314151617class Sorter(object): def __init__(self, group): self.group = group self.found = False def __call__(self, x): if x in self.group: self.found = True return (0, x) return (1, x)numbers = [8, 3, 1, 2, 5, 4, 7, 6, -1]group = &#123;2, 3, 5, 7&#125;sorter = Sorter(group)numbers.sort(key=sorter)print(sorter.found)print(numbers) True [2, 3, 5, 7, -1, 1, 4, 6, 8] 考虑用生成器来改写直接返回列表的函数如果函数要产生一系列结果，最简单的做法是返回一份列表。例如，想知道一个字符串中每个单词的首字母在句子中的位置。代码如下： 12345678910111213def index_words(text): result = [] if text: result.append(0) for index, letter in enumerate(text): if letter == ' ': result.append(index + 1) return resultaddress = 'Four score and seven years ago...'result = index_words(address)result [0, 5, 11, 15, 21, 27] 这段代码的问题如下： 代码杂乱拥挤。每次都要调用append方法，而且要初始化result，返回result。 在返回前，要把所有的结果放在列表中，在输入量大的情况下，有内存崩溃的风险 下面这个生成器函数，返回一个迭代器，与之前的代码功能相同。 1234567891011def index_words(text): if text: yield 0 for index, letter in enumerate(text): if letter == ' ': yield index+1address = 'Four score and seven years ago...'result = index_words(address)print(result)list(result) &lt;generator object index_words at 0x000001A6AC0FA9E8&gt; [0, 5, 11, 15, 21, 27] 在参数上迭代时，要多加小心 Python的迭代器协议，描述了容器和迭代器应该如何与iter和next内置函数、for循环函数及相关表达式相互配合。 把__iter__方法实现为生成器，即可定义自己的容器类型。 想判断某个值是迭代器还是容器，可以拿该值为参数，两次调用iter函数，若结果相同，则是迭代器。 当一个函数接收的参数是一个对象列表，那么很有可能要在这个列表上迭代。如下代码计算个体占总体的百分比。 123456789def normalize(numbers): total = sum(numbers) result = [] for num in numbers: result.append(100 * num / total) return resultvisits = [15, 35, 80]normalize(visits) [11.538461538461538, 26.923076923076923, 61.53846153846154] 如果将个体数据放在一份文件里，然后从文件中读取，我们定义read_vists函数，然后定义一个生成器以便将数据应用到更大数据集上。 12345678def read_vists(path): with open(path) as f: for line in f: yield int(line)it = read_vists('data.txt')percentages = normalize(it)percentages [] 奇怪的是，以生成器返回的迭代器为参数，来调用normalize，没有产生任何结果。原因是迭代器只能产生一轮结果，在sum函数那里已经用完了。但之后在已经用完的迭代器上继续迭代时，没有报错。一种解决方法是通过参数来接受另外一个函数，那个函数每次调用后，都能返回新的迭代器。代码如下。 1234567def normalize_func(numbers): total = sum(get_iter()) # 一个新的迭代器 result = [] for value in get_iter(): # 又一个新的迭代器 percent = 100 * value / total result.append(percent) return result 这种方法显得生硬，很不Pythnic。另一种解决方法是新编一种实现迭代器协议的容器类。 实际上，当Python执行类似for x in foo这样的表达式的时候，它就会调用iter(foo)。内置的iter函数然后会调用foo.__iter__方法。该方法返回一个迭代器对象，而那个迭代器对象，则实现了__next__方法。然后循环语句会在迭代器对象上反复调用next方法，直到产生StopIteration异常。 只需自己的类把__iter__方法实现为生成器就满足上述要求。 1234567891011class ReadVisits(object): def __init__(self, data_path): self.data_path = data_path def __iter__(self): with open(self.data_path) as f: for line in f: yield int(line) visits = ReadVisits('data.txt')normalize(visits) &lt;generator object ReadVisits.__iter__ at 0x000001A6AC0D1C50&gt; normalize的sum方法会调用ReadVisits.__iter__方法，之后的for循环也会调用ReadVisits.__iter__方法。 迭代器协议有这样的规定：如果把迭代器对象传给内置的iter函数，那么此函数会把该迭代器返回，反之，如果传给iter的是个容器类型的对象，那么iter函数则每次回返回新的迭代器对象，我们可以根据这种行为来判断输入值是不是迭代器对象本身，如果是，就抛出错误 下面是对第一个函数的完善。 123456789101112def normalize_defensive(numbers): if iter(numbers) == iter(numbers): raise TypeError('Must supply a container') total = sum(numbers) result = [] for value in numbers: percent = 100 * value / total result.append(percent) return resultvisits = ReadVisits('data.txt')normalize_defensive(visits) [7.6923076923076925, 28.205128205128204, 33.97435897435897, 9.615384615384615, 20.512820512820515] 12it = read_vists('data.txt')normalize_defensive(it) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-32-df5cf9d8e72a&gt; in &lt;module&gt;() 1 it = read_vists(&#39;data.txt&#39;) ----&gt; 2 normalize_defensive(it) &lt;ipython-input-29-ce5cccbce26d&gt; in normalize_defensive(numbers) 1 def normalize_defensive(numbers): 2 if iter(numbers) == iter(numbers): ----&gt; 3 raise TypeError(&#39;Must supply a container&#39;) 4 total = sum(numbers) 5 result = [] TypeError: Must supply a container 用数量可变的位置参数减少视觉杂讯令函数接受可选的位置参数（*args)，能够使代码更加清晰，并能减少视觉杂讯。 例如： 你想打印一些调试信息。假如该函数的参数个数固定不变，那它就必须接受一段信息及一份含有待打印值的列表。 1234567def log(message, values): if not values: print(message) else: value_str = ','.join(str(x) for x in values) print('%s: %s' % (message, value_str))log('My Number are', [11, 22, 33]) My Number are: 11,22,33 但是如果没有values要打印的时候也必须传递一个空列表，这样使得代码既麻烦又杂乱。 我们在位置参数前加一个*来实现可变参数 123456789101112def log(message, *values): if not values: print(message) else: value_str = ','.join(str(x) for x in values) print('%s: %s' % (message, value_str))log('My Number are', 12, 33 ,44)numbers = [12, 33, 44]log('My Number are', *numbers)log('My Number are') My Number are: 12,33,44 My Number are: 12,33,44 My Number are 有两个应该注意的问题： 第一个是可变参数在被传递给函数的时候要转变成元组。这意味着如果调用者对生成器使用了*操作符，来调用这种函数，程序将会先把生成器迭代一轮，并把生成器锁生成的每一个值，都放在元组中，如果数据量巨大，可能消耗大量内存，并导致程序崩溃。 使用*args参数的话，如果以后再新增其他的位置参数，就必须修改原来调用该函数的那些旧代码，如果不更新调用代码，则会产生难以调试的错误。 用关键字参数表达可选的行为 Python函数中的所有的位置参数都可以通过关键字来传值，关键字参数的顺序不限，只要把函数所要求的全部位置参数都指定好即可。 还可以混合使用关键字参数和位置参数来调用函数。 位置参数必须出现在关键字参数之前。 只使用位置参数来调用函数，可能导致这些参数值的含义不够明确，而关键字参数则能够阐明每个参数的意图。 给函数添加新的行为时，可以使用带默认值的关键字参数，以便于原有的调用代码保持兼容，关键字参数提供了一种扩充函数参数的有效方式。 可先的关键字参数，总是应该以关键字形式来指定，而不应该以位置参数的形式来指定。 1234567def remainder(number, divisor): return number % divisorremainder(20, 7)remainder(20, divisor=7)remainder(number=20, divisor=7)remainder(divisor=7, number=20) 6 1remainder(number=20, 7) File &quot;&lt;ipython-input-15-fa871e527313&gt;&quot;, line 1 remainder(number=20, 7) ^ SyntaxError: positional argument follows keyword argument 用None和文档字符串来描述具有动态默认值的参数 参数的默认值，只会在程序加载模块并读到本函数的定义时评估一次，对于{}、[]等动态的值，者可能导致奇怪的行为。 对于以动态值作为实际默认值的关键字参数来说，应该把形式上的默认值写为None，并在函数的文档字符串里面描述该默认值所对应的实际行为。 例如打印日志时想要加上打印的时间。先看如下的写法： 1234from datetime import datetimeimport timedef log(message, when=datetime.now()): print('%s: %s' % (when, message)) 123log('Hi there!')time.sleep(1)log('Hi again!') 2018-05-30 11:37:23.481681: Hi there! 2018-05-30 11:37:23.481681: Hi again! 从上面打印的信息来看，时间戳是相同的，这是因为datetime.now仅仅被执行了一次，也及时它只在函数定义的时候执行了一次。 参数的默认值，仅仅在模块被加载进来的时候执行一次，而这通常发生在程序开始运行的时候。当模块已经加载完这段代码后，参数的默认值就不会被改变了。 在Python中如果想真正实现动态默认值，习惯上把默认值设为None，并且在文档字符串中记录详细的行为和使用方法。当代码发现一个值为None的参数的时候，就可以为其分配默认值了。 修改打印日志的函数，产生不同的时间戳，代码如下。 123456789101112131415def log(message, when=None): """ Log a message with a timestamp. Args: message: Message to print when: datetime of when the message occurred. Default to the present time. """ when = datetime.now() if when is None else when print("%s: %s" %(when, message))log("hi there!")time.sleep(1)log('hi again!') 2018-05-30 11:43:57.224250: hi there! 2018-05-30 11:43:58.224661: hi again! 如果参数的实际默认值是可变类型（mutable），那就一定要记得用None作为形式上的默认值。 例如实现一个功能：加载一个被编码为JSON的数据值，如果解码的时候失败了，你想默认返回一个空字典。代码如下： 123456import jsondef decode(data, default=&#123;&#125;): try: return json.loads(data) except ValueError: return default 123456foo = decode('bad data')foo['stuff'] = 5bar = decode('also bad')bar['meep'] = 1print(foo)print(bar) {&#39;stuff&#39;: 5, &#39;meep&#39;: 1} {&#39;stuff&#39;: 5, &#39;meep&#39;: 1} 我们本以为foo和bar会表示两份不同的字典，每份字典都有一个键值对，但以上代码的结果却是，修改一个的话很明显也会改变另一个。错误的根本原因是：foo和bar其实都等同于卸载default参数默认值中的那个字典，它们表示的都是同一个字典对象。 由于default参数的默认值只在模块加载时执行一次，所以凡是以默认的空字典调用这个函数的代码，都将共享一份字典 1assert foo is bar 解决办法就是对关键字参数设置默认值None并且记录在该函数的说明文档中。 12345678910111213141516171819202122def decode(data, default=None): """Load JSON data from string. Args: data: JSON data to be decoded. default: Value to return if decoding fails. Defaults to an empty dictionary. """ if default is None: default = &#123;&#125; try: return json.loads(data) except ValueError: return default foo = decode('bad data')foo['stuff'] = 5bar = decode('also bad')bar['meep'] = 1print('Foo:', foo)print('Bar:', bar) Foo: {&#39;stuff&#39;: 5} Bar: {&#39;meep&#39;: 1} 用只能以关键字形式指定的参数来确保代码明确清晰对于函数的某些关键变量，希望调用者明确改参数的值，使代码清晰，减少逻辑错误。此时，可以使用命名关键字参数。示例如下： 123def safe_division_c(number, division, *, ignore_overflow=False, c=False): pass ignore_overflow和ignore_overflow只能通过关键字参数赋值的形式被使用，而不是通过位置参数赋值的方式，也不能像默然参数那样省略。 类与继承尽量用辅助类来维护程序的状态，而不是字典和元组 不要使用包含其他字典的字典，也不要使用过长的元组（元组里的元素超过两个，就应该考虑用其他办法实现）。 如果容器中包含简单又不可变的数据，那么可以先使用namedtuple来表示，待稍后有需要时，再修改为完整的类。 保存内部状态的字典如果变得复杂，那就应该吧这些代码拆解为多个辅助类。 namedtuple是继承自tuple的子类。namedtuple创建一个和tuple类似的对象，而且对象拥有可访问的属性。由于这种具名元组的属性都带有名称，所以当需求发生变化，以致要给简单的数据容器添加新的行为时，很容易就能从namedtuple迁移到自己定义的类。 尽管namedtuple在很多场合很有用，但它在有些场合使用反而不好。 namedtuple类无法指定各参数的默认值。对于可选属性比较多的数据来说，namedtuple用起来很不方便。 namedtuple实例的各项属性，依然可以通过下标及迭代访问。这可能导致其他人以不符合设计者意图的方式使用这些元组，从而使以后很难迁移成真正的类。 1234567from collections import namedtupleUser = namedtuple('User', ['name', 'sex', 'age'])user1 = User(name='kongxx', sex='male', age=21)print('age:', user1.age)print('sex:', user1.sex)print('user1[0]:',user1[0]) age: 21 sex: male user1[0]: kongxx 简单的接口应该接受函数，而不是类的实例 对于连接各种Python组件的简单接口来说，通常应该给其直接传入函数，而不是先定义某个类，然后再传入该类的实例。 Python中的函数和方法都可以像一级对象那样引用，因此，它们和其他类型的对象一样，也能够放在表达式里。 通过名为__call__的特殊方法，可以使类的实例能够像普通的Python函数那样得到调用。 如果要用函数来保存状态，那就应该定义新的类，并令其实现__call__方法，而不要定义带状态的闭包。__call__方法强烈地暗示了该类的用途，它告诉我们，这个类的功能就相当于一个带有状态的闭包。 例如：Python内置的defaultdict类，这个数据结构允许调用者提供一个函数，在查询本字典时，如果字典中没有待查询的键时，此函数返回一个默认值。而且为字典中的这个缺省键来返回一个默认值。提供像这样的函数接口，可以使得API更容易被构建和测试，因为它能够把附带的效果和确定的行为分开。 例如：现在我们要给defaultdict传入一个产生默认值的挂钩函数，并令其统计出该字典一共遇到了多少个缺失的键。 第一种方式：带状态的闭包。 12345678910111213141516171819202122232425from collections import defaultdictdef increment_with_report(current, increments): added_count = 0 def missing(): nonlocal added_count # 状态闭包 added_count += 1 return 0 result = defaultdict(missing, current) for key, amount in increments: result[key] += amount return result, added_countcurrent = &#123;'green': 12, 'blue': 3&#125;increments = [ ('red', 5), ('blue', 17), ('orange', 9)]result, count = increment_with_report(current, increments)assert count == 2 尽管defaultdict并不知道missing挂钩函数里保存了状态，但是运行上面的代码，依旧会产生预期的结果。 但上述方法读起来不够直观。 第二种方法：定义一个小的类，把想追踪的状态信息封装起来。 12345678910111213class CountMissing(object): def __init__(self): self.added = 0 def missing(self): self.added += 1 return 0 counter = CountMissing()result = defaultdict(counter.missing, current)for key, amount in increments: result[key] += amountassert counter.added == 2 这种方法却是比increment_with_report函数更加清晰，但是，单看这个类，依然不太容易理解CounMissing的意图，CountMissing对象由谁构造？missing方法谁来调用？该类以后是否需要添加新的方法？直到你看到了使用它的defaultdict函数，你才会明白这些问题。 为了厘清这些问题，我们可以在Python代码中定义__call__这个特殊的方法。该方法使对象能够像函数一样被调用。此外，如果把这样的实例传给内置的callable函数，那么callable会返回True。 第三种方法：类中实现__call__方法。 1234567891011121314class BetterCountMissing(object): def __init__(self): self.added = 0 def __call__(self): self.added += 1 return 0counter = BetterCountMissing()assert callable(counter)result = defaultdict(counter, current)for key, amount in increments: result[key] += amountassert counter.added == 2 用@classmethod形式的多态去通用地构建对象 Python的每个类只能有一个构造器，也就是一个__init__方法。 使用@classmethod可以用一种与构造器相仿的方式构造类的对象。 参考链接：https://stackoverflow.com/questions/12179271/meaning-of-classmethod-and-staticmethod-for-beginner例子如下。首先我们有一个处理时间的类： 12345class Date(object): def __init__(self, day=0, month=0, year=0): self.day = day self.month = month self.year = year 如果我们要通过字符串创建Date实例，此时我们要做如下操作： 将str的日期转为int。 通过int的日期构建Date 12day, month, year = map(int, '11-09-2012'.split('-')) date1 = Date(day, month, year) 如果我们经常要使用字符串创建Date实例，很显然如果能实现重载就更加方便，C++有重载的方法，但Python没有重载，每个类只有一个构造器，只有一个__init__方法，因此@classmethod方法应运而生。 12345678910111213class Date(object): def __init__(self, day=0, month=0, year=0): self.day = day self.month = month self.year = year @classmethod def from_string(cls, date_as_string): day, month, year = map(int, date_as_string.split('-')) date1 = cls(day, month, year) return date1date2 = Date.from_string('11-09-2012') cls表示这个类本身 @staticmethod与@classmethod的区别是不构建类的实例，也不访问、依赖和改变类，只是一个函数。和普通的非class的method作用是一样的，只不过是命名空间是在类里面。一般使用场景就是和类相关的操作。 加入我们要验证日期的合法性，我们可以使用@staticmethod，如下。 1234567891011121314151617181920class Date(object): def __init__(self, day=0, month=0, year=0): self.day = day self.month = month self.year = year @classmethod def from_string(cls, date_as_string): day, month, year = map(int, date_as_string.split('-')) date1 = cls(day, month, year) return date1 @staticmethod def is_date_valid(date_as_string): day, month, year = map(int, date_as_string.split('-')) return day &lt;= 31 and month &lt;= 12 and year &lt;= 3999date2 = Date.from_string('11-09-2012')is_date = Date.is_date_valid('11-09-2012') 用super初始化父类 Python采用MRO解决超类初始化次序以及菱形继承问题。 总是应该使用super来初始化父类。 初始化父类的传统方式是在子类中直接调用父类的__init__方法。 1234567class MyBaseClass(object): def __init__(self, value): self.value = valueclass MyChildClass(MyBaseClass): def __init__(self): MyBaseClass.__init__(self, 5) 这种方法对于单继承体系是可行的，但是如果子类收到多继承的影响，由于调用父类初始化方法的顺序并不固定，可能产生无法预知的行为（尤其在菱形继承体系下）。 super函数定义了方法解析顺序(MRO)以解决这一问题，MRO以标准的流程来安排超类之间的初始化顺序：深度优先、从左至右，也保证了菱形继承中超类的初始化方法只执行一次。这个MRO的顺序可以通过名为mro的类方法类查询。 12345678910111213141516171819202122from pprint import pprint# Python 2class TimesFiveCorrect(MyBaseClass): def __init__(self, value): super(TimesFiveCorrect, self).__init__(value) self.value *= 5 class PlusTwoCorrect(MyBaseClass): def __init__(self, value): super(PlusTwoCorrect, self).__init__(value) self.value += 2 class GoodWay(TimesFiveCorrect, PlusTwoCorrect): def __init__(self, value): super(GoodWay, self).__init__(value)foo = GoodWay(5)print("Should be 5 * (5 + 2) = 35 and is " , foo.value)pprint(GoodWay.mro()) Should be 5 * (5 + 2) = 35 and is 35 [&lt;class &#39;__main__.GoodWay&#39;&gt;, &lt;class &#39;__main__.TimesFiveCorrect&#39;&gt;, &lt;class &#39;__main__.PlusTwoCorrect&#39;&gt;, &lt;class &#39;__main__.MyBaseClass&#39;&gt;, &lt;class &#39;object&#39;&gt;] 调用GoodWay(5)的时候，它会调用TimesFiveCorrect.__init__，而TimesFiveCorrect.__init__又会调用PlusTwoCorrect.__init__，PlusTwoCorrect.__init__会调用MyBaseClass.__init__。到达钻石顶部之后，所有的初始化过程会按照相反的顺序进行。 在Python3中将super方法写法简化了，具体示例如下。 123class Implicit(MyBaseClass): def __init__(self, value): super().__init__(value * 2) 只在使用Mix-in组件制作工具类时进行多重继承 尽量避免多继承，能用mix-in组件实现的效果，就不要用多继承来做。 123456789101112131415161718192021222324252627282930313233class ToDictMixin(object): def to_dict(self): return self._traverse_dict(self.__dict__) def _traverse_dict(self, instance_dict): output = &#123;&#125; for key, value in instance_dict.items(): output[key] = self._traverse(key, value) return output def _traverse(self, key, value): if isinstance(value, ToDictMixin): return value.to_dict() elif isinstance(value, dict): return self._traverse_dict(value) elif isinstance(value, list): return [self._traverse(key, i) for i in value] elif hasattr(value, '__dict__'): return self._traverse_dict(value.__dict__) else: return value class BinaryTree(ToDictMixin): def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right# 这下把大量的Python对象转换到一个字典中变得容易多了。tree = BinaryTree(10, left=BinaryTree(7, right=BinaryTree(9)), right=BinaryTree(13, left=BinaryTree(11)))print(tree.to_dict()) {&#39;value&#39;: 10, &#39;left&#39;: {&#39;value&#39;: 7, &#39;left&#39;: None, &#39;right&#39;: {&#39;value&#39;: 9, &#39;left&#39;: None, &#39;right&#39;: None}}, &#39;right&#39;: {&#39;value&#39;: 13, &#39;left&#39;: {&#39;value&#39;: 11, &#39;left&#39;: None, &#39;right&#39;: None}, &#39;right&#39;: None}} 多用public属性，少用private属性 对Python来说，其属性的可见度只有两种，public和private。以两个下划线__开头的属性，是private字段，本类的方法可以访问它们，类外直接访问会引发异常。 子类无法访问父类的private字段。 Python会对私有属性做一些简单的变换，以保证private字段的私密性，例如：在MyObject中定义self.__privated_file，Python会以变换后的_MyObject__privated_file保存，以保证私密性。所以如果我们以这种方式在类外访问私有属性，也可以访问到。Python为什么不从语法上对于私有属性严格保证呢？最简单的原因就是”We are all consenting adults here”。Python程序员相信开放要比封闭好。 为了尽量减少无意间访问内部属性所带来的意外，Python程序员应该遵守《Python风格指南》中建议，用一种习惯性的命名方式来表示这种字段：以单下划线开头的字段应该视为protected字段，本类之外的那些代码在使用这种字段的时候要多加小心。 由于在开发中，以后的代码可能需要从这个类上继承子类，并在子类中添加新的行为，假设超类使用了private属性，那么在覆写子类的时候就会遇到麻烦。虽然此时仍然可以通过第3条中所述的方式访问超类私有属性，但如果继承体系发生变化，private字段很可能失效，导致子类出现错误。 一般来说，宁可叫子类更多地访问超类的protected属性，也别把这些属性设为private，我们应该在文档中说明每个protected字段的含义，解释哪些字段是可供子类使用的内部api，哪些是不应该完全触碰的数据。 继承collections.abc以实现自定义的容器类 如果要定制一个实现list等功能的简单子类，那就可以直接从Python的容器类型（如list或dict）继承。 想正确实现自定义的容器类型，可能需要编写大量的特殊方法。 编写自制的容器类型时，可以从collections.abc模块的抽象基类中继承，那些基类能够确保我们的子类具备适当的接口及行为。 如果要设计比较简单的序列，我们自然会想到继承Python的内置List类型，如下我们要定义一种自定义的列表类型，并提供统计各元素出现频率的功能。 12345678910111213141516class FrequencyList(list): def __init__(self, members): super().__init__(members) def frequency(self): counts = &#123;&#125; for item in self: counts.setdefault(item, 0) counts[item] += 1 return counts foo = FrequencyList(['a', 'b', 'c', 'b', 'c', 'f'])print('Length is: ', len(foo))foo.pop()print('after pop: ', repr(foo))print(foo.frequency()) Length is: 6 after pop: [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;b&#39;, &#39;c&#39;] {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 2} 现在要编写一个本身不属于list子类，但是可以像list一样通过下标来访问，并且可以使用len来获取长度。 Python用下标访问序列中的元素时，会把访问代码转译为：xx.__getitem__(index)，所以我们要想一个类可以用下标访问，只需在类中实现特殊方法__getitem__方法即可。 想要是内置的len函数正常工作，就必须在自己定制的序列类型中实现一个名叫__len__的特殊方法…… 例如要令表示二叉树节点的类，也能通过下标访问节点，并且用len能够得到长度信息。 12345678910111213141516171819class BinaryNode(object): def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = rightclass IndexableNode(BinaryNode): def _search(self, count, index): pass def __getitem__(self, index): found, _ = self._search(0, index) if not found: raise IndentationError('Index out of range') return found.value def __len__(self): _, count = self._search(0, None) return count 如果还要实现其他功能，那会是一件很麻烦的事。为了避免这些麻烦，我们可以使用内置的collections.abc模块，该模块定义了一系列的抽象基类，它们提供了每一种容器所应当具备的常用方法。从这样的基类继承子类之后，如果忘记实现某个方法，那么collections.abc模块就会指出这个错误。如果子类实现了抽象基类所要求的每个方法，那么基类就会自动提供剩下的那些方法。 123456from collections.abc import Sequenceclass BadType(Sequence): passfoo = BadType() --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-10-997f61651b50&gt; in &lt;module&gt;() 4 pass 5 ----&gt; 6 foo = BadType() TypeError: Can&#39;t instantiate abstract class BadType with abstract methods __getitem__, __len__ 12345678910111213class IndexableNode(BinaryNode, Sequence): def _search(self, count, index): pass def __getitem__(self, index): found, _ = self._search(0, index) if not found: raise IndentationError('Index out of range') return found.value def __len__(self): _, count = self._search(0, None) return count]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向传播算法]]></title>
    <url>%2F2018%2F06%2F01%2F%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[神经网络中的梯度计算主要靠正向传播（forward propagation）和反向传播（back-propagation）。 正向传播指的是对神经网络沿着从输入层到输出层的顺序，依次计算并存储模型中间变量的过程。反向传播指的是计算神经网络参数梯度的方法。总的来说，反向传播中会依据微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储损失函数有关神经网络各层的中间变量以及参数的梯度。反向传播时，计算有关各层变量和参数的梯度可能会依赖于各层变量和参数的当前值。而这些变量的当前值来自正向传播的计算结果。 为了解释正向传播和反向传播，我们以一个简单的$L_2$范数正则化的多层感知机为例。 定义模型我们以类别数为$q$的分类问题为例。给定一个特征为$\boldsymbol{x} \in \mathbb{R}^d$和标签为离散值$y$的训练数据样本。不考虑偏差项，我们可以得到中间变量 \boldsymbol{z} = \boldsymbol{W}^{(1)} \boldsymbol{x}其中$\boldsymbol{W}^{(1)} \in \mathbb{R}^{h \times d}$是模型参数。将中间变量$\boldsymbol{z} \in \mathbb{R}^h$应用按元素操作的激活函数$\phi$后，我们将得到向量长度为$h$的隐藏层变量 \boldsymbol{h} = \phi (\boldsymbol{z})隐藏层$\boldsymbol{h} \in \mathbb{R}^h$也是一个中间变量。通过模型参数$\boldsymbol{W}^{(2)} \in \mathbb{R}^{q \times h}$可以得到向量长度为$q$的输出层变量 \boldsymbol{o} = \boldsymbol{W}^{(2)} \boldsymbol{h}假设损失函数为$\ell$，我们可以计算出单个数据样本的损失项 L = \ell(\boldsymbol{o}, y)根据$L_2$范数正则化的定义，给定超参数$\lambda$，正则化项即 s = \frac{\lambda}{2} (|\boldsymbol{W}^{(1)}|_F^2 + |\boldsymbol{W}^{(2)}|_F^2)其中每个矩阵Frobenius范数的平方项即该矩阵元素的平方和。最终，模型在给定的数据样本上带正则化的损失为 J = L + s我们将$J$称为有关给定数据样本的目标函数，并在以下的讨论中简称目标函数。 模型计算图为了可视化模型变量和参数之间在计算中的依赖关系，我们可以绘制模型计算图，如图3.6所示。例如，正则化项$s$的计算依赖模型参数$\boldsymbol{W}^{(1)}$和$\boldsymbol{W}^{(2)}$。 正向传播在反向传播计算梯度之前，我们先做一次正向传播。也就是说，按照上图中箭头顺序，并根据模型参数的当前值，依次计算并存储模型中各个中间变量的值。例如，在计算损失项$L$之前，我们需要依次计算并存储$\boldsymbol{z}, \boldsymbol{h}, \boldsymbol{o}$的值。 反向传播计算图模型的参数是$\boldsymbol{W}^{(1)}$和$\boldsymbol{W}^{(2)}$。对于小批量中每个样本，我们都需要对目标函数$J$有关$\boldsymbol{W}^{(1)}$和$\boldsymbol{W}^{(2)}$的梯度求平均来迭代$\boldsymbol{W}^{(1)}$和$\boldsymbol{W}^{(2)}$。也就是说，每一次迭代都需要计算模型参数梯度$\partial J/\partial \boldsymbol{W}^{(1)}$和$\partial J/\partial \boldsymbol{W}^{(2)}$。根据计算图中的依赖关系，我们可以按照其中箭头所指的反方向依次计算并存储梯度。 为了表述方便，对输入输出$\mathsf{X}, \mathsf{Y}, \mathsf{Z}$为任意形状张量的函数$\mathsf{Y}=f(\mathsf{X})$和$\mathsf{Z}=g(\mathsf{Y})$，我们使用 \frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}})来表达链式法则。 首先，我们计算目标函数有关损失项和有关正则项的梯度 \frac{\partial J}{\partial L} = 1 \\ \frac{\partial J}{\partial s} = 1其次，我们依据链式法则计算目标函数有关输出层变量的梯度$\partial J/\partial \boldsymbol{o} \in \mathbb{R}^q$： \frac{\partial J}{\partial \boldsymbol{o}} = \text{prod}(\frac{\partial J}{\partial L}, \frac{\partial L}{\partial \boldsymbol{o}}) = \frac{\partial L}{\partial \boldsymbol{o}}接下来，我们可以很直观地计算出正则项有关两个参数的梯度： \frac{\partial s}{\partial \boldsymbol{W}^{(1)}} = \lambda \boldsymbol{W}^{(1)} \\ \frac{\partial s}{\partial \boldsymbol{W}^{(2)}} = \lambda \boldsymbol{W}^{(2)}现在，我们可以计算最靠近输出层的模型参数的梯度$\partial J/\partial \boldsymbol{W}^{(2)} \in \mathbb{R}^{q \times h}$。在计算图中，$J$分别通过$\boldsymbol{o}$和$s$依赖$\boldsymbol{W}^{(2)}$。依据链式法则，我们得到 \frac{\partial J}{\partial \boldsymbol{W}^{(2)}} = \text{prod}(\frac{\partial J}{\partial \boldsymbol{o}}, \frac{\partial \boldsymbol{o}}{\partial \boldsymbol{W}^{(2)}}) + \text{prod}(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \boldsymbol{W}^{(2)}}) = \frac{\partial J}{\partial \boldsymbol{o}} \boldsymbol{h}^\top + \lambda \boldsymbol{W}^{(2)}沿着输出层向隐藏层继续反向传播，隐藏层变量的梯度$\partial J/\partial \boldsymbol{h} \in \mathbb{R}^h$可以这样计算： \frac{\partial J}{\partial \boldsymbol{h}} = \text{prod}(\frac{\partial J}{\partial \boldsymbol{o}}, \frac{\partial \boldsymbol{o}}{\partial \boldsymbol{h}}) = {\boldsymbol{W}^{(2)}}^\top \frac{\partial J}{\partial \boldsymbol{o}}其中，激活函数$\phi$是按元素操作的。中间变量$\boldsymbol{z}$的梯度$\partial J/\partial \boldsymbol{z} \in \mathbb{R}^h$的计算需要使用按元素乘法符$\odot$： \frac{\partial J}{\partial \boldsymbol{z}} = \text{prod}(\frac{\partial J}{\partial \boldsymbol{h}}, \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}) = \frac{\partial J}{\partial \boldsymbol{h}} \odot \phi^\prime(\boldsymbol{z})最终，我们可以得到最靠近输入层的模型参数的梯度$\partial J/\partial \boldsymbol{W}^{(1)} \in \mathbb{R}^{h \times d}$。在计算图中，$J$分别通过$\boldsymbol{z}$和$s$依赖$\boldsymbol{W}^{(1)}$。依据链式法则，我们得到 \frac{\partial J}{\partial \boldsymbol{W}^{(1)}} = \text{prod}(\frac{\partial J}{\partial \boldsymbol{z}}, \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}^{(1)}}) + \text{prod}(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \boldsymbol{W}^{(1)}}) = \frac{\partial J}{\partial \boldsymbol{z}} \boldsymbol{x}^\top + \lambda \boldsymbol{W}^{(1)}需要强调的是，每次迭代中，上述各个依次计算出的梯度会被依次存储或更新。这可以用来避免重复计算某些梯度需要的值。例如，由于输出层变量梯度$\partial J/\partial \boldsymbol{o}$被计算存储，反向传播稍后的参数梯度$\partial J/\partial \boldsymbol{W}^{(2)}$和隐藏层变量梯度$\partial J/\partial \boldsymbol{h}$的计算可以直接读取输出层变量梯度的值，而无需重复计算。 正向传播和反向传播相互依赖事实上，正向传播和反向传播相互依赖。为什么这么说呢？ 一方面，正向传播的计算可能依赖于模型参数的当前值。而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。例如，计算图中，计算正则化项$s$依赖模型参数$\boldsymbol{W}^{(1)}$和$\boldsymbol{W}^{(2)}$的当前值。而这些当前值是优化算法最近一次根据反向传播算出梯度后迭代得到的。 另一方面，反向传播的梯度计算可能依赖于各变量的当前值。而这些变量的当前值是通过正向传播计算的。举例来说，参数梯度$\partial J/\partial \boldsymbol{W}^{(2)}$的计算需要依赖隐藏层变量的当前值$\boldsymbol{h}$。这个当前值是通过从输入层到输出层的正向传播计算并存储得到的。 因此，在模型参数初始化完成后，我们可以交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。 卷积网络中的反向传播卷积神经网络相比于多层感知机，增加了两种新的层次——卷积层与池化层。 池化层梯度池化层用于削减数据量，在这一层上前向传播的数据会有损失，则在反向传播时，传播来的梯度也会有所损失。一般来说，池化层没有参数，于是仅需要计算梯度反向传播的结果。 池化层的反向传播的方法是upsample，先将矩阵还原成原大小，之后： 对于最大值池化，将梯度放置于每个池化区域取得最大值的位置，其他位置为0 对于平均值池化，则把的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置 例如对于矩阵： \left( \begin{array}{} 1& 2 \\ 3& 4 \end{array} \right)假设经过$2\times2$的池化，还原为原来大小： \left( \begin{array}{ccc} 0&0&0&0 \\ 0&1& 2&0 \\ 0&3&4&0 \\ 0&0&0&0 \end{array} \right)若是最大值池化，假设每个窗口的最大值位置都是左上，则传播结果为： \left( \begin{array}{ccc} 1&0&2&0 \\ 0&0& 0&0 \\ 3&0&4&0 \\ 0&0&0&0 \end{array} \right)若是经过平均值池化，则传播结果为： \left( \begin{array}{ccc} 0.25&0.25&0.5&0.5 \\ 0.25&0.25& 0.5&0.5 \\ 0.75&0.75&1&1 \\ 0.75&0.75&1&1 \end{array} \right)卷积层梯度已知卷积层的$\delta^l$，求上一隐藏层的$\delta^l-1$ 对于卷积网络，前向传播公式为： a^l= \sigma(z^l) = \sigma(a^{l-1}*W^l +b^l)其中，上标代表层数，星号代表卷积，而 $b$ 代表偏置, $\sigma$ 为激活函数，这里一般都是ReLU。 DNN的反向传播公式为： \delta^{l} = \cfrac{\partial J(W,b)}{\partial z^l} = \cfrac{\partial J(W,b)}{\partial z^{l+1}}\cfrac{\partial z^{l+1}}{\partial z^{l}} = \delta^{l+1}\cfrac{\partial z^{l+1}}{\partial z^{l}}因此要推导出$\delta^{l-1}$和$\delta^{l}$的递推关系，必须计算$\frac{\partial z^{l}}{\partial z^{l-1}}$的梯度表达式。注意到$z^{l}$和$z^{l-1}$的关系为： z^l = a^{l-1}*W^l +b^l =\sigma(z^{l-1})*W^l +b^l因此： \delta^{l-1} = \delta^{l}\frac{\partial z^{l}}{\partial z^{l-1}} = \delta^{l}*rot180(W^{l}) \odot \sigma^{'}(z^{l-1})这里对于含有卷积的式子求导时，卷积核被旋转了180度。即式子中的rot180()，翻转180度的意思是上下翻转一次，接着左右翻转一次。在DNN中这里只是矩阵的转置。那么为什么呢？ 举一个例子来说明，对于以下卷积等式： \left( \begin{array}{ccc} a_{11}&a_{12}&a_{13} \\ a_{21}&a_{22}&a_{23}\\ a_{31}&a_{32}&a_{33} \end{array} \right) * \left( \begin{array}{ccc} w_{11}&w_{12}\\ w_{21}&w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&z_{12}\\ z_{21}&z_{22} \end{array} \right)利用卷积的定义，很容易得出： z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22} \\ z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} + a_{23}w_{22} \\ z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} + a_{32}w_{22} \\ z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} + a_{33}w_{22} \\接着我们模拟反向求导： \nabla a^{l-1} = \frac{\partial J(W,b)}{\partial a^{l-1}} = \frac{\partial J(W,b)}{\partial z^{l}} \frac{\partial z^{l}}{\partial a^{l-1}} = \delta^{l} \frac{\partial z^{l}}{\partial a^{l-1}}比如对于$a_{11}$的梯度，由于在4个等式中$a_{11}$只和$z_{11}$有乘积关系，从而： \nabla a_{11} = \delta_{11}w_{11}对于$a_{12}$的梯度，由于在4个等式中$a_{12}$和$z_{12}，z_{11}$有乘积关系，从而： \nabla a_{12} = \delta_{11}w_{12} + \delta_{12}w_{11}同样的道理我们得到： \nabla a_{13} = \delta_{12}w_{12} \\ \nabla a_{21} = \delta_{11}w_{21} + \delta_{21}w_{11} \\ \nabla a_{22} = \delta_{11}w_{22} + \delta_{12}w_{21} + \delta_{21}w_{12} + \delta_{22}w_{11} \\ \nabla a_{23} = \delta_{12}w_{22} + \delta_{22}w_{12} \\ \nabla a_{31} = \delta_{21}w_{21} \\ \nabla a_{32} = \delta_{21}w_{22} + \delta_{22}w_{21} \\ \nabla a_{33} = \delta_{22}w_{22} \\这上面9个式子可以用一个矩阵卷积的形式表示，即： \left( \begin{array}{ccc} 0&0&0&0 \\ 0&\delta_{11}& \delta_{12}&0 \\ 0&\delta_{21}&\delta_{22}&0 \\ 0&0&0&0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&w_{21}\\ w_{12}&w_{11} \end{array} \right) = \left( \begin{array}{ccc} \nabla a_{11}&\nabla a_{12}&\nabla a_{13} \\ \nabla a_{21}&\nabla a_{22}&\nabla a_{23}\\ \nabla a_{31}&\nabla a_{32}&\nabla a_{33} \end{array} \right)为了符合梯度计算，我们在误差矩阵周围填充了一圈0，此时我们将卷积核翻转后和反向传播的梯度误差进行卷积，就得到了前一次的梯度误差。这个例子直观的介绍了为什么对含有卷积的式子求导时，卷积核要翻转180度的原因。 已知卷积层的$\delta^l$，推导该层的$W,b$的梯度现在已经可以递推出每一层的梯度$\delta^l$了，对于全连接层，可以按DNN的反向传播算法求该层$W,b$的梯度，而池化层并没有$W,b$，也不用求$W,b$的梯度。只有卷积层的$W,b$需要求出。 注意到卷积层z和W,b的关系为： z^l = a^{l-1}*W^l +b因此我们有： \frac{\partial J(W,b)}{\partial W^{l}} = \frac{\partial J(W,b)}{\partial z^{l}}\frac{\partial z^{l}}{\partial W^{l}} =\delta^l*rot180(a^{l-1})而对于$b$,则稍微有些特殊，因为$\delta^l$是三维张量，而$b$只是一个向量，不能像DNN那样直接和$\delta^l$相成。通常的做法是将$\delta^l$的各个子矩阵的项分别求和，得到一个误差向量，即为$b$的梯度： \frac{\partial J(W,b)}{\partial b^{l}} = \sum\limits_{u,v}(\delta^l)_{u,v}参考文献[1]. 动手深度学习[2]. 卷积神经网络(CNN)反向传播算法]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>反向传播</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K近邻法]]></title>
    <url>%2F2018%2F05%2F31%2FK%E8%BF%91%E9%82%BB%E6%B3%95%2F</url>
    <content type="text"><![CDATA[k近邻算法简单，直观：给定一个训练数据集，对新的输入实例，在训练集中找到与该实例最邻近的k个实例，对于分类问题，这k个实例的多数属于某个类，就把该输入实例分为这个类；对于回归问题，将这k个实例的平均输出值作为输出。 K近邻模型由三个基本要素组成：距离度量，k值选择，分类决策规则。 距离度量k近邻模型的特征空间一般是 $n$ 维实数向量空间 $R^n$。使用的距离是欧氏距离，但也可以是其它距离。设特征空间 $X$ 是 $n$ 维实数向量空间 $R^n，x_i,x_j \in X, x_i = (x_i^{(1)},x_i^{(2)},…x_i^{(n)}),x_j = (x_j^{(1)},x_j^{(2)},…x_j^{(n)})$，$x_i,x_j$ 的 $L_p$ 距离定义为 L_p(x_i,x_j) = \left( \sum_{l=1}^n \vert{x_i^{(l)}-x_j^{(l)} \vert}^p \right)^{ \frac1p}这里 $p \ge1$。当 $p = 2$ 时称为欧氏距离，即 L_2(x_i,x_j) = \left( \sum_{l=1}^n \vert{x_i^{(l)}-x_j^{(l)} \vert}^2 \right)^{ \frac12}当 $p = 1$ 时，称为曼哈顿距离，即 L_1(x_i,x_j) = \sum_{l=1}^n \vert{x_i^{(l)}-x_j^{(l)} \vert}当 $p = \infty$ 时，它是各个坐标距离的最大值。 K值选择k值得选择会对k近邻算法的结果产生重大影响。如果选择的k值较小，就相当于用较小的的邻域中的训练实例进行预测。此时预测的结果会对近邻的实例点非常敏感。 如果选择较大的k值，就相当于在较大的邻域中训练实例进行预测。此时，与输入实例较远的训练实例也会对预测起作用，使预测发生错误。 如果k等于训练样本个数，此时将输入实例简单的预测为训练样本中最多的类。这时模型过于简单，会完全忽略训练样本中的大量有用信息，是不可取的。 在应用中，k值一般选取一个比较小的数值，通常采用交叉验证法来选取最优的k值。 分类决策规则k近邻算法中分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。 参考文献[1]. 《统计学习方法》 李航]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>K近邻法</tag>
        <tag>欧氏距离</tag>
        <tag>曼哈顿距离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成方法之Bagging与随机森林]]></title>
    <url>%2F2018%2F05%2F31%2F%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%E4%B9%8BBagging%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[在《集成方法概述》一节中，我们知道要想获得泛化性能强的集成，集成中的个体学习器应尽可能相互独立。而“独立”在现实任务中比较难以做到，不过我们可以设法使基学习器尽可能具有较大的差异。给定一个训练集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器，这样，由于训练数据不同，我们获得的基学习器可望具有比较大的差异。然而，为获得好的集成，我们同时希望个体学习器不能太差。如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不能进行有效的学习，更不谈确保产生比较好的基学习器了。为了解决这个问题，我们使用相互有交叠的采样子集。 BaggingBagging基于自助采样法（bootstrap sampling）。给定包含 $m$ 个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过 $m$ 次随机采样操作，我们得到含 $m$ 个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。 对于一个样本，它在某一次含 $m$ 个样本的训练集的随机采样中，每次被采集到的概率是 $\frac{1}{m}$。不被采集到的概率为 $1-\frac{1}{m}$。$m$ 次采样都没有被采集中的概率是 $(1-\frac{1}{m})^m$。当 $m \to \infty$ 时，$(1-\frac{1}{m})^m \to \frac{1}{e} \simeq 0.368$。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。 对于这部分大约36.8%的没有被采样到的数据，我们常常称之为包外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。可知，初始训练集中大约有63.2%的样本出现在采样集中。 Bagging对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是决策树和神经网络。在对预测输出进行结合时，Bagging通常对分类任务采用简单投票法，对回归任务使用简单平均法。若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者。 从偏差-方差分解的角度看，Bagging主要关注降低方差（防止过拟合），因此它在不剪枝决策树、神经网络等容易受样本扰动的学习器上效用更为明显。 算法流程如下： 输入为样本集 $D=\{(x_,y_1),(x_2,y_2), …(x_m,y_m)\}$，弱学习器算法, 弱分类器迭代次数 $T$。 输出为最终的强分类器$f(x)$ 对于 $t=1,2…,T$:a. 对训练集进行第 $t$ 次随机采样，使用自助采样法共采集 $m$ 次，得到包含 $m$ 个样本的采样集 $D_t$。b. 用采样集 $D_t$ 训练第t个弱学习器 $G_t(x)$。 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。 随机森林随机森林（Random Forest，简称RF）是Bagging的一个扩展变体。RF使用了CART决策树作为弱学习器。 在决策树的训练过程中，引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前结点的属性集合（假定有 $d$ 个属性）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 $k$ 个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数 $k$ 控制了随机性的引入程度：若令 $k=d$，则基决策树的构建与传统决策树相同；若令 $k=1$，则是随机选择一个属性用于划分；一般情况下，推荐值 $k=log_2d$。 RF简单、容易实现、计算开销小，而令人惊奇的是，它在很多学习任务中展现出强大的性能，被誉为“代表集成学习技术水平的方法”。RF中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过个体学习器之间的差异度的增加而进一步提升。 RF的收敛性与Bagging相似。如下图所示，随机森林的起始性能往往相对较差，特别是在集成中只包含一个基学习器时，这很容易理解，因为通过引入属性扰动，随机森林中个体学习器的性能往往有所降低。然而，随着个体学习器数目的增加，随机森林通常会收敛到更低的泛化误差。 参考文献[1]. 《机器学习》 周志华[2]. Bagging与随机森林算法原理小结[3]. 集成学习（Ensemble Learning）]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成方法</tag>
        <tag>Bagging</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成方法之XGboost]]></title>
    <url>%2F2018%2F05%2F30%2F%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%E4%B9%8BXgboost%2F</url>
    <content type="text"><![CDATA[XGboost由陈天奇大牛开发，它是目前最快最好的开源Boosting包，比常见的工具包快大概10倍。在数据科学方面，有大量kaggle选手选用它进行数据挖掘比赛，其中包括两个以上kaggle比赛的夺冠方案。在工业界，xgboost的分布式版本有广泛的可移植性，支持在YARN, MPI, Sungrid Engine等各个平台上面运行，并且保留了单机并行版本的各种优化，使得它可以很好地解决于工业界规模的问题。 算法原理 基学习器：CART首先讲模型。Boosted tree 最基本的组成部分叫做回归树(regression tree)。我们可以简单地把它理解为decision tree的一个扩展，从简单的类标到分数。 Tree Ensemble对于tree ensemble，模型是： \hat y_i = \sum_{k=1}^K f_k(x_i), \quad f_k \in \mathcal{F}其中每个 $f$ 是一个在函数空间 $\mathcal{F}$ 里面的函数，也就是表示具体的一棵CART树，$K$是树的棵数。而 $\mathcal{F}$ 对应了所有regression tree的集合。 一般目标函数包括误差函数 $L(\theta)$ 和正则化项 $\Omega$： Obj(\theta) = L(\theta) + \Omega(\theta)具体到我们设计的目标函数，包含两部分： \begin{align*} J(f_t) &= \sum_{i=1}^n L(y_i,\hat y_i) +\sum_{k=1}^K\Omega(f_k) \\ &= \sum_{i=1}^nL(y_i, \hat y_i^{(t-1)} + f_t(x_i)) + \Omega(f_t) + constant \end{align*}根据Taylor展开式： $$ f\left( x \right) = f\left( {{x_0}} \right) + f'\left( {{x_0}} \right)\left( {x - {x_0}} \right) + \frac{1}{2} f''\left( {{x_0}} \right){\left( {x - {x_0}} \right)^2} $$ 我们对 $l(y_i, Y)$ 在 ${\hat y}^{(t-1)}_i$处进行二阶展开得到 $$ l({y_i}, Y) = l\left( {{y_i},\hat y_i^{(t-1)}} \right) + {{\partial l\left( {{y_i},\hat y_i^{(t-1)}} \right)} \over {\partial \hat y_i^{(t-1)}}}\left( {Y - \hat y_i^{(t-1)}} \right) + \frac{1}{2} {{{\partial ^2}l\left( {{y_i},\hat y_i^{(t-1)}} \right)} \over {\partial \hat y_i^{(t-1)}}}{\left( {Y - \hat y_i^{(t-1)}} \right)^2} $$ 令 Y=\hat y_i^{(t-1)}+f_t({x}_i) \\ g_i= \frac{\partial L(y_i,\hat y_i^{(t-1)})}{\partial \hat y_i^{(t-1)}} \\ h_i = \frac{\partial^2 L(y_i,\hat y_i^{(t-1)})}{\partial \hat y_i^{(t-1)}}我们得到 $l(y_i, {\hat y}^{(t-1)}+f_t({x}_i))$ 的二阶泰勒展开： $$ l\left( {{y_i},\hat y_i^{(t-1)}} \right) + {g_i}{f_t}\left( {{{{x}}_i}} \right) + {1 \over 2}{h_i}f_t^2\left( {{{{x}}_i}} \right) $$ 带入目标函数可得： J(f_t) \approx \sum_{i=1}^n \left[L(y_i, \hat y_i^{(t-1)}) + g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)\right] + \Omega(f_t) + C把常数项移除，则会得到如下一个比较统一的目标函数。这一个目标函数有一个非常明显的特点，它只依赖于每个数据点的在误差函数上的一阶导数和二阶导数。 J(f_t) \approx \sum_{i=1}^n \left[g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)\right] + \Omega(f_t) 有一个小问题，在优化第$t$棵树时，有多少个$g_i$和$h_i$要计算？嗯，没错就是各有$N$个，$N$是训练样本的数量。如果有10万样本，在优化第$t$棵树时，就需要计算出个10万个$g_i$和$h_i$。这10万个gi之间没有关系，所以可以并行计算。为什么xgboost会那么快!!! $g_i$和$h_i$是不依赖于损失函数的形式的，只要这个损失函数二次可微就可以了。这有什么好处呢？好处就是xgboost可以支持自定义损失函数，只需满足二次可微即可。强大了我的哥!!! 树的复杂度到目前为止我们确定了目标函数中训练误差的部分。接下来我们讨论如何定义树的复杂度。我们先对于 $f$ 的定义做一下细化，把树拆分成结构部分 $q$ 和叶子权重部分 $w$。假定某决策树的叶结点数目为 $T$，每个叶结点的权值为 $\vec w = (w_1,w_2…w_T)$。决策树的学习过程，就是构造如何使用特征得到划分，从而得到这些权值的过程。样本 $x$ 落在叶结点 $q$ 中，定义 $f$ 为： f_t(x) = w_{q(x)}下图是一个具体的例子。结构函数 $q$ 把输入映射到叶子的索引号上面去，而 $w$ 给定了每个索引号对应的叶子分数是什么。 当我们给定了如上定义之后，我们可以定义一棵树的复杂度如下 \Omega(f_t) = \gamma {T_t} + \frac{1}{2}\lambda \sum_{j=1}^{T_t} w_j^2其中 $T_t$ 为叶结点数，$w_j$ 为 $j$ 叶子结点权重。这个复杂度包含了一棵树里面节点的个数，以及每个树叶子节点上面输出分数的 $L2$ 模平方。当然这不是唯一的一种定义方式，不过这一定义方式学习出的树效果一般都不错。 这里出现了$\lambda$和$\gamma$，是xgboost自己定义的，在使用xgboost时，你可以设定它们的值，显然，$\gamma$越大，表示越希望获得结构简单的树，因为此时对较多叶子节点的树的惩罚越大。$\lambda$越大也是越希望获得结构简单的树。 下图还给出了复杂度计算的一个例子。 关键步骤目标函数的计算： \begin{align*} J(f_t) & \approx \sum_{i=1}^n \left[g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)\right] + \Omega(f_t) \\ &= \sum_{i=1}^n \left[g_i w_{q(x_i)} + \frac{1}{2}h_i w_{q(x_i)}^2\right] + \gamma {T_t} + \frac{1}{2}\lambda \sum_{j=1}^{T_t} w_j^2 \\ &=\sum_{j=1}^{T_t} \left[(\sum_{i \in I_j}g_i)w_j + \frac{1}{2}(\sum_{i \in I_j}h_i)w_j^2\right] +\gamma {T_t} + \frac{1}{2}\lambda \sum_{j=1}^{T_t} w_j^2 \\ &=\sum_{j=1}^{T_t} \left[(\sum_{i \in I_j}g_i)w_j + \frac{1}{2}(\sum_{i \in I_j}h_i + \lambda)w_j^2\right] +\gamma {T_t} \end{align*} $I_j$代表什么？它代表一个集合，集合中每个值代表一个训练样本的序号，整个集合就是被第t棵CART树分到了第j个叶子节点上的训练样本。 这一个目标包含了 $T_t$ 个相互独立的单变量二次函数，定义： G_j = \sum_{i \in I_j}g_i \\ H_j = \sum_{i \in I_j}h_i从而 J(f_t) =\sum_{j=1}^{T_t} \left[G_jw_j + \frac{1}{2}(H_j + \lambda)w_j^2\right] +\gamma {T_t}对$w$求偏导，得： \frac{\partial J(f_t)}{\partial w_j} = G_j + (H_j + \lambda) w_j令偏导等于0，得到： w_j = - \frac {G_j}{H_j + \lambda}代回目标函数，得 J(f_t) = -\frac{1}{2} \sum_{j=1}^{T_t}\frac{G_j^2}{H_j+\lambda} + \gamma {T_t}打分函数计算举例$J(f_t)$ 代表了当我们指定一个树结构的时候，我们在目标上面最多减少多少。我们可以把它叫做结构分数(structure score)。可以认为这个就是类似吉尼系数一样更加一般的对于树结构进行打分的函数。下面是一个具体的打分函数计算的例子: 枚举所有不同树结构的贪心算法实践中，很难去穷举每一颗树进行打分，再选出最好的。通常采用贪心的方式。按照下图从左至右扫描，我们就可以找出所有的切分点。对于一个具体的分割方案，我们可以获得的增益可以由如下公式计算： Gain = \frac{1}{2} \left[ {{{G_L^2} \over {{H_L} + \lambda }} + {{G_R^2} \over {{H_R} + \lambda }} - {{{(G_L + G_R)^2}} \over {H_L + H_R + \lambda }}} \right] - \gamma 其中，${G_L^2} \over {H_L} + \lambda$ 是左子树的分数，${G_R^2} \over {H_R} + \lambda$ 是右子树的分数，${(G_L + G_R)^2} \over {H_L + H_R + \lambda }$ 是不分割时的分数，$\gamma$ 是加入新叶子节点引入的复杂度代价，实际上是一个临界值，它的值越大，表示我们对切分后obj下降幅度要求越严。 扫描结束后，我们就可以确定是否切分，如果切分，对切分出来的两个节点，递归地调用这个切分过程，我们就能获得一个相对较好的树结构。 对于每次扩展，我们还是要枚举所有可能的分割方案，如何高效地枚举所有的分割呢？我假设我们要枚举所有 $x&lt;a$ 这样的条件，对于某个特定的分割 $a$ 我们要计算 $a$ 左边和右边的导数和。如下图所示： 可以发现对于所有的 $a$，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和$G_L$和$G_R$。然后用上面的公式计算每个分割方案的分数就可以了。 观察这个目标函数，会发现第二个值得注意的事情就是引入分割不一定会使得情况变好，因为我们有一个引入新叶子的惩罚项。优化这个目标对应了树的剪枝， 当引入的分割带来的增益小于一个阀值的时候，我们可以剪掉这个分割。xgboost在切分的时候就已经考虑了树的复杂度。所以，它不需要进行单独的剪枝操作。可以发现，当我们正式地推导目标的时候，像计算分数和剪枝这样的策略都会自然地出现，而不再是一种因为heuristic而进行的操作了。 参考文献 [1] XGBoost 与 Boosted Tree[2] XGboost: A Scalable Tree Boosting System论文及源码导读[3] XGBoost/GBDT相关blog推荐[4] xgboost的原理没你想像的那么难]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成方法</tag>
        <tag>GBDT</tag>
        <tag>XGboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成方法之GBDT]]></title>
    <url>%2F2018%2F05%2F29%2F%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%E4%B9%8BGBDT%2F</url>
    <content type="text"><![CDATA[提升方法实际采用加法模型与前向分步算法。提升树通常决策树作为基学习器，对分类问题决策树是二叉分类树，回归问题就是二叉回归树。提升树被认为是统计学习中性能最好的方法之一。提升树模型可以表示为决策树的加法模型 f_M (x)=\sum_{m=1}^MT(x;Θ_m )其中，$T(x;Θ_m)$表示决策树；$Θ_m$表示决策树的参数；$M$为树的个数。提升树的前向分步算法，首先确定初始提升树$f_0(x)=0$，第$m$步的模型可以写成： f_m(x)=f_{m−1}(x)+T(x;Θ_m)然后得到损失函数 L(f_m(x),y)=L(f_{m−1}(x)+T(x;Θ_m),y)迭代的目的是构建$T(x;Θ_m)$，使得本轮损失$L(f_m(x),y)$最小。通过经验风险最小化确定$Θ_m$ Θ_m = arg\min_{Θ_m}\sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i; Θ_m)思想其实并不复杂，但是问题也很明显，对于不同的任务会有不同的损失函数，包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。 二分类问题对于二类分类问题，提升树算法只需将AdaBoost算法中的基本分类器限制为二类分类树即可，可以说是AdaBoost算法的特殊情况。 回归问题对于回归问题，可看做是把输入空间 $\chi$ 划分为 $J$ 个不相交的区域$R_1, R_2,…,R_J$，在每个区域上输出的常量为 $C_j$ ，那么树可以表示为： T(x; Θ)=\sum_{j=1}^J C_j I(x \in R_j)其中$Θ=\{(R_1, C_1), (R_2, C_2),…,(R_J, C_J)\}$表示树的区域划分和各区域上的常数，$J$表示区域的个数。对于回归树的前向分步式算法有： f_Θ(x)=0 \\ f_m(x)=f_{m-1}(x)+T(x; Θ), m= 1, 2, ..., M \\ f_M(x)=\sum_{m=1}^M T(x;Θ_m)在前向分步式算法的第$m$步，给定$f_{m-1}(x)$，需要求解： Θ_m = arg\min_{Θ_m}\sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i; Θ_m)得到第$m$颗树的参数。这里采用平方误差拟合回归问题，则： L(y,f(x)=(y-f(x))^2其损失变为： \begin{align*} &L(y, f_{m-1}(x)+T(x; Θ_m)) \\ &=[y-f_{m-1}(x)-T(x; Θ_m)]^2 \\ &=[r-T(x; Θ_m)]^2 & \\ \end{align*}其中 r=y-f_{m-1}(x)是当前拟合的残差，所以对回归问题的提升树来说只需要简单的拟合当前模型的残差。 整个算法的步骤： 初始化$f_0(x)=0$ 对$m=1,2,..,M$:a. 计算残差$r=y-f_{m-1}(x)$b. 拟合残差，学习一个回归树，得到$T(x; Θ_m)$c. 更新$f_m(x)=f_{m-1}(x)+T(x; Θ_m)$ 得到回归提升树$f_M(x)=\sum_{m=1}^M T(x; Θ_m)$ 梯度提升GBDT当损失函数是平方损失和指数损失函数时，提升树每一步优化很简单。但对于一般的损失函数，往往每一步的优化并不是那么容易，针对这一问题，Freidman提出了梯度提升(gradient boosting)算法。这是利用最速下降法的近似方法，关键在于利用损失函数的负梯度在当前模型的值 -\Biggl[{\partial L(y,f(x_i)) \over \partial f(x_i)}\Biggr]_{f(x)=f_{m-1}\;(x)}作为近似残差，拟合一个回归树。 为什么可以用负梯度近似残差？观察式子： \sum_{i=1}^N L(y_i, f_{m-1}(x_i)+\beta b(x_i))我们要最小化的式子由N部分相加而成，如果能够最小化每一部分，自然也就最小化了整个式子。考察其中任一部分，并将其进行泰勒一阶展开（这是关键所在！）： L(y_i, f_{m-1}(x_i)+\beta b(x_i)) = L\Biggl((y_i, f_{m-1}(x_i)) + \beta b(x_i){\partial L(y_i,{f_{m-1}\;(x_i)}) \over \partial {f_{m-1}\;(x_i)}}\Biggl)如由于$\beta$是大于0的，若： b(x_i) = {- \partial L(y_i,{f_{m-1}\;(x_i)}) \over \partial {f_{m-1}\;(x_i)}}不难得出： L(y_i, f_{m-1}(x_i)+\beta b(x_i))]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成方法</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成方法之Adaboost]]></title>
    <url>%2F2018%2F05%2F28%2F%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%E4%B9%8BAdaboost%2F</url>
    <content type="text"><![CDATA[本文介绍集成方法中的Adaboost。 Adaboost总体思想 多专家判断。 提高那些被前一轮弱分类器误分样本的权值，降低被正确分类样本的权值。 针对多个弱分类器，AdaBoost采用加权多数表决的方法，即加大分类误差率小的弱分类器的权值，使其在分类表决中起较大的作用。 Adaboost算法流程假设给定一个二分类的训练数据集为$T=\left\{ \left( x_{1},y_{1} \right),\left(x_{2},y_{2} \right) ,…,\left( x_{N},y_{N} \right) \right\}$，其中$y_{i}$属于二分类的标记组合，即$y_{i}\in \left\{ +1,-1 \right\}$，整个算法流程如下： 初始化每个训练样例的权值 D_{1}=\left( w_{11},w_{12},...w_{1N} \right) ,w_{1i}=1/N,i=1,2,...,N 对于$m=1,2,…,M$： a. 使用权值分布为$D_{m}$的训练样例学习得到基分类器$G_{m}$ b. 计算基分类器的误差率$e_{m}$ e_{m}=P(G_{m}(x_{i})\ne y_{i})=\frac{\sum_{i=1}^{n}{w_{mi}I(G_{m}(x_{i})\ne y_{i})} }{\sum_{i=1}^{n}{w_{mi}} } =\sum_{i=1}^{n}{w_{mi}I(G_{m}(x_{i})\ne y_{i})} （已知$\sum_{i=1}^{n}{w_{mi}} =1$） c. 计算$G_m(x)$的系数$\beta _m$ \beta _m=\frac{1}{2}log\frac{1-e_m}{e_m} 由该式可知，当$e_m\leq \frac{1}{2}$ 时，$\beta_m \geq 0$，且$\beta _m$随着$e_m$的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。 d. 更新训练样例的权值分布： D_{m+1}=\left( w_{m+1,1},w_{m+1,2},...w_{m+1,n} \right) \\ w_{m+1,i}=\frac{w_{mi}}{Z_m} exp(-\beta _my_iG_m(x_i)),i=1,2,...,n 在$w_{m+1,i}$的公式中，引入了规范化因子$Z_m$ Z_m=\sum_{i=1}^{n}{w_{mi}exp(-\beta _my_iG_m(x_i))} 它的作用在于使$D_{m+1}$成为一个概率分布。 构建基分类器的线性组合$f(x)$，得到最终分类器$G(x)$ f(x)=\sum_{m=1}^{M}{\beta _mG_m(x)} \\ G(x)=sign(f(x))=sign(\sum_{m=1}^{M}{\beta _mG_m(x)} ) 前向分步算法Adaboost使用前向分步算法，以下是前向分步算法的流程： 初始化$f_0(x) = 0$ 对于$m=1,2,…,M$(1). 极小化损失函数, 得到$\beta_m, \gamma_m$，$N$为样本数量 (\beta_m,\gamma_m) = \arg\min_{\beta,\gamma} \sum_{i=1}^N L(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))(2). 更新 f_m(x) = f_{m-1}(x) + \beta_m b(x;\gamma_m) 得到加法模型 f(x) = f_M(x) = \sum_{m=1}^M \beta_m b(x;\gamma_m) Adaboost的前向分步算法AdaBoost算法是前方分步算法的特例，这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。 求指数损失函数下的模型Loss令前向分步算法的损失函数为指数损失函数 L(y,f(x)) = exp(-y f(x))基函数为: b(x;\gamma)=G(x)，G_m(x) \in \lbrace-1,1\rbrace则其在指数损失函数的基础上，解决以下问题： (\beta_m,G_m) = \arg\min_{\beta,G} \sum_{i=1}^N exp[-y_i(f_{m-1}(x_i)+\beta G(x_i))] 求具体目标函数令 $\overline w_{mi} = exp(-y_i f_{m-1}(x_i))$，可以看出$\overline w_{mi}$既不依赖$\beta$也不依赖$G$，所以与最小化无关，但其以来与$f_{m-1}$，随着每一轮迭代而变化，上述公式可以写成 (\beta_m,G_m) = \arg\min_{\beta,G} \sum_{i=1}^N \overline w_{mi} exp(-\beta y_i G(x_i))因为 $y_i \in \lbrace-1,1\rbrace$，且 $y_i$ 要么等于 $G(x_i)$，要么不等于。所以将上述公式拆成两部分。暂时省略 $\arg\min$ 之前的部分，$exp$简写成$e$，有 e^{-\beta} \sum_{y_i=G(x_i)} \overline w_{mi} + e^{\beta} \sum_{y_i \ne G(x_i)} \overline w_{mi}在这基础上再添上两项，有 e^{-\beta} \sum_{y_i=G(x_i)} \overline w_{mi} + e^{\beta} \sum_{y_i \ne G(x_i)} \overline w_{mi} + e^{-\beta} \sum_{y_i \ne G(x_i)} \overline w_{mi} - e^{-\beta} \sum_{y_i \ne G(x_i)} \overline w_{mi}再进一步合并，得到 (e^{\beta} - e^{-\beta}) \sum_{i=1}^N \overline w_{mi} I(y_i \ne G(x_i)) + e^{-\beta} \sum_{i=1}^N \overline w_{mi} \tag 1 求$G_m(x)$对于迭代的第m步，假设 $\beta$ 为常数，那么公式的右边以及 $(e^{\beta}-e^{-\beta})$都可以看成常数，则要让损失函数取得最小值，只需要让 $\sum_{i=1}^N w_i I(y_i \ne G(x_i))$ 取最小值。因此有 G_m = \arg\min_G \sum_{i=1}^N w_i^{(m)} I(y_i \ne G(x_i)) 求$\beta_m$现在假设 $G_m$ 已知的情况下，回到公式(1)。此时的变量为 $\beta$，要让损失函数取得最小值，先对$\beta$求偏导，得到 \frac {\partial_L} {\partial_{\beta}} = e^{\beta} \sum_{i=1}^N \overline w_{mi} I(y_i \ne G(x_i)) + e^{-\beta} \sum_{i=1}^N\overline w_{mi} I(y_i \ne G(x_i)) - e^{-\beta} \sum_{i=1}^N \overline w_{mi}再令 $\frac {\partial_L} {\partial_{\beta}} = 0$，得 e^{\beta} \sum_{i=1}^N \overline w_{mi} I(y_i \ne G(x_i)) = [\sum_{i=1}^N \overline w_{mi} - \sum_{i=1}^N \overline w_{mi} I(y_i \ne G(x_i))] e^{-\beta}对两边同求$log$，得到 log \sum_{i=1}^N\overline w_{mi} I(y_i \ne G(x_i)) + log e^{\beta} = log [\sum_{i=1}^N \overline w_{mi} - \sum_{i=1}^N \overline w_{mi} I(y_i \ne G(x_i))] + log e^{-\beta}又因为 $log e^{-\beta} = -log e^{\beta}$，所以有 log e^{\beta} = \frac {1} {2} log \frac {\sum_{i=1}^N \overline w_{mi} - \sum_{i=1}^N \overline w_{mi} I(y_i \ne G(x_i))} {\sum_{i=1}^N \overline w_{mi} I(y_i \ne G(x_i))}解得 \beta_m = \frac {1} {2} log \frac {\sum_{i=1}^N \overline w_{mi} - \sum_{i=1}^N \overline w_{mi} I(y_i \ne G(x_i))} {\sum_{i=1}^N w_i I(y_i \ne G(x_i))}又因为加权误差率 err_m = \frac {\sum_{i=1}^N \overline w_{mi} I(y_i \ne G(x_i))} {\sum_{i=1}^N \overline w_{mi}}所以$\beta_m$可以写成 \beta_m = \frac {1} {2} log \frac {1 - err_m} {err_m} 更新$f(x)$求出了 $G_m(x)$ 与 $\beta_m$ ，就可以写出$f(x)$的更新公式了 f_m(x) = f_{m-1}(x) + \beta_m G_m(x) 更新样本权重根据 $\overline w_{mi} = exp(-y_i f_{m-1}(x_i))$，可以写出$w$的更新公式 \overline w_{m,i+1} = exp(-y_i f_m (x_i)) \\ = exp(-y_i (f_{m-1}(x_i)+\beta_m G_m(x_i))) \\ = \overline w_{mi} exp(- \beta_m y_i G_m(x_i)) References: [1]. 《统计学习方法》 李航[2]. 从前向分步算法推导出AdaBoost]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成方法</tag>
        <tag>Adaboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】集成方法概述]]></title>
    <url>%2F2018%2F05%2F27%2F%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[集成学习(ensemble learning)通过将多个单个学习器集成/组合在一起，使它们共同完成学习任务， 从个体学习器的角度来说，集成学习可分为两种，同质集成和异质集成。 同质集成所有的个体学习器都是一个种类的。比如都是决策树个体学习器，或者都是神经网络个体学习器。 异质集成是所有的个体学习器不全是一个种类的。比如我们有一个分类问题，对训练集同时采用支持向量机个体学习器、逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。 如何保证集成有效如何保证整体的效果会比最好的那个单一学习器的效果更好呢？用一个简单的例子来进行说明：在一个二分类任务中，假设三个分类器在三个测试样本上的表现如下图所示。假设集成学习的结果通过三个个体学习器用投票产生，即“少数服从多数”，那么当三个个体学习器分别对三个测试例有不同的判别优势时，集成的效果也会不一样。 在（a）图中，每个分类器原本只有66.6%的精度，集成学习却达到了100%；（b）图中，每个分类器都是一样的，集成之后性能没有任何提高；在（c）图中，每个分类器的精度只有33.3%，集成之后结果反而变得更糟。 这个例子表明：要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的准确性，即学习器不能太坏，并且要有“多样性”（diversity），即学习器间具有差异。 为什么有效假设有一个二分类问题，其中 $y\in\{-1,+1\}$，以及真实函数 $f$，假定及分类器的错误率为 $\epsilon$，即对于每个基分类器 $h_i$ 有： P(h_i(x)\neq f(x))=\epsilon。假设集成通过简单投票法结合 $T$（为简化讨论，假设$T$为奇数）个基分类器，若有超过半数的基分类器正确，则集成分类就正确： H(x)=sign\left(\Sigma_{i=1}^T h_i(x) \right)。假设基分类器的错误率相互独立，则由Hoeffding不等式可知，集成的错误率为： P(H(x)\neq f(x))=\Sigma_{k=1}^{[T/2]}C_T^k(1-\epsilon)^k\epsilon^{T-k}\le exp(-\frac{1}{2}T(1-2\epsilon)^2)上式表明，随着集成个体分类器数目 $T$ 的增大，集成的错误率将指数级下降，最终趋向于0。 上述式子的推导是基于一个关键假设：基学习器的误差相互独立。然而现实情况是，个体学习器都是为解决同一个问题训练出来的，它们之间显然不可能相互独立。而事实上，个体学习器的“准确性”和“多样性”本身就存在冲突。一般的，准确性很高之后，要增加多样性就需牺牲准确性。而如何产生并结合“好而不同”的个体学习器，恰是集成学习研究的核心。 根据个体学习器生成方式的不同，目前集成学习方法大致可分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting，后者的代表是和Bagging和“随机森林”（Random Forest）。 BaggingBagging的弱学习器之间没有依赖关系，可以并行生成，我们可以用一张图做一个概括如下： 从上图可以看出，bagging的个体弱学习器的训练集是通过随机采样得到的。通过T次的随机采样，我们就可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。 这里的随机采样一般采用的是自助采样法（Bootstap sampling），即对于m个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，初始训练集中大概有$63.2\%$的样本出现在采样集中。由于是随机采样，这样每次的采样集和其他采样集不同，这样得到多个不同的弱学习器。 随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。 Boostingboosting的算法原理我们可以用一张图做一个概括如下： 从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器，根据弱学习的学习误差率表现来调整训练样本的权重，使得之前弱学习器学习误差率高的训练样本点在后续得到更多关注。然后基于调整权重后的训练集来训练下一个弱学习器。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器加权集合，得到最终的强学习器。 Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。 结合策略我们假定我得到的T个弱学习器是 $\{h_1,h_2,…h_T\}$。 平均法对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干个弱学习器的输出进行平均得到最终的预测输出。 最简单的平均是算术平均，也就是说最终预测是 H(x) = \frac{1}{T}\sum\limits_{1}^{T}h_i(x)如果每个个体学习器有一个权重 $w$，则最终预测是 H(x) = \sum\limits_{i=1}^{T}w_ih_i(x)其中 $w_i$ 是个体学习器 $h_i$ 的权重，通常有 w_i \geq 0 ,\;\;\; \sum\limits_{i=1}^{T}w_i = 1投票法最简单的投票法是相对多数投票法，也就是我们常说的少数服从多数，也就是 $T$ 个弱学习器的对样本x的预测结果中，数量最多的类别 $c_i$ 为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。 稍微复杂的投票法是绝对多数投票法，也就是我们常说的要票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数。否则会拒绝预测。 更加复杂的是加权投票法，和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。 学习法上两节的方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法，对于学习法，代表方法是stacking，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。 在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。 参考文献本文转自集成学习原理小结，并且参考《统计学习方法》 李航和《机器学习》 周志华]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL基础]]></title>
    <url>%2F2018%2F05%2F27%2FSQL%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[主要记录一些平时积累的SQL知识。 SQL经典问题一个SQL查询出每门课程的成绩都大于80的学生姓名 id name subject score 1 张三 语文 81 2 张三 数学 99 3 李四 语文 77 4 李四 数学 80 5 王二 英语 89 6 王二 数学 88 7 赵武 英语 77 8 赵武 数学 88 1234567891011121314151617select distinct name from Student where name not in(select distinct name from Student where score &lt; 80);select distinct A.name from Student A where not exists (select B.name from Student B where B.score &lt; 80 and A.name=B.name );select S.namefrom Student Sgroup by S.namehaving min(S.score) &gt;= 80 1234567select * from Awhere id in(select id from B);select a.* from A awhere exists(select b.* from B b where a.id=b.id) in()只执行一次，它查出B表中的所有id字段并缓存起来。之后,检查A表的id是否与B表中的id相等，如果相等则将A表的记录加入结果集中，直到遍历完A表的所有记录，当B表数据较大时不适合使用in()，因为它会B表数据全部遍历一次。 exists()会执行A.length次,它并不缓存exists()结果集,将主查询的数据，放到子查询中做条件验证，根据验证结果（TRUE 或 FALSE）来决定主查询的数据结果是否得以保留。当B表比A表数据大时适合使用exists()，因为它没有那么遍历操作，只需要再执行一次查询就行。 表中有重复项，请删除 id name class subject score 1 王二 2 英语 89 2 王二 2 数学 88 3 赵武 2 英语 77 4 赵武 2 数学 88 5 赵武 2 语文 89 6 赵武 2 语文 89 7 王二 2 数学 88 1234delete from Student where id not in (select min(id) from Student group by name, class, subject, score) 怎样把如下的表，查询成另一种形式原表： year month amount 1991 1 1.1 1991 2 1.2 1991 3 1.3 1991 4 1.4 1992 1 2.1 1992 2 2.2 1992 3 2.3 1992 4 2.4 查询结果： year m1 m2 m3 m4 1991 1.1 1.2 1.3 1.4 1992 2.1 2.2 2.3 2.4 123456select year,(select amount from aaa m where month=1 and m.year=aaa.year) as m1,(select amount from aaa m where month=2 and m.year=aaa.year) as m2,(select amount from aaa m where month=3 and m.year=aaa.year) as m3,(select amount from aaa m where month=4 and m.year=aaa.year) as m4from aaa group by year SQL用法group byGROUP BY 语句用于结合Aggregate函数，根据一个或多个列对结果集进行分组。 id name class subject score 1 张三 1 语文 81 2 张三 1 数学 99 3 李四 1 语文 77 4 李四 1 数学 80 5 王二 2 英语 89 6 王二 2 数学 88 7 赵武 2 英语 77 8 赵武 2 数学 88 9 赵武 2 语文 89 查每个学生的总分123select name, count(*) as number, sum(score) as sum_scorefrom Student group by name name number sum_score 张三 2 180 李四 2 157 王二 2 177 赵武 3 254 每个班级每个科目的总分，平均分1234select class, subject, sum(score) as sum_score, avg(score) as avg_score, count(*) as num from Student group by subject, classorder by class class subject sum_score avg_score num 1 数学 179 89.5 2 1 语文 158 79.0 2 2 数学 176 88.0 2 2 英语 166 83.0 2 2 语文 89 89.0 1 having在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与聚合函数一起使用。HAVING 子句可以让我们筛选分组后的各组数据。 列出平均分小于86分的班级与科目1234select class, subject, avg(score) as avg_scorefrom Studentgroup by subject, classhaving avg_score &lt; 86 class subject avg_score 2 英语 83.0 1 语文 79.0]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown使用技巧]]></title>
    <url>%2F2018%2F05%2F16%2FMarkDown_Skill%2F</url>
    <content type="text"><![CDATA[这里记录平时用markdown时的一些技巧。 同一行插入多张图片MarkDown + HTML 具体代码如下123456&lt;table&gt; &lt;tr&gt; &lt;td &gt;&lt;center&gt;&lt;img src=&quot;https://img-blog.csdn.net/20180731150122598?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzODI2NTY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; &gt;图1 新垣结衣1 &lt;/center&gt;&lt;/td&gt; &lt;td &gt;&lt;center&gt;&lt;img src=&quot;https://img-blog.csdn.net/20180731150122598?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzODI2NTY0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70&quot; &gt;图2 新垣结衣1&lt;/center&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;]]></content>
      <categories>
        <category>杂七杂八</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[《机器学习》读书笔记：模型评估与选择]]></title>
    <url>%2F2018%2F04%2F25%2F%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[对于模型来说，其在训练集上面的误差我们称之为“训练误差”或者“经验误差”，而在测试集上的误差称之为“测试误差”。 评估方法作为模型评估，我们主要由三种划分数据集的方式：留出法，交叉验证法和自助法。 留出法留出法是直接将数据集$D$划分为两个互斥的集合，其中一个集合作为训练集$S$，另一个作为测试集$T$，我们需要注意的是在划分的时候要尽可能保证数据分布的一致性，即避免因数据划分过程引入额外的偏差而对最终结果产生影响。 为了保证数据分布的一致性，通常我们采用分层采样的方式来对数据进行采样。 交叉验证法k折交叉验证通常把数据集$D$分为$k$份，其中的$k-1$份作为训练集，剩余的那一份作为测试集，这样就可以获得$k$组训练/测试集，可以进行$k$次训练与测试，最终返回的是$k$个测试结果的均值。这里数据集的划分依然是依据分层采样的方式来进行。对于交叉验证法，其k值的选取往往决定了评估结果的稳定性和保真性。通常$k$值选取10。与留出法类似，通常我们会进行多次划分得到多个$k$折交叉验证，最终的评估结果是这多次交叉验证的平均值。 自助法留出法与交叉验证法都是使用分层采样的方式进行数据采样与划分，而自助法则是使用有放回重复采样的方式进行数据采样。 我们每次从数据集$D$中取一个样本作为训练集中的元素，然后把该样本放回，重复该行为$m$次，这样我们就可以得到大小为$m$的训练集，在这里面有的样本重复出现，有的样本则没有出现过，我们把那些没有出现过的样本作为测试集。进行这样采样的原因是每个样本不被采到的概率为$1-\frac{1}{m}$，那么经过$m$次采样，该样本都不会被采到的概率为$(1-\frac{1}{m})^m$，取极限有 \lim_{m \to \infty}(1-\frac{1}{m})^m \to \frac{1}{e} \approx 0.368因此我们可以认为在$D$中约有$36.8%$的数据没有在训练集中出现过。 这种方法对于那些数据集小、难以有效划分训练/测试集时很有用；此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。但是由于该方法改变了数据的初始分布导致会引入估计偏差。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>交叉验证</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[标量、向量、矩阵求导]]></title>
    <url>%2F2018%2F04%2F12%2FMatrix_Derivation%2F</url>
    <content type="text"></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C++知识积累]]></title>
    <url>%2F2018%2F03%2F15%2FC%2B%2B_Learning_note%2F</url>
    <content type="text"><![CDATA[记录一些平时积累的C++知识。 数据class与struct的区别struct能包含成员函数，能继承吗，能实现多态。 最本质的一个区别就是默认的访问控制：默认的继承访问权限struct是public的，class是private的。 多个指针声明误区多个指针声明应该为：例：int *a, *b, *c, *d;。当我们int* a, b;时，看起来就是定义了两个int*型的指针，实际上是定义了一个int*型的变量a和一个int型的b。 typedeftypedef允许你为各种数据类型定义新的名字，可以减少使声明变得又长又臭的危险，尤其是那些复杂声明。而且，如果以后要修改程序中所使用的一些数据的类型时，修改typedef会容易的多。 const当你声明变量时，如果变量的值不会被修改，你应该在声明中使用const关键字。这种做法不仅使你的意图在其他阅读你的程序的人面前得到更清晰的展现，而且当这个值被意外修改时，编译器能够发现这个问题。 当使用带有const的指针时其实有两种意思。一种指的是你不能修改指针本身的内容。另一种指的是你不能修改指针指向的内容。 常量取代C中的宏定义。可以使用const关键字来声明常量，常量的值不可改变，所以必须在声明时进行初始化。 12const int a = 10;int const a = 10; 指向const实体的指针下面声明指向整型常量的指针，可以修改指针的值，但不能修改它所指向的值。 12345const int *p;int const *p;const double pi = 3.14;double *ptr = &amp;pi //错误，ptr是一个普通指针，pi是一个const常量 const指针下面声明一个常量指针，此时指针是常量，它的值无法改变，但它所指向的值却可以改变。 1int * const p; 指针本身和指向的值都不可改变。 1int const * const p; const用于函数的地址传递参数void foo(const int *p);这种形式就是相当于函数调用者声称：”我给你一个指向它的指针，但你不能去修改它”。这也是const最有用之处了：用来限定函数的形参，这样该函数将不会修改实参指针所指的数据。参数为引用，不会简历临时副本，增加效率同时防止修改。 1void function(const TYPE&amp; Var); //引用参数在函数内为常量不可变 const用于限定函数的返回值也是用const来修饰返回的指针或引用，保护指针指向的内容或引用的内容不被修改，也常用于运算符重载。归根究底就是使得函数调用表达式不能作为左值。 123const int fun1() //这个其实无意义，因为参数返回本身就是赋值。const int * fun2() //调用时 const int *pValue = fun2();我们可以把fun2()看作成一个变量，即指针内容不可变。int* const fun3() //调用时 int * const pValue = fun3();我们可以把fun3()看作成一个变量，即指针本身不可变。 const 成员函数默认情况下，this的类型是指向类类型的非常量版本的常量指针，因此默认情况下我们不能把this绑定到一个常量对象上，也就意味着我们不能在一个常量对象上调用普通的成员函数。而当类开发者不允许成员函数改变类内变量的时候，普通的this指针无法做到，因为它指向的是非常量，此时就需要将this指针声明为一个指向常量的常量指针，C++通过在成员函数参数列表之后家const关键字来修改this指针类型为指向非常量。 1234567891011class TestConst&#123;public: TestConst(int x): m_nX(x) &#123;&#125;; void AddOne() const &#123; m_nX++; &#125;; //错误，this指针指向常量，不可改变 void DisPlay() const &#123; cout &lt;&lt; m_nX &lt;&lt; endl; &#125;; private: int m_nX;&#125; 引用引用就是某一变量（目标）的一个别名，对引用的操作与对变量直接操作完全一样。引用的声明方法：int &amp;refVal = val; 定义引用时，必须同时对其进行初始化。程序会把引用与它的初始值进行绑定在一起，而不是简单的将初始值拷贝给引用。因而一旦初始化完成，引用将和它的初始值对象一直绑定在一起。 定义一个引用，不是新定义了一个变量，它只表示该引用名是目标变量名的一个别名，引用不是对象，本身不占存储单元，没有实际地址，因此不能定义指向引用的指针。123456int ival = 1024;int &amp;refVal = ival;int &amp;refVal2; //报错：refVal2引用必须初始化int &amp;refVal3 = 10; //报错：引用的初始值必须是一个对象double dval = 3.0;int &amp;refVal4 = dval; //报错：引用类型的初始值必须是int extern和static1. 第一条也是最重要的一条：隐藏。（static函数，static变量均可） 当同时编译多个文件时，所有未加static前缀的全局变量和函数都具有全局可见性，为外部链接。 举例来说明。同时编译两个源文件，一个是a.c，另一个是main.c。12345678910111213141516//a.cchar a = 'A'; // global variablevoid msg()&#123; printf("Hello\n");&#125;//main.cint main()&#123; extern char a; // extern variable must be declared before use printf("%c ", a); (void)msg(); return 0;&#125;//运行结果A Hello 为什么在a.c中定义的全局变量a和函数msg能在main.c中使用？前面说过，所有未加static前缀的全局变量和函数都具有全局可见性，是外部链接，其它的源文件也能访问。此例中，a是全局变量，msg是函数，并且都没有加static前缀，因此对于另外的源文件main.c是可见的。 如果加了static，就会对其它源文件隐藏。例如在a和msg的定义前加上static，就改链接属性为内部链接，main.c就看不到它们了。利用这一特性可以在不同的文件中定义同名函数和同名变量，而不必担心命名冲突。static可以用作函数和变量的前缀，对于函数来讲，static的作用仅限于隐藏。 2. static的第二个作用是保持变量内容的持久。（static变量中的记忆功能和全局生存期） 存储在静态数据区的变量会在程序刚开始运行时就完成初始化，也是唯一的一次初始化。共有两种变量存储在静态存储区：全局变量和static变量，只不过和全局变量比起来，static可以控制变量的可见范围，说到底static还是用来隐藏的。虽然这种用法不常见。 static变量的默认初始化为0。其实全局变量也具备这一属性，因为全局变量也存储在静态数据区。在静态数据区，内存中所有的字节默认值都是0x00。 PS：如果作为static局部变量在函数内定义，它的生存期为整个源程序，但是其作用域仍与自动变量相同，只能在定义该变量的函数内使用该变量。退出该函数后， 尽管该变量还继续存在，但不能使用它。 基于以上两点可以得出一个结论：把局部变量改变为静态变量后是改变了它的存储方式即改变了它的生存期。把全局变量改变为静态变量后是改变了它的作用域， 限制了它的使用范围。因此static 这个说明符在不同的地方所起的作用是不同的。1234567891011121314＃include &lt;stdio.h&gt;int fun()&#123; static int count = 10; //在第一次进入这个函数的时候，变量a被初始化为10！并接着自减1，以后每次进入该函数，a return count--; //就不会被再次初始化了，仅进行自减1的操作；在static发明前，要达到同样的功能，则只能使用全局变量： &#125;int count = 1;int main(void)&#123; printf("global\t\tlocal static\n"); for(; count &lt;= 10; ++count) printf("%d\t\t%d\n", count, fun()); return 0;&#125; 4. static的第三个作用：C++中的类成员声明static（有些地方与以上作用重叠） 在类中声明static变量或者函数时，初始化时使用作用域运算符来标明它所属类，因此，静态数据成员是类的成员，而不是对象的成员，这样就出现以下作用： 类的静态成员函数是属于整个类而非类的对象，所以它没有this指针，这就导致它仅能访问类的静态数据和静态成员函数。 不能将静态成员函数定义为虚函数。 静态数据成员是静态存储的，所以必须对它进行初始化。 （程序员手动初始化，否则编译时一般不会报错，但是在Link时会报错误）。 由于静态成员声明于类中，操作于其外，所以对其取地址操作，就多少有些特殊，变量地址是指向其数据类型的指针 ，函数地址类型是一个“nonmember函数指针”。 由于静态成员函数没有this指针，所以就差不多等同于nonmember函数，结果就 产生了一个意想不到的好处：成为一个callback函数，使得我们得以将C++和C-based X W indow系统结合，同时也成功的应用于线程函数身上。 （这条没遇见过） static并没有增加程序的时空开销，相反还缩短了子类对父类静态成员的访问时间，节省了子类的内存空间。 静态成员初始化与一般数据成员初始化不同： 初始化在类体外进行，而前面不加static，以免与一般静态变量或对象相混淆；初始化时不加该成员的访问权限控制符private，public等； 初始化时使用作用域运算符来标明它所属类； 所以我们得出静态数据成员初始化的格式：&lt;数据类型&gt;&lt;类名&gt;::&lt;静态数据成员名&gt;=&lt;值&gt; 为了防止父类的影响，可以在子类定义一个与父类相同的静态变量，以屏蔽父类的影响。这里有一点需要注意：我们说静态成员为父类和子类共享，但我们又重复定义了静态成员，这会不会引起错误呢？不会，我们的编译器采用了一种绝妙的手法：name-mangling 用以生成唯一的标志。 函数内联函数影响性能的一个重要因素是内联技巧。 在C++中，函数调用需要建立栈环境，进行参数的复制，保护调用线程，返回时，还要进行返回值复制，恢复调用现场。这些工作都是与完成特定任务的操作系统的额外开销。程序效率由于改下工作而受到影响，所以，流行的CPU都已经将函数调用的额外工作硬件化了，以此来建减少运行开销。尽管如此，调用工作还是有一些微小的开销的，如果频繁调用很少语句的小函数，则这些开销对性能的影响还不好说。例如，下面代码频繁的调用一个小函数：1234567891011121314151617#include&lt;iostream&gt;using namespace std;bool isnumber(char); // 函数声明int main()&#123; char c; while(cin&gt;&gt;c &amp;&amp; c != '\n') // 反复读入字符，若为回车便结束 if(isnumber(c)) cout&lt;&lt;"you entered a digit.\n"; else cout&lt;&lt;"you entered a non-digit.\n";&#125;bool isnumber(char ch)&#123; return ch&gt;='0' ** ch &lt;= '9' ? 1 : 0;&#125; 程序不断到输入设备中读取数据，频繁调用isnumber函数。isnumber是个小函数，所以函数调用的开销相对来说占的比重就大了。为了免去调用开销，提高效率，可将程序改写为：123456789101112#include&lt;iostream&gt;using namespace std;//------------------------------------int main()&#123; char c; while(cin&gt;&gt;c &amp;&amp; c!= '\n') if(c &gt;= '0' &amp;&amp; c &lt;= '9' ?1 : 0) //将调用改为直接判断 cout&lt;&lt;"you entered a digit.\n"; else cout&lt;&lt;"you entered a non_digit.\n";&#125; 该程序在if语句中用表达式替换了函数调用。在程序运行上，因为免去了大量的函数调用开销，提高了执行效率。由于isnumber函数比相应的表达式可读性好，所以若程序中多处出现isnumber，而又将其替换为复杂的实现语句的话，就会降低程序的可读性。我们既要用函数调用来体现其结构化和可读性，又要是效率尽可能地高。解决办法就是将这种小函数声明为内联(inline):1234567891011121314151617#include&lt;iostream&gt;using namespace std;inline bool isnumber(char); // 内联声明int main()&#123; char c; while(cin&gt;&gt;c &amp;&amp; c != '\n') if(isnumber(c)) cout&lt;&lt;"you entered a digit.\n"; else cout&lt;&lt;"you entered a non_digit.\n";&#125;bool isnumber(char ch)&#123; return ch &gt;= '0' &amp;&amp; ch &lt;= '9' ? 1 : 0;&#125; 对函数的内联声明必须在调用之前。因为内联函数的代码在程序运行时是直接潜在调用处执行的，他不影响链接，只在编译时确定运行代码。因此编译时，在调用之前看到内联声明就是十分必要。 内联函数体应该尽可能小，且要结构简单，这样签入的代码才不会影响调用函数的主体结构。所以内联函数中，不能含有复杂的结构控制语句，如switch和while。如果内联函数有这些语句，则编译将无视内联声明，只是视同普通函数那样产生调用代码。 经验上，内联函数只适合于只有1~5行的小函数。对一个含有许多语句的大函数，函数调用的开销相对来说微不足道，所以也没有必要将函数内联。 把内联函数的定义放在头文件中，可以确保在调用函数式所使用的定义是相同的，并且保证在调用点该函数的定义对编译器可见。 参考[1]. C++中static关键字作用总结]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[岭回归、Lasso回归与稀疏性]]></title>
    <url>%2F2018%2F02%2F02%2FLasso_Regression%2F</url>
    <content type="text"><![CDATA[Ridge Regression其实就是带了L2正则化的线性回归。Lasso Regression其实就是带了L1正则项的线性回归。 线性回归给定数据集$D=(\boldsymbol x_1, y_1), (\boldsymbol x_2, y_2) \cdots (\boldsymbol x_m, y_m)$，其中 $\boldsymbol x_i=[x_i^{(1)}, x_i^{(2)} \cdots x_i^{(d)}]^T$ 表示一条样本数据有 $d$ 个属性，我们的目标是寻找 $d$ 维列向量 $\boldsymbol w$ 和常数 $b$，使得模型 f(\boldsymbol x_i)=\boldsymbol w^T\boldsymbol x_i+b \tag 1所得的预测值与真实值 $y_i$ 尽可能接近。 我们统一用矩阵和向量表示，把常数 $b$ 放入权值向量 $\boldsymbol w$ 得到一个 $d+1$ 维的权值向量 $\boldsymbol{\hat w}=(\boldsymbol w; b)$，同时在每个样本实例中添加第 $d+1$ 个属性，置为 $1$，$\boldsymbol{\hat {x_i}}=(\boldsymbol x_i;1)$。将样本所有属性排列为矩阵可以得到： \boldsymbol X= \left [ \begin{matrix} \boldsymbol{\hat {x_1}} \\ \boldsymbol{\hat {x_2}} \\ \vdots \\ \boldsymbol{\hat {x_m}} \\ \end{matrix} \right]令 $\boldsymbol y=(y_1, y_2\cdots y_m)^T$ ，我们最小化 ||\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}}||^2即 \boldsymbol w^*=\arg_\boldsymbol{\hat w}\min (\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}})^T(\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}})令 $h(\boldsymbol{\hat w})=(\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}})^T(\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}})$，求它的最小值只需其对 $\boldsymbol{\hat w}$ 求导，导数值为 $0$ 时 $\boldsymbol{\hat w}$ 的取值即为所求。 \begin{align*} \frac{\partial h(\boldsymbol{\hat w})}{\partial \boldsymbol{\hat w}} &= \frac{\partial [(\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}})^T(\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}})]}{\partial \boldsymbol{\hat w}}\\ &= 2\frac{\partial (\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}})^T}{\partial \boldsymbol{\hat w}}(\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}})\tag 2\\ &= 2\frac{\partial \boldsymbol y^T}{\partial \boldsymbol{\hat w}}(\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}})-2\frac{\partial (\boldsymbol X\boldsymbol{\hat {w}})^T}{\partial \boldsymbol{\hat w}}(\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}}) \tag 3\\ &= 0-2\boldsymbol X^T(\boldsymbol y-\boldsymbol X\boldsymbol{\hat {w}})\tag 4\\ &= 2\boldsymbol X^T(\boldsymbol X\boldsymbol{\hat {w}}-\boldsymbol y)\tag 5\\ \end{align*}最后我们令式(5)为0，此时的 $\boldsymbol{\hat {w}}$ 即为所求 $\boldsymbol w^*$ 2 \boldsymbol X^T(\boldsymbol X\boldsymbol{\hat {w}}-\boldsymbol y)=2\boldsymbol X^T\boldsymbol X\boldsymbol{\hat {w}}-2\boldsymbol X^T\boldsymbol y=0 \\ \boldsymbol X^T\boldsymbol X\boldsymbol{\hat {w}}=\boldsymbol X^T\boldsymbol y \\ \boldsymbol{\hat {w}}=(\boldsymbol X^T\boldsymbol X)^{-1}\boldsymbol X^T\boldsymbol y \\ \boldsymbol w^*=(\boldsymbol X^T\boldsymbol X)^{-1}\boldsymbol X^T\boldsymbol y然而，如果$d&gt;m$，即特征大于样本的话，矩阵$\boldsymbol X^T\boldsymbol X$将会不是满秩的，而这个解也没法算出来。或者更确切地说，将会有无穷多个解。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们 overfitting 了 解决 overfitting 的常用方法是 regularization。 Ridge Regressionridge regression就是在原始线性回归基础上，使用$L_{2}$范数正则化，具体为 min_{w} \sum_{i=1}^{m}(y_{i}-w^{T}x_{i})^{2}+\lambda ||w||_{2}^{2}解得： \boldsymbol w^*=(\boldsymbol X^T\boldsymbol X + \lambda I)^{-1}\boldsymbol X^T\boldsymbol y直观地来看，添加这个 regularizer 会使得模型的解偏向于 norm 较小的 $w$ 。从凸优化的角度来说， 等价于如下问题： min_{w} \frac {1}{n} ||y - Xw||^2, \quad s.t.\ ||w||_2 \leq C其中 $C$ 是和 $\lambda$ 一一对应的是个常数。也就是说，我们通过限制 $w$ 的 norm 的大小实现了对模型空间的限制，从而在一定程度上（取决于$\lambda$ 的大小）避免了 overfitting 。不过 ridge regression 并不具有产生稀疏解的能力，得到的系数 $w$ 仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。 可以看到岭回归得到的估计只多了一个正则项$\lambda I$.这一项的存在使得$(X^T X+\lambda I)^{-1}$在数值计算上表现更加稳定。尤其是当多重共线性(Multicollinearity)情况发生，$X^T X$接近奇异时，岭回归还是能得到稳定的结果。另外，在岭回归中还可以通过观察岭迹来剔除多重共线性的变量。 岭回归中的“岭”是什么？ 岭回归使用了单位矩阵乘以常量$\lambda$，我们观察其中的单位矩阵$I$，可以看到值I贯穿整个对角线，其余元素全是0，形象地，在0构成的平面上有一条1组成的“岭”，这就是岭回归中的“岭”的由来。 不过，特别是在像生物或者医学等通常需要和人交互的领域，稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。比如说，一个病如果依赖于 5 个变量的话，将会更易于医生理解、描述和总结规律，但是如果依赖于 5000 个变量的话，基本上就超出人肉可处理的范围了。 在这里引入稀疏性的方法是用 $L_1$ regularization 代替 $L_2$ regularization。 Lasso Regression在原始的线性回归上加一个$L_1$正则化，就变成了Lasso回归，具体是： min_{w} \sum_{i=1}^{m}(y_{i}-w^{T}x_{i})^{2}+\lambda ||w||_{1}LASSO 仍然是一个 convex optimization 问题，不过不再具有解析解。它的优良性质是能产生稀疏性，导致 $w$ 中许多项变成零。 为什么它能产生稀疏性呢？下面我们就先来看一个直观上的理解。 首先，和 ridge regression 类似，上面形式的 LASSO 问题也等价于如下形式： min_{w} \frac {1}{n} ||y - Xw||^2, \quad s.t.\ ||w||_1 \leq C也就是说，我们将模型空间限制在 $w$ 的一个$l_1 -ball$ 中。为了便于可视化，我们考虑2维的情况，在 $(w^1, w^2)$ 平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为 $C$ 的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解。如图所示： 可以看到，$l_1-ball$ 与 $l2-ball$ 的不同就在于它与等高线相交的地方大部分时候实在坐标轴上，而相交点就有 $w^1 = 0$ ，而更高维的时候除了坐标轴以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。 相比之下，$l_2-ball$ 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么 $l_1$ regularization 能产生稀疏性，而 $l_2$ regularization 不行的原因了。 更进一步了解稀疏性来源可移步参考文献1。 参考文献[1]. Sparsity and Some Basics of L1 Regularization]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
</search>
